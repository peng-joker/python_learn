[{"page_content": "hello today I want to talk about open gpts open gpts is a project that we built here at linkchain uh that replicates the GPT store in a few ways so it creates uh end user-facing friendly interface to create different Bots and these Bots can have access to different tools and they can uh be given files to retrieve things over and basically it's a way to create a variety of bots and expose the configuration of these Bots to end users it's all open source um it can be used with open AI it can be used with other models as as we'll see um and it's an exciting way to create a a GPT store like experience if you're building a more focused platform an internal platform or any of that so we launched this a few months ago actually right when uh open AI released their GPT store and but we haven't really dove into what's going on or how to use it um and so there's several things that I want to cover in this video there's maybe two main areas one I want to talk about it as an enduser facing application so how can you interact with we we have a we have a simple research preview hosted version of this um how can you interact what can you do what the functionality in the second half of the video I want to talk about how openg gpts is built um we do not intend to uh you know monetize openg gpts we want this to be a platform that people can clone and spin up their own versions of the GPD store or an internal platform or things like that so the second half of this video will focus more on how to build this platform and some of the considerations that we made please note that I am filming this on January 30th we are going to continue working on this continue changing it so if you are watching it later the hosted version or the code may have changed so starting with how to use it um the the the first thing that you'll want to do is create a bot and we actually have three different types of bots that one can create I'm going to start with assistant which is the default bot and and is also the most powerful one so the assistant can use an arbitrary number of tools and you can give it arbitrary instructions the llm will then take those instructions and those tools and decide which tools if any to call um get back the response and then and then continue on its way so let's create one that has access to the internet um so let's name it internet bot um let's give it some instructions you are a helpful weatherman always tell a joke when giving the weather let's scroll down there's a bunch of tools that we can use I'm going to use the search by tavil um they're a LM focused search engine um and then the currently the only agent type that we have supported in the research preview is with GPT 3.5 turbo but we will show in the code when you're building this you can enable CLA you can enable uh Google Gemini you can use gp4 you can use Azure you can use Bedrock but this is just the one that we have in the research preview so I'm going to save uh this now I'm going to try chatting with it let's say hi so several things to notice um one it's streaming responses so we put a lot of work into streaming of tokens this is important for uh uh you know most chat-based applications um it's got some feedback as well so I can give it thumbs up thumbs down um and start recording feedback and and when we talk about the platform side of things we'll see how that can be used now let's ask it a question that will require the search ability what is the weather in SF currently we can see that it decides to use a tool and so importantly it lets us know that it's deciding to use a tool and then it also lets us know what the result of the tool is and then it starts streaming back the response so this is streaming not just tokens but also these intermediate steps which provide really good visibility into what is going on we can see here we can see the response that we got back from tavil um and then we can see um the response from the AI and so there's lots of dad jokes in here this is using open aai tool calling under the hood so we can also ask it to look up multiple things and see multiple tool calls in parallel what about in LA and in New York so we can see that it now calls two things two things it C it call calls the tavil search Json for weather in Los Angeles the tavil search for weather in New York we can look at the responses this is for Los Angeles this is for New York um and then we can uh see the response here so this is an example of parallel tool calling that's enabled with open ai's most recent feature their their tool calling so this is the assistant it's using Tools in an arbitrary way to accomplish its task let's uh go create a new bot and now let's create one uh of type of rag so rag is really focused on retrieval over arbitrary files that you can upload so you can upload files and then the uh you can also give it custom instructions um and and then the bot will respond based on those files what is the difference between this and the assistant because in the assistant you could also upload files and you could choose retrieval as one of many to tools the main difference is that this is much more focused on answering questions specifically about files that you upload so this means that it will always look things up in a retriever it's actually hardcoded and we'll show this in in the um when we talk about the the back end but it's actually hardcoded that it will always do a retrieval step here the assistant decides whether to do a retrieval step or not sometimes this is good sometimes this is bad sometimes it you don't need to do a retrieval step when I said hi it didn't need to call it tool um but other times you know the the llm might mess up and not realize that it needs to do a retrieval step and so the rag bot will always do a retrieval step so it's more focused there because this is also a simpler architecture so it's always doing a retrieval Step at the start and then it's actually always responding after that it's not doing potentially two retrieval steps it's not doing an iterative search this is a very simple rag architecture um which has its downsides it's not as flexible it can't handle multihop questions things like that but it's much more focused streamlined and that means it can work with simpler models as well so we have actually enabled mraw um through fireworks to work on uh this type of gbt a rag gbt so I have a uh I have a PDF here that I'm going to upload um this is Spade this is a paper Shrea recently wrote um at Berkeley and it goes over setting up kind of like a testing pipeline um Bas for for your prompts super interesting paper um I'd uh I'd recommend reading it regardless um let's upload the PDF um we can we can change the message slightly um let's still use 235 turbo research Spade let's save this now it's taking a little bit longer to save because what's going on under the hood is that it's injesting the file now it's in a method where it can be retrieves and and I'll talk about this when we talk about the back end as well um if let's take a look at this paper um and let's figure out something we can um ask what is a propped Delta so here it always uses retrieval the the it calls the retrieval function it gets back documents we format documents nicely um and you can see what these documents are and then it responds here so this is an example of a simple rag bot which always does retrieval hyperfocused on rag if you want to ground a bot in some external data source that you can upload this is probably the simplest and most reliable way to do that again it's a little bit less trustworthy than uh than than the chat bot we have it as a separate type of Bot because it is simpler so that means that it can work with other models like mraw which is an open source model so it just provides more flexibility and that flexibility is the same reason we have a third type of Bot this chat bot this is just solely parameterized by the instructions so you can write out long complicated instructions for how it should behave you can give it a character and it can act like that again because this is simpler it can work with simpler models let's create an example chatbot we'll create one that responds like a pirate so you are a helpful pirate always respond in a pirate tone pirate save this hi and we get back a response and pirate so a lot of gpts in the GPT store are really just complicated system prompts and so for those you can create them using using this chatbot type the other a lot of the other gbts that I've seen at least are the rag style chat Bots where they're parameterized by a system prompt and then also um and then also a bunch of uh files that you can upload to give it information besides what it knows about and it can search over those and so these you know these are much simpler architectures than the assistant but for a majority of use cases they're actually completely fine the nice thing about assistance is you can do more complicated with things things with it and you can also equip it with arbitrary tools and so here there's a bunch of tools that we've enabled in the back end by default but you can easily add your own and and explore with those um and so that's part of the power of this platform being open source as well you can Fork it you can make it your own you can deploy it either to end users internal company users anything like that other things that I want to highlight in the front end um you can make Bots public this means that you can share links um you can see old conversations and jump back in um you can create new chats um when you're in a conversation you can click in here and see the bot that it is using um when you create a new chat you can look at the saved Bots that you have if you have any public Bots they'll be down here as well and yeah that's basically it for an overview of the front end I'm now going to switch to talk about some of the architecture of the back end which will be really helpful if you want to Fork this and make it your own so this is the openg gpts repo it's under the linkchain org um there's some instructions here there's a good read me on everything that's in here there's some Docker composed files for deploying it um there's some other uh uh files for environment variables the API docs things like that for this I really want to focus on the back end so we can take a look at what's going going on in here um and most of the logic here there's there's some requirements files most of the logic here is going to be in app and so we can see there's a bunch of different files here so there's a few things that I want to draw attention to first let's uh let's maybe look at agent types so when we talk about the assistant in the in the uh in the front end this is where these agent types are defined and so different assistants have different architectures that are going on behind the hood let's take a look at the open AI agent for example so important to note this is built on top of L graph so if you aren't familiar with L graph you should definitely go check it out it's a really easy way to build these types of cyclical agentic Frameworks so we have this open AI agent executor which takes in a list of tools an llm a system message and then a checkpoint um and we'll see how we use this so first um we're going to create basically this uh quote unquote agent and this agent is responsible for taking in messages and deciding what to do next so there's first a step where we format um the messages um and so we add a system message um and that's defined up here and then we pass in the rest of the messages the llm also then has access to the tools so we bind it with tools and then we combine it using this pipe syntax to get this agent we next Define a tool executor this is a class that is just does some minor boiler plate for calling tools um and then we start to define the different nodes of the graph so first we Define the function that determines whether to continue or not this is should continue it looks at the messages if there's no tool calls then it finishes if there is tool calls then it will continue and we'll see how we'll use this later on We Now define the node that calls the tools um so here uh we uh take in the list of messages if we we get the last messages we get the last message we know that it involves a function call um because otherwise we would have ended um we get all the tool calls so when there's multiple tool calls we get them all we then pass them in uh into here into the tool executor in a batch so it runs them in parallel um and then we append them to the the messages and we return the messages from this node so importantly this will use a message graph and so this means that every node in the graph should return a message or a list of message so here we return a list of messages the other node that we add the agent this is just this is basically an llm call the LM call returns a message so both of the nodes return messages we set the entry point to the agent so when anything comes in we go to the agent we then add a conditional Edge so after the agent is called we then check this should continue function if uh it says continue then we call the action node otherwise we finish um we add an edge so after we call the tools in the action node we go back to the agent we then compile it compiling we're passing in check pointer equals checkpoint this is basically a way to persist um the state of the graph so we're persisting all the messages that that happen so this is nice for a few reasons um the main immediate way that we are using it right now is we are saving it to reddis and then we're showing that in the front end so in the front end when you see that we save the chat history um those are pulling from reddis the way that is getting saved to redus we don't have separate functions saving everything we just pass in this checkpoint and it all kind of gets written there there's similar things for the Google agent Google agent looks very similar there's some minor differences uh because it uh uh is a a Google agent so it's a little bit different it doesn't have tool calling it has function calling there's also an XML agent designed to work with anthropic models and so this is different as well so it uses some of the prompts um and things like that so those are the agent types um we also have a uh really simple executor for the chatbot chatbot just calls the message once with the system message so it has a really simple node the chatbot node just calls it and then ends dead simple um but we use the message graph again uh so that it it all of these Bots can speak kind of like on the same kind of like State um which will make it nice if we want to do any multi-agent or multibot things in the future um and then we also have this retrieval bot so this retrieval bot um basically it it's it had it's simpler than the agent node um so it doesn't have any Loops but it's more complex than the chatbot node so we have this prompt template um this is used for coming up with a search query to pass to the retrieval um and so we can see that we have the conversation here and then we generate the search query um and then the response pop template takes in instructions and then has context so what's going on here is that we have this get messages function um and basically what's going to happen is we're passing all the state around as messages um and so part of that um has the the search query involved and so we can see here if we scroll down I'm going to scroll down to this graph we F we have this invoke retrieval uh node we have this retrieve node and then we have this response node and then we always invoke retrieval at the start and then we go from invoke retrieval to retrieve and then we go from retrieve to response and then we end so remember how I said the difference one of the differences with the retrieval with the rag bot was that it always did retrieval this is this is what's happening so the invoke retrieval node it's always going to return an AI message that calls retrieval so we're not even actually calling the language model sometimes so if the length of the messages is one this means if it's the first message in the conversation we're just going to look up whatever that first thing was um so this is a little cheat that a lot of rag based systems or conversational rag Bas systems do is the first time someone types in something in we just look up that input the issue start starts to happen when you have a conversation so if I have a follow-up question or you know a series of follow-up questions I don't really just want to pass that follow-up question in because it could be referencing things previously and so what I do instead is I call um this other method um which is itself a call to a language model um and so this is using the search prompt to generate a search query um and then I specify that as the the retrieval thing then the retrieval thing is just calling the retrieval it's passing the results in this function message um and then the response is just a call to the language model with some formatting into this prompt so if we see this get messages thing what we're doing is we're getting the uh most recent message which is the result of calling this tool we know that it will always be that because we have this this determined graph we're getting the response from that and we're we're formatting that into the system message um and yeah so basically Al we're constructing the chat history the chat history is going to be AI messages that do not have function calls if they do have function calls then they are the result of retrieval steps and we don't want to include those in the messages that we pass to the final LM um and then uh the chat history also includes human messages um and then it also includes the system message system message is where we use the system message prompt as well as the context that we retrieve from the documents so that is the so those are the three types of um Bots that we have they're all put together in this agent file and this is where we also start to use um uh uh configuration basically so configurable fields and configurable alternatives are something that exists in linkchain and they're really handy when you want end users to be able to configure things or sometimes when you want to do the configuration on the Fly for example if you want to randomly select a model to use and you want to configure that on runtime and basically the way that that looks like and the way that we've implemented it here um is that we have this idea of like a configurable agent that wraps around a binding and there's these different parameters on here inside the initialization we take in the parameters and we construct the agent um and then we pass it in and then what we'll do down below and I'll return to this later on what we do about down below is we initialize this configurable agent and then we Mark certain Fields as configurable so agent field here is configurable with an agent type um the system message field is configurable with a system message um the assistant ID this is the assistant ID of the um of the bot that you've selected the tools are configurable the retrieval descriptions configurable and so a lot of the uh fields that we create the bot with are configurable and those are exactly what we expose in the front end of uh open gpts we've also exposed some configurable Alternatives and these are the different architectures so uh there's um there's a chatbot which follows the chatbot architecture and there's the chat retrieval which follows the rag bot architecture and if we scroll up we can see that chat retrieval uses this idea of configurable retrieval um so this is the same kind of like runable binding it's got these same parameters we Mark these fields as configurable um and then chatbots exactly the same and so basically the difference between the fields these are things that go in again we have like three seate types of high Lev Bots the configurable fields are things to configure this specific the the assistant type of Bot and then the alternatives are completely different kind of like Alternatives that you could even use so there's two different ways that you can configure things fields and Alternatives one more thing I want to highlight is just the ingestion bit um so there's the ingest pipeline um which is really quite simple this is something we're going to look to expand on in the future um and so if if you want to help make this uh retrieval more advanced would love that but basically we just split documents um and then we add them to a simple Vector store um and then the retrieval um is here part of tools I believe um and we can see here that it's just a uh retriever um really simple um really simple and then we just filter based on the name space so uh based on the assistant that you're using the assistant only has access to the files that were uploaded to it um so there's a lot that we can do to improve this we can add a reranking step um there's already some sort of query transformation going on based on the Bots um but we can add more things um and so improving the retrieval is one aspect that we want to lean into in the future um the last thing I want to point out is that this all integrates with Lang Smith so if you're a little bit lost about what the different types of agents are what exactly is happening uh uh what got configured um that's totally normal these applications start to get really really complex and that's where lsmith comes in handy so the deployed version we have hooked up to a project in lsmith and so we can click on here we have this open gpts example project um and so if I go in here and I click on a trace I can see exactly what's kind of like going on under the hood um and so here um this is if you remember this is actually the system message that I added um when I configured the pirate chatbot um and so this is a system message this is what I said and is the response and so I can track it and so I can also leave when I when I left the thumbs up and thumbs down um I can track that here um and so I believe yeah so here I left the thumbs up on this was this was the weatherman um I I got back this response if we click in what's here I think yeah so this is when we gave it access to the Search tool um and so you can see like exactly what's going on um this is a pretty simple agent because it just responded um but I can see the feedback here as well it's at the top level so I can see the feedback here as well I have a user score of one um and so yeah Lang Smith is a whole separate concept but I just want to point out that openg gpts if you deploy it it's integrated with Lang Smith um if you need Lang Smith access shoot me a DM on Twitter or LinkedIn and can get you access to that that's pretty much it for what I wanted to cover um hopefully this gives you a good sense of both how to use the front end the the research example as well as how to configure the back end um so I think I think the important thing to note that I would highlight is that you can put any different type of architecture behind here right now all the architectures the three different architectures we have the assistant architecture the rag architecture and the chatbot architecture they all use this message graph implementation which passes around a list of messages and I really like this because it's a single common uh kind of like interface that'll make it easy to add on different bots in the future um so there's a lot of things we want to do in the future one of those is having like multiple bots on the same thread or allowing you to switch Bots between threads um and so having this common state representation will make it easy to do that that's pretty much all I got hope you guys enjoyed this we're really excited about open gpts if you want to use this I mean one feel free to Fork it but also feel free to reach out to us we're more than happy to help thanks", "metadata": {"source": "HAn9vnJy6S4", "title": "OpenGPTs", "description": "Unknown", "view_count": 8929, "thumbnail_url": "https://i.ytimg.com/vi/HAn9vnJy6S4/hq720.jpg", "publish_date": "2024-01-31 00:00:00", "length": 1530, "author": "LangChain"}, "type": "Document"}, {"page_content": "hey folks I'm Eric from Lang chain and today we're going to be building a search enabled chatbot with EXA which launched today um let's get started uh to begin let's go through the main pieces of software that we're going to be using um to start we're going to be using Lang chain um Lang chain is a framework for developing llm powered applications um it's the company I work at and it's the framework we'll be using we'll be using the python library but we also offer JavaScript library um for folks building in that language um the second thing we're going to be using is EXA EXA is a llm focus search engine which allows us to retrieve results uh from the web to provide additional context for our uh generation chatbot um before giving you your results uh Lang Smith is what we're going to be using for debugging and observability today it is the first party um observability tool built by the us at Lang chain um we're going to be using it to inspect some traces and we're also going to be using um the hosted Lang serve product which is part of Lang Smith um in order to host our application in the end um and uh we're also going to be using Lang serve kind of as previously mentioned uh the open it is an open source package um that allows you to host your chains as rest endpoints um and uh we're going to be using the hosted one as well within Lang Smith uh let's dive it so first we're going to be building our chain in a jupyter notebook then we're going to be porting that over to Lang serve as a rest endpoint we're going to be uh playing with that in the playground that's provided there and last but not least we're going to be hosting that in Lang serve in order to um get it accessible from anyone on the internet um so let's dive in so for our notebook um I'm just going to be using the Jupiter notebook in my vs code instance um first we're going to want to install some of our dependencies um as some of you may have noticed we're starting to split out the package into um multiple smaller partner packages as well so today we're going to be um pip installing uh Lang chain core which offers um we're going to be mostly using the runable utilities as well as prompts from there um we're going to use Lang chain open AI um in order to um use the GPT 3.5 model for Our Generation you can obviously install any uh other llm into this application as well um and last but not least we're going to be installing the brand new L chain exop package um which is going to give us our retriever which allows us to kind of search uh the web the Lang chain EXA package also offers some tools which enable agents to um have search capabilities as well um but for today's generation use case we're going to be using uh the retriever um once you have that installed um we are going to set some environment variables um so to do that we'll have OS and we'll set um our open AI API key and our X API key um um I already have these set in my environment so I'm not going to fill this out for yall today um but you can provision your own open AI API key at uh platform. open.com and uh I'll link the exod docs um on how to get your own xit API key with some free um search credits um on it as well to this video um the other thing that we're going to do is we're going to set some environment variables that um enable us to use Langs Smith um Langs Smith is in private beta at the moment feel free to DM me in order to get access to it um so in order to do that um we are going to set the Lang chain facing uh V2 uh to do that we're going to set the L chain tracing V2 environment variable we're going to set our um L change API key and we're going to set our Lang chain project as well um the last step is optional um and it just allows us to uh find um the traces for what we're doing today a little bit easier um and most importantly we're going to build our chain um so the rough architecture of today's chain is we're going to first uh take some sort of user query um we're going to search EXA to get some documents related to that um EXA has this great feature we're going to use called highlights which kind of summarizes the highlights from any of the sources it retrieves um and so we're going to be able to PL those into an llm uh we're going to kind of wrestle with um formatting that prompt in a way that the llm understands uh well for a little bit and then we're going to pass that into the llm for Generation Um so to start uh let's actually just play with Exel a little bit and see what that looks like um so we can import our retriever uh from the L chain EXA package we're going to be using the retriever today um let's just use the default settings and see what documents we get from that so we can just run retriever do invoke of uh best time to visit Japan um and if we inspect those um or inspect the first one uh we can see that we have some content which is actually going to be the entire page content um of that website so it's going to be rather large um and we'll also have some metadata on that document um and so here we'll have um some like kind of page titles URLs um as well as um if we pass the highlights feature in we'll actually get some highlights as well so let's pass in um highlights as true and then let's actually only get the three most relevant results just so we can save some of our EXA credits um so here we'll now see that we get um this highlights array which is going to summarize kind of the main points um of this page um so we're mostly going to be using that uh highlights field as well as um the URL uh since we want our generation bot to to site its sources in the generations it do does um so with that let's try to create some sort of um retrieval chain which is going to format all of those outputs in a way that we want to pass it to the Ln um so the kind of first step of that is we're going to run our retriever um and just to keep it clear we can Define our retriever down here as well um keeping the kind of final output in a single cell makes it little easier uh for us to convert this to a l serve application at the end um we'll also want to format this with um a prompt template so let's import that as well from Lang chain core. prompt and uh we can import other things as we go on so first we're going to start with our retriever um then we're going to want to uh format that list of documents in some sort of way so let's pass that into some sort of documents chain um or single document chain uh and we'll have it operate on each element of that list so we can kind of call map on that um and let's define that up here and so the Assumption here is that input is going to be that single document um here we'll have this be a runnable Lambda where we'll take uh that document and we will pass back the highlights as well as the uh URL of that document so here we'll have document. metadata highlights and document metadata URL um and we'll need to import um our rable Lambda as well which is going to be from L chain core runnables P runable Lambda um and then the next step is we're going to want to format this with some sort of prompt so let's pass this into um our document prompt which we can Define up here as prompt template uh from template and here uh let's actually just wrap the whole thing in XML tags just because it'll allow the llm to kind of isolate information from um the inputed documents a little bit better um and then we can include the URL and the highlights um and in this format the highlights are going to get past in as a list of strings that's probably okay um if we wanted to make this um a little more intense and we can do this later we can actually uh split those out into different sections um but because all the highlights come from the same URL uh this is probably okay for the uh point of our application today um so that kind of forms our document chain um where at the end of this we're going to have a list of um prompt values um which we can use Downstream um and then let's then combine all of those kind of source information contexts um with another Lambda um which is going to take all the docs and we're going to want to join some sort of list of these so we can do the text of each of those prompt values um for each of those documents so let's see what we actually get from that um it looks like we are missing a comma somewhere um because a string object is not callable it looks like here I should have called backend. join at this um and we can see that we're getting kind of a single string back with our three sources uh as well as the highlights of uh each of these sources uh where it's recommending public holidays uh the most romantic time of the year um and the rainy season um which is quite interesting um but now we kind of have our retrieval chain which we can use in our broader chain down here um for further processing so um our overall chain is actually going to take a very similar format to a general rag chain um where we're going to have some sort of um runnable parallel to start we're going to want to um start with um plumbing through the query which is just going to be a runnable pass through um this is going to just pass the users's query onto the next step through this query uh key and then we're going to want to pass some context Tex um which will be our retrieval chain then we'll want to pass that into some sort of generation prompt which is going to take the query and context um and format it for the llm and then we'll pass that into our llm um so let's fill this out a little bit so first We'll add some imports um we'll import our runnable pass through and our runnable parallel from Leng chain core um we'll Define a generation prompt as uh and let's do this one as a chat prompt so we can use an a chat model for the llm um so we'll use a chat prompt template for messages um and we'll have some sort of system prompt as well as the content which we can fill out in a second um we'll need chat PR template for that for that uh which co-pilot is quite nicely supporting for us um and then we'll just use the default chat open AI uh which uses GPT 3.5 turbo um for our llm so we can import that from Lang chain open aai um import chat open AI um this is kind of the new format of importing from some of those partner packages um cool so let's fill out our prompt um so let's tell our llm that you are an expert uh research assistant um you use um XML formatted contexts to um to research people's questions uh and then we'll actually format this uh query that we're going to ask down here where we're going to say please answer the following query based on the provided context and we'll pass in our query um add a little separator and then continuing with some of the XML tags we can pass in our context here which is just going to be this some form of the string up here which is going to have multiple sources um let's also add an instruction to please site your sources at the end of your response um cool so now we have our chain defined let's see if we're able to invoke it and we'll continue with our same question and it looks like it's giving us a nice and long response um detailing some things and hopefully at the end it'll have some sources of what it used to generate that um so now that we have that constructed and tested um let's actually go back to Lang Smith and see what it generated for us um so here in Lang Smith we can see um the default view is going to show us the most uh kind of relevant pieces of the trace so in this case because it's a retrieval augmented generation bot um it's going to be our retrieval step such that we can inspect which documents it's actually retrieving here um as well as the llm call so we can see uh what prompt we're actually passing in so we can see that we're formatting our contexts and sources um properly early here which is good as well as the output from the llm at the end um if you want to inspect kind of what's happening in each of those runnable Lambda steps uh you can inspect further and see um that the inputs to each of those kind of Lambda steps is going to be one of those documents and then we're outputting um like the highlights DL segment and then formatting that with our prompt template uh if you recall from when we were constructing it um so now we have kind of our uh fully constructed chain uh for our uh search enable chatbot with XF um and now let's convert that to Lang serve um so to do that we'll go back to vs code um and here we're going to um start with uh installing the um Lang chain CLI um we'll just do this in a terminal down here um you can just install um recommend running it with upgrade in case you already haveen it installed from uh either using Lang chain templates or something else um you can just run install upgrade Lang chain clly um then you can uh run the line chain commands so first we're going to initialize our app with Lang chain app new um I'm just going to create it in the current perplexity directory um if you give it a name it'll just create a directory uh within that um we will skip adding packages just because we're going to paste in a chain that we have defined up here um the packages are just L chain templates um and then we can see that it has bootstrapped an application for us um with a Docker file P project toml a readme um as well as an app directory and a packages directory um we're actually not going to be using the packages directory um as mentioned that's for L chain templates and we're just going to be playing in this uh server.py file in the app um so first we're going to um add some dependencies um I use poetry for my dependency management um it allows um it to automatically update this P project toml file which is quite nice um we're going to install the same set of dependencies um that we had in our uh jupyter notebook so we're going to poetry add um linkchain core um and we'll see that show up in the Pi Project tomel and then we're also going to add L chain open Ai and Lang chain XF um I'll add those and then be back in a second and now that we have our dependencies installed um also added the command to the notion doc over here just so it's easier to follow along um now we will copy our chain over into our application um I like keeping the server.py file a little bit cleaner in order to just keep the um fast API kind of app logic separate from the Lang chain chains um so let's actually just Define a chain. piy file next to this um where we can um paste in our um chain from this other document um so I'm pretty sure I can just paste in these two um cells and we'll remove the invoke in the middle um so first we'll Define a retrieval chain and then down here we will uh Define our regular chain um and we can even put all of our Imports together up here to keep uh vs code a little bit happier um and we'll save that file and now our chain should be accessible from our server file um and then by default it doesn't have any rout defined but it gives you this nice little stub and so we'll actually just add uh that chain which we'll import um from app. chain import chain um because we have it in chain. piy as well is chain is the name of our variable um and then that will just host uh this endpoint at the kind of default SL endpoint um if we want we can define a path um as kind of Slash search or something like that um which could be kind of nice so we've copied the chain over we've added the routes um and now uh we can serve it um so let's poetry run L chain serve um and see if that works hopefully I remembered to add all the dependencies um and if you get this message you're probably in business um so from here uh we can see that we're hting it on Port 8000 so we can click onto that um by default it'll show us the stocks page um but we want to go to the search playground um right now it's telling me that there is no applicable renderer found which is too bad um let's go back to our chain and see if we can add a uh type definition for the input um where here we have our input type uh set just to a string um and see if that does anything for us and perfect so now we can see that we get our input as a string um generally Lang serve will try to detect uh the defined input type for our input over here I think it got a little bit confused because the input of our runnable pass through and our retrieval chain um could be something other than a string um or some sort of formatted dictionary and so uh we had to Define our input type as a string if you want to pass in uh multiple um pieces of the input you are welcome to Define this as like a pedantic base model um which we can play with in a little bit if we want um so here let's try um when is the best time to visit Japan um and we can see uh in the link of playground it actually will show us kind of the individual steps that it's running as part of this um so right now it's kind of running through the retrieval step before passing it all through to the llm um it's taking a little bit of time so we can check our stack Trace right now it is telling me that I am using a wrong API key um so let's try adding my API keys to my environment again and running Lang chain serve again um and then we can try this again and that looks to be going better so we can see that it ran the intermediate steps for retrieval um and then in the playground we actually get streaming output as well um which is pretty good uh so here we get kind of a guide for uh why it might depend on several factors let's ask specifically for a ski trip um and see if it gives us a more concrete answer and here it does and it starts planning our trip as well figuring out which ski area we're going to go to um the other nice thing about Lan serve is even when using it hosted uh locally like this um we will actually see these traces show up in lsmith as well um so here because the playground is using streaming output the only difference is the output is going to be um kind of uh some sort of chunked output but Lang Smith handles that pretty well um in terms of displaying that to you uh and overall our Trace actually looks pretty similar um where we can see kind of the that's actually formatted in the um chat open AI step and we can even see which documents are actually plumed through um one note is this app that we build is just uh using the highlights that are given to us by EXA um if you want you can also operate on the entire page content um but the EXA highlights work pretty well and they save you a lot of tokens um as mentioned earlier um if you want access to Lang Smith uh feel free to DM me um and I can get you access to that okay and then last but not least uh we're going to um host our chain in hosted Lang serve um so hosted Lang serve is this deployments tab in Lang Smith um right now I don't have any deployments in my um uh in this account um so we can go over to uh new deployment and see kind of what steps it wants us to do we'll name our deployment so here we'll have this as perplexa T um emphasis on EXA and then we'll want to import our repository from GitHub um so in this case I'll actually just deploy um this uh project to just a public repository um you can also deploy uh private repos and that all works well too um so here we'll want to initialize our repo um we'll see what it is going to try to commit um just in case we've created any of these py cach folders let's actually create a g ignore where we'll oh we already have py cach in there so let's try adding everything and see what files it's going to add that looks pretty good um and uh we can commit um that project there um I'm realizing I'm using a a local development version of the EXA package because it's not published yet um so I'm going to fix that and be back in a second okay I fixed that dependency um that will not be a problem for you because you'll be using piie dependencies if you do ever use um kind of local development dependencies in your your application you will have to either make those git dependencies or uh publish them to piie in order for hosted L serve to be able to download them um unless it's kind of within the same directory um so let's uh commit our project um and then we can create a repo um we'll push our existing local deposit repository we'll call it FX T um lank chain perplexity uh we'll push it to just my personal ER don't need a description and we'll make it public um we can add the remote As origin and we will push our local commits to it um cool so now we can and go over to GitHub and see if that is um properly showing up as L chain perplexity um and looks like we are in business um so if we go back to our deployments we can import that repository from GitHub um we'll give it access to Lang chain perplexity and we will authorize and we can close that back here we can see that we have access to that repository if you give access to all your repos um you'll just be able to select that here um it's stored in the root of the directory we're using the main branch um and we'll need to set some environment variables so here we're going to want our open AI API key we're going to want uh that to be a secret and we're going to want our EXA API key as well um I'm going to populate those two and be back in a second okay now that we have those environment variables Set uh we can submit our deployment and uh hosted Lang serve will kind of take over from here um it'll provision a machine for us um hosted Lang serve is very well integrated with Lang Smith um such that all of our traces will actually populate in its own project over there um and we'll be ble to see all the traffic that comes into it um through the hosted endpoints here um for hosted Lang serve um this is also in private beta um and if you would like access to it uh feel free to DM me about that as well um so we set our environment variables and um I'll kind of cut out the section where this is deploying um and then we can test it in the hosted playground okay and now our uh posted Lang application is deployed we can see our deployment shows up up here um we can go to it we can see that uh the docs for it are showing up and we can even go to our same search playground endpoint um and see if we can ask a question um such as when is the best time to visit Japan for the cherry blossoms and we get a result in recap uh today we uh went through uh building kind of a search enabled chatbot with EXA um using Lang chain EXA Lang Smith and Lang serve uh we started by developing our chain in a jupyter notebook um that was kind of where the bulk of the Lang chain specific logic was um then we ported that over to a lang serve application using the Lang chain CLI um and kind of monitored what was uh coming out of that through lsmith traces and last but not least we deployed our Lang serve application in hosted Lang serve uh also a beta product um as part of Lang Smith um again if you want to get access to the private beta of Lang Smith or hosted Lang serve um please let me know um and and thanks for listening bye-bye", "metadata": {"source": "dA1cHGACXCo", "title": "Building a web RAG chatbot: using LangChain, Exa (prev. Metaphor), LangSmith, and Hosted Langserve", "description": "Unknown", "view_count": 7440, "thumbnail_url": "https://i.ytimg.com/vi/dA1cHGACXCo/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGEQgWyhyMA8=&rs=AOn4CLDz7KQK8VbZMWIgIheOS8TQzo9sNw", "publish_date": "2024-01-26 00:00:00", "length": 1950, "author": "LangChain"}, "type": "Document"}, {"page_content": "streaming is uh an incredibly important ux consideration for building L Ms in a few ways first of all even if you're just working with a single llm call it can often take a while and you might want to stream individual tokens to the user so they can see what's happening as the llm responds second of all a lot of the things that we build in the laying chain are more complicated chains or agents and so being able to stream the intermediate steps what tool are being called what the input to those tools are what the output to those tools are um streaming those things is really important for a nice ux as well so we've recently added a new method in Lang chain to help with this called stream events um which hooks into the Callback handlers we have so it's very easily modifiable and what it will do is it makes it easy to stream everything basically and so this video will cover uh stream events so we'll also cover the basic stream and a stream methods that we have so these are simpler methods for dealing with the end responses of of chains or of models and it will take a look at doing this for generic rentables and then for uh link chain agents as well so let's jump into it this is the docs right here little known fact all of these docs are also notebooks so I'm going to open up the notebooks version of this um let's restart my kernel um just so I can do it with a clean end um and yeah we're going to cover two main things here we're going to cover the stream and then a stream methods and these are methods for streaming the final outputs of chains and then we're going to cover the stream events um method we're going to cover the stream events method and this is a way to stream all the events that happen within a chain so um all runnable objects in link chain and so these are every these are all chains these are all models these are basically all objects in link chain EX Expos a sync method called stream and an async variant called a stream and so these methods will stream the final output in chunks yielding each chunk as soon as it's available and so as we see as we'll go along streaming is only available if for if every step in the program kind of like knows how to stream things along so if you're working with one step in the program like an llm if it knows how to stream then you can use stream if you're working with multiple maybe one of them doesn't know how to stream and we'll see an example of this later on then it might not stream so let's start by looking at the llms themselves so we'll use an anthropic model here and basically what we'll do is we'll iterate over the chunks in uh model.am things that come out of these are going to be AI messages um and so uh what we're going to do is we're going to append them to a list and then we're going to print um chunk got do content um and so this is just a Content field on the aage and we're going to print this little operator to to show this the spaces between the streams so we can see that it starts to stream as it responds if we look at one of these chunks we can see as AI message these AI messages we've we've made kind of like addable under the hood so you can add them up and you can get the final response okay so that's a simple llm now we're going to look at chains and so in this chain and so chain is kind of like anything constructed in L chain expression language this chain will just be a really simple one where we have a prompt a parser um and then the chain is prompt model parser um we will invoke it uh with a stream so we'll use async here and we'll stream over things and we can see what happens as it goes through and so basically we're asking we have a prompt that's formatting it with uh uh it's tell me a joke about blah um we have this model and then we have this parser and this parser is just responsible for taking the cont content property off of the message and so it just return to string instead of this AI message the model and the pars Expos stream method so we'll see that it will stream out as it completes there we go all right so we can also see an example of other things um so uh a more complicated parer that we have um is the Json output parser which takes in a response from a language model that is supposed to be Json um but it might be like half completed Json right so this happens when you're streaming the the like when you when you're halfway through the stream of a Json blob it's not valid Json and so this parser deals with a lot of those peculiarities and parses it accordingly and so we can see here that it will start to stream out the more completed kind of like Json blob as we create it here we'll see an example um where this doesn't stream um so this is a method that just returns all the country names um and so here if we add this on the end of The Blob we can see that there's actually no streaming that will happen because this final thing does it it doesn't have it doesn't yield things it doesn't stream it doesn't take in it doesn't take in a a generator and it doesn't return a generator and so it kind of blocks everything so it just returns it once at the end if we want to change that um we can change this and this will start to yield things and so why is this happening um I think this is happening because I have a uh I have some flush issues here um let's do this see if this changes things yeah so we can see here once I add in this so there's some flush issues going on but basically we can see that it streams back um tokens and again that's because this yields things com some built-in components like retrievers these don't stream anything so if I call uh the um if I call this it just returns a list of documents we haven't really seen a need for retrievers to be streaming so that's why we kind of haven't implemented a version of that but you can still use these within chains um that uh uh you can still use retrievers within chains and still stream the final results so here's like a really simple rag chain that gets the context back um so basically calls a retriever gets some documents passes it to a prompt passes it to a model passes it to an output pars here these still stream so we'll see that the final response we can stream all right so those are covering kind of like the basics of stream and aam the the big uh kind of like limitation of these is that it only streams the final kind of like output of a of a chain often times we might want to stream the the intermediate steps or the events that happen within the chain along the way so we'll walk through this new method um stream events um and the events that it produces are aligned with the Callback handlers that we have and so we have a bunch of different types here that we've listed here um so we kind of like have uh uh so this is for chat models and llms very similar we we yield something on the start we we yield something on the end and then on stream we also yield each token if the model support streaming same for llms um same for chains same for tools same for retrievers and then same for prompts prompts just has start and end um one thing to note is that this is a beta API um and would love to hear any feedback about it so please let us know so here we can start we can use the the we can just use the um model mod and we can we can call a simple model and we can take a look at the stream events that are associated with this model one thing that you may notice is this version thing in a stream events so what is this uh basically we're documenting kind of like the API for this a stream events um interface as mentioned it's a beta interface it may change so we want to uh make it make it easier to do that in the future let's take a look at what some of these events are so we can see that the first one is on chat Model start so it starts kind of like the the streaming the first event it hits the chat Model start and then it's chat model stream chat model Stream So basically what's happening is the input's going in it's starting the chat model and then after that it's getting token by token if we look at the last results we can see that it's on chat model stream and then on chat model end so this is a very simple single call to a chat model that first calls the event on chat Model start then calls stream on each token and then calls on chat model at the end let's take a look at the chain that we have so now we have this model and we have this Json output puror let's gather the list of events and we can explore them this way so the first three events that we see are actually all on blank start so first we have on chain start because we enter the overall chain here then we have on chat Model start because that's the first one but then the model streams things through so it passes a generator through it passes that generator through right away to the output parser and so we immediately hit kind of like on parser start we can explore this a little bit more by looking at the by looking at some different events so let's look at maybe like the last three events in here we can see that we have on chat model end on parser end and then on chain end um so this is this this makes sense it's uh you know if we imagine kind of like the the SE quence of of chain encapsulating a model and then a parser basically the model will kind of like finish then the parser will finish and then the chain will finish let's look at maybe some of the middle ones so if we look at like 3 to six we can see that what happens maybe let's do 3 to seven for just a slightly nicer thing um we can see that it's a mingling of of on chat model stream on Pur stream on chat model stream um sorry on chat model stream on par of stream on chain stream so what's happening is as the chunks occur because we stream them all the way through we first see one token come out of the chat model that gets passed through to the parser that gets passed through to the end of the chain and so it's the sequence of events so now we can take a look at uh using these real time to do some streaming so we will do this by parsing the revent the the the events that get emitted by the stream events so first we'll take a look at each event and we'll take a look at what the kind of the event is then we can basically do things depending on what the kind is and we'll just do some simple string matching based on the name so if the chat if it's on chat model stream this is when it's printing out a token we'll P we'll print this chat model chunk and then if it's on parser stream this is when it's uh going through the output parser we will pass that and we'll set flush equals true this will get rid of some of the flushing issues that I had above and yeah we'll we'll just only do this for the first 30 so we can see now that we start printing things out and it's intermingled with chap model and parser because basically the chat model starts accumulating tokens and at some point then the parser emits something um but but they start streaming kind of like at the same time there might often be a lot of different events coming out from your chains when you have complicated ones with more types of models um um different types of tools subchains things like that and so we want to make it really easy to basically um filter these events we have a few ways of doing that one is by name so you can assign any runnable so again a runnable is just a model or a prompt or a chain or an output parser you can assign it with a run name and then you can filter um the events by include names so here we give uh the model the Run name model the output parser the Run name my parser we have include names only my parser and then if we print out events we will only get events related to the output parser because we're filtering out the ones with the Run name model if we switch it up and we go model then we only get ones associated with the model another way to filter is by type so we have the same chain here um but we're just going to uh change include types and so now include types we're going to set next to chat model this will get all chat models so if you want to get all chat models regardless of the name that you give them um you can you can get them this way tags are another way to do it tags are basically inherited from any child component so here we're going to give this whole chain the tags my chain and then we're going to stream all events and include tags my chain my chain with so the way that tags work it's not just events associated with this chain it's with any sub child components as well so if we stream it out we see that we start to get everything we get chain um we get onchain start on chat model on parser start in this case because we taged the whole chain with my chain this is everything if we take a look at non-streaming components again we can see that so this is the this is the one that blocks the final result um doesn't stream if we look at a stream again we this is the same example as above it doesn't stream but if we do stream events it does and because that's get they get passed through up until that point so we can see kind of like everything going on another thing this is a slightly more advanced one is basically if you have um if you have basically custom tools and they have inside them a runnable um then what you want to do is you want to propagate the call back so that otherwise no stream events will be generated a common example of this is if you have a tool for an agent and that tool calls an llm you need to make sure to propagate the the callbacks correctly um so here is an example of it not propagating correctly um and so you can see that there is basically just the stuff for tools because even though it calls this runnable inside of the tool the callbacks aren't propagated so it doesn't actually know that exists if we now propagate them and you can do that by just adding in callbacks as an argument to this tool function um and then passing it in through callbacks here um you can see now that we get this onchain start and onchain end event and these occur in the middle and so these are I think more examples of just the same all right so that covers basic streaming stuff we get most of the questions around streaming around agents and so I want to show two examples of doing that so this is the streaming page in the Lang chain documentation that shows how to stream with the agent executor so let's restart the kernel so we can see what's going on here if you haven't if you don't know what agents are check them out in a separate video I'll link to one in the description basically with agents the first thing we're going to do is we're going to define a model and we're going to make sure that streaming set equal to true this is so that it's this is necessary so that it it streams no matter where it's called from so in an agent it will be called within the agent many times so we set streaming equals to True um and then we Define our tools um and so these are just two example tools tools um we can play around with them and then we initialize the agent and so here we can see that we are going to first pull a prompt and we're going to use this prompt for agent we're then going to list out the tools we're then going to create the open AA tools agent and we're going to give the model here a tag agent llm and then we're going to create the agent executor this is the runtime for the agent um and we're going to give it run name agent this covers streaming so this is the streaming of the agent the streaming of the agent does the individual steps so that's nice but often times we want to get the individual tokens as well so let's stream let's go down to custom streaming with events so here we can do the same thing that we had before so we have the input we'll use version one we're using a stream events and now we can start doing stuff with the uh the events that are emitted so if onchain start and if the event name was agent so this is basically this is if you remember back up here we tagged the agent executor with the Run name agent so this is basically saying whenever on on the start and end of this we are going to print out this and then same thing on the end of this we're going to print out this and this is needed because there's a lot of subchains within this agent um there's places where we kind of like do the prompt into the model um those are subchains we really care about this overall agent and then we care about the streaming of the tokens from the chat model so we are going to uh basically if it's on chat model stream and if the content is exists and so there's actually cases where the content doesn't exist and this is when tools may be called we can we can play around with this and see what that looks like after but for now only if there's content we're going to stream that and then we'll also stream the tool start and Tool end so if we do this we can see that it starts printing things out you can see that it really quickly streamed that um and then gets that so if we take a closer look we can see that it prints out starting agent we then get a bunch of information about the tool um tool one tool two and then it starts uh streaming the final response we can also change this so that it always prints out the content and so now what we'll see yeah so we'll see here so these things are when it actually so okay so this is when the content didn't exist we can maybe even just print out what exactly it was yeah here we can see that it prints out the whole chunk and so we can see that the tool calls um and it's still streaming kind of like stuff um which is why it looks incomplete but basically this is printing out the um individual chunks which have the chunks of the tool call in it so there's no content but there's the chunks of the tool call and that's what we're printing out here that covers streaming with agent executor there's one more thing I want to go over which is streaming with L graph L graph is a package that we introduced that makes it really easy to create uh agent run times as graphs and or state machines essentially the same thing and so because Lan graph is built on top of Lane chain and is uh runnable at the end of the day it has this exact same interface so if we go over to L graph we can see in example of this um let's also restart the kernel here we can create our tools we can create our tool executor which just runs tools we can create the model again we set streaming equals to True um for for chat open AI um we bind the functions to the model so it knows which tool it has available we're now creating the nodes in lingraph if you aren't familiar with this I'll also link to a video for lra so you should check that out we just did a whole series of them on on YouTube We Define a bunch of our nodes here uh um again we'll cover this in a video separately or it's already been covered in a separate video we Define our graph here um and then this app that we get back this is a runnable like any other thing that's constructed with Lang chain expression language so we can start taking a look at it and uh uh using it in the same way so we'll use a stream events we'll we'll look at the type of the event if it's a chat model Stream So if it is a uh token basically we'll do the same thing where we print it out if it's not empty um and then we also print out the on tool start and the on tool end so we'll we'll run this with the inputs what is the weather and SF we can see that we get back or it's logged the the starting input for the tool we get the output and it start streaming back the response um of the tokens there that's basically all I wanted to cover in this video streaming is super important to llm applications and so hopefully this new stream event method will make it really easy to stream back what is going on inside your applications to the end user again this is in beta so please let us know if you have any feedback or questions thank you", "metadata": {"source": "ZcEMLz27sL4", "title": "Streaming Events: Introducing a new `stream_events` method", "description": "Unknown", "view_count": 2592, "thumbnail_url": "https://i.ytimg.com/vi/ZcEMLz27sL4/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AHUCIAC0AWKAgwIABABGGQgZChkMA8=&rs=AOn4CLBZCZLx203dWs9DKzqrJY3Lvxpu6A", "publish_date": "2024-01-24 00:00:00", "length": 1272, "author": "LangChain"}, "type": "Document"}, {"page_content": "hey everyone this is Harrison from Lang chain here I wanted to do a video on Lang graph which is a new library we released um pretty recently and how you can use l graph to create some multi-agent workflows so for those of you who aren't caught up on L graph we have a whole series of videos going through it and it's basically a way to dynamically create agent-like workflows as graphs so what do I mean by that by an agent-like workflow I I generally mean running a language model in a loop in a variety of ways um and there's different structure and how you might want to run that language model in a loop and so L graph is useful for defining that structure and specifically it allows for the definition of Cycles because this is a loop after all um another way to think about this um is thinking about it as a way to define kind of like State machines um and and so a state machine machine and a labeled directed graph are pretty similar and that you have different nodes and if if you think of this as a state machine then a node would be a state and if you think of it um as as uh uh kind of like in this multi-agent workflow then that node state is an agent um and then you have these edges between them and so in a graph that's an edge in a state machine that is uh you know transition probabilities and when you think of these multi-agent workflows then those are basically how do these agents communicate to each other so we'll think about building multi-agent workflows using L graph the nodes are the agents the edges are how they communicate um if you haven't already checked out L graph again highly recommend that you check out the this the video that we have at a high level the syntax looks like this it's very similar to network X you define a graph that tracks some State over time you then add nodes into that graph you then Define edges between those nodes and then you can use it um like you would uh any other um L chain chain so today I want to talk about three different variants of multi-agent workflows that we're going to use laying graph to create so the first one is what we're calling multi-agent collaboration and so this is largely defined as you have multiple agents working on the same state of messages so when you think about multiple agents collaborating they can either kind of like share State between them or they can be kind of like independent and work on their own and then maybe pass like final responses to the other and so in a multi-agent collaboration they share the state and so specifically this this example that we've set up it has two different agents and these are basically prompts plus llms so there's a prompts plus an llm for a researcher and there's a prompt plus an llm for a chart generator they each have a few different tools that they can call but basically the way that it works is we first call the researcher uh node or the researcher agent we get a message and then from there there's three different ways that it can go one if the researcher says to call a function then we call that tool and then we go back to the researcher two if the researcher says a message that that says final answer so we in the prompt we say like if you're done say final answer so if the researcher says final answer then it returns to the user and then three if it just sends a normal message there's no tool calls and there's no final answer then we let the chart generator take a look at the state that's accumulated um and kind of uh respond after that so we're going to we're going to walk through what this looks like um we're first going to set up everything we've need I've already done this um importantly we're going to be using Lang Smith here to kind of like track the the multi-agents and see what's going on for for a really good debugging experience um Lang Smith right now is in private beta if you don't have access shoot me a DM on LinkedIn or Twitter and we can get you access so the first thing we're going to do do is we're going to Define this helper function that creates an agent so basically we're going to create uh these two kind of like agents up here um and these agents are parameterized by an llm the tools they have and a system message and so then we're taking kind of like uh we're basically creating this mini chain this very simple mini chain that is a prompt which has uh the system message in it um and then it's a it's a call to the llm and it has access to some tool so let's define this helper function we're then going to define the tools that we want the agents to be able to call so we'll have one Search tool we'll use tavil search for that and then we'll have one python tool that can run python code um now we can start to create our graph so first we're going to define the state of what we want to track so we want to track the messages and so again like each agent will add a message to this messages property and then we also want to track who the most recent sender is and the reason reason we need this info is because after we call this tool we're going to check who the sender was and we're going to return to them so this is just some some State that's useful to track over time um so let's define this state um we're now going to Define some agent nodes so here we've created a little helper function to create an agent node and so specifically this agent node is going to uh take in state agent name it's going to call the agent on the state and it's then going to look at the uh uh result we need to do a little bit of kind of like converting here so the agent is going to respond with an AI message but when we add that to the messages States that's accumulated over time we want to represent that as a human message actually um because we basically want the next AI that sees this to kind of like work with that and so we're going to if it's a function message then we're going to represent it as a function message because it will just say a function message otherwise we're going to cast it to a human message and we're going to give it a specific name we're going to give it the name of the agent so that it knows basically is this message that it sees again because we're keeping track of This Global State and so in the global State when it sees a human message is it from this researcher or is it from this charge generator um and so we're going to do that and we're going to create two nodes we're going to create this research agent node um by basically partialing out the agent node function um and then we're going to create this chart generator node we're now going to define the tool node this is the node that just runs the tools it basically looks at the most recent message it loads the tool arguments it kind of it will call it it will call the tool executor the tool executor is just a simple wrapper around tools that makes it easy to call them and then it will convert the response into a function message and append that um to the messages property and now we need the edge logic so this is the this is the uh big router logic here um which basically defines uh some of the logic of where to go so we look at the messages we look at the last message if there's a function call in the last message then we go to the call tool or then we return call tool and we'll see what that leads to later on if there's final answer in the last message then we return end um otherwise we return continue and so let's see how we use this in the graph so first we create this graph let me run this m first first we create this graph with the agent State then we add the three nodes that we have the research node the chart node and the tool node um we then add this conditional Edge so the researcher after we call the researcher um it uses the router logic if if the router returns continue then we go to chart generator so if the researcher returns basically something that doesn't have a tool and something that isn't final answer then we return continue in which case we go to the chart generator if the router returns call tool then we call the tool and if it returns end then we end similar with the chart generator um and then we add this conditional Edge after call tool so after we call the tool we basically look at who the sender is and if the sender is the researcher then we go to the researcher if the sender is the charge generator then we go to the chart generator and we set the entry point to be the researcher node now we can invoke it um we'll use the stream method so that we can see things being printed out um we'll pass in this human message asking it to to fetch the UK's GDP over the past 5 years then draw a line graph of it um and then after we code it finish it um and we'll run this it'll take a little bit so I'll probably pause the video here or I'll kick off the I'll kick it off pause it come back when it's finished and then we can see what it looks like in Lang Smith as well all right so it's finished so it was streaming out so we can take a look at the whole stream of things in here if I can figure out how to expand it so we can see the researcher um you know the first thing it does is it queries UK GDP data for the past 5 years and then calls a tool um and then it kind of keeps on going um at some point there's this chart so you know it PL plots it over the past uh three years it looks like so it didn't quite get all the all the data it needs so you can see uh yeah you can see it says that the data for years 2018 2019 is missing um and so it plots that and then this is just the final response so that's why it shows up so big if we want to see this in a little bit nicer view because this is a lot to digest we can go to Lang Smith um and so Lang Smith uh at the top of the notebook I sent I set it so that it would log to this specific project the multi-agent collaboration I can see that I've got one run can see some basic stats on it like it took over a minute it's got some tokens on it um if I click into it I can now see exactly what was going on under the hood so first it called the researcher and we can click into here and we can see the exact call that it made to open AI um and so there was this system prompt and then uh this was the user's first message and then output this it then called toil um which is the Search tool we can see the results there it then called the researcher again we can see that you know there's now more in it because we start to build up this message Bank we can see it calls toil again okay so now the researcher and it doesn't call toil okay interesting let's see what's going on here so at the bottom it basically says okay so now it's uh it's responding what the values are um and there's no tool call and there's no uh uh thing saying that it's finished so that means that it goes to the next one which is the chart generator so it calls the chart generator under the hood that calls open AI um we can see that down here we get a function call and this is running python code so it prints this out calls the python reppel um uh we can see that it runs this um the chart generator then goes um and the chart generator says uh you know here's the line graph see the chart above the researcher then goes and it says final answer so now that there's final answer it's finished and that's basically what happened um so that's the multi-agent collaboration bit the the kind of like specific thing about this that's interesting is it has This Global state of messages that each llm sees and a penss to so it keeps on adding to this over time and so on one hand this is good because it allows each agent to see kind of like exactly the other steps that the other one is doing and that's why we called it collaboration it's very collaborative the other version of this is that you have agents that have their own independent scratch pads um and so we'll take a look at the next multi-agent example that we have which is this agent supervisor so here we have the supervisor which basically routes between different independent agents so these agents will be Lang chain agents um so they'll basically be an llm that's run with the agent executor in a loop the llm decides what to do um the agent executive then calls the tool goes back to the llm calls another tool goes back to the llm and then then finishes and uh then there's basically the supervisor the supervisor has its own list of messages and essentially what it will do is it will call agent One agent one will go do its work but it will return only the final answer only that final answer is then added the supervisor only sees his final answer and then based on that it can call another agent or it can call another agent another way to think about this is that this is essentially um you know the supervisor an agent and it's got a bunch of tools and these tools themselves are agents so it's a slightly different framing than before it's not as collaborative they're not working on kind of like the same sections so let's get this started um again we're going to be uh I'm going to let me restart my kernel we're going to be using Lang Smith um so again uh hit me up if you don't have access to that um first thing we're going to do is we're going to create some tools we'll use the same two tools as before the tavil tool and the python tool we'll next Define some helpers again to create these agents um as the individual nodes so we have this create agent function similar to last time the difference is now that it's actually doing a little bit more work under the hood so it's not just a prompt plus a language model it's actually creating an open AI tool agent which is an agent class that we have in Lang chain from there it's creating an agent executor which is also in Lang chain and we're returning that that's the final agent so let's run that then what we're doing is we're also creating similar to last time this agent node thing which does this human message converion just like we did last time is taking the agent result um which is an AI message and converting it into human message with a name so we know kind of like where it's coming from now we create the agent supervisor so this is going to be the the you know system that's responsible for looking at the different agents that it has and deciding which one to to pass it on to so here you can see that we're defining a bunch of stuff we're defining system prompt your supervisor task with managing conversation a between the following workers and it's basically using this function definition to select the next Ro to send things to um and uh it's or it's selecting finish finish is another option we're creating this prompt template we're creating this llm and we're creating this supervisor chain um which basically has a prompt um it's got an llm with some functions that it can call specifically we're forcing it to call this route function where it selects the next best role and then we're pressing the the response in Json um and we're using that determine where to go to next now that we've got that we can create the graph so we've got this agent State um which is messages and then a next field um we've got this research agent so we create the agent we then create the node for it so we partial out agent node we then do the same for the code agent um and then we start creating the graph so we create the graph with the agent State and we add the three nodes we now connect the edges in the graph okay so for all the members um and again if we look at where members is defined members is defined up here so it's the researcher encoder it's two it's the two basically sub nodes or sub agents um so for those after we call those agents we go back to the supervisor um then we also have this conditional Edge so from the supervisor um after the supervisor um is run we look at what's in next and we look in the conditional map and the conditional map is basically a mapping between the the members so uh and and finish so so you know if if it says researcher then we go to research node if it says coder then we go to coder node if it says finish then we finish we set the entry point to the supervisor and then we run the graph now we can do the same thing we can invoke the team um we can uh see what happens um and see how it performs all right so it runs it um you can see that it calls the coder so here we're asking it to print out hello or to say hello world and print it to the terminal so it calls the coder and then it finishes here we can write a brief research report on pasas and it will do that while that's running um we can maybe look in Lang Smith um to see the previous ones so here we have the coding one um we can see that we first called the supervisor agent it's got this call to open AI um again it's got the system prompt and then the human um and then it decides what to do next and it's calling it's using this function call we can see it has this function definition of Route here based on that it goes to the coder so now there's the coder agent the coder agent does several things under the hood it calls open AI first um it does a function call it then calls the python reppel um and then it calls open AI again and so when we go back to the supervisor if we look at what the supervisor see says it just sees the response from the coder so there's a human message with the name coder and it gets this response it doesn't see any of these calls though or any of the calls to the python reppel and then we get a response and it says to finish so this is an example of where you know there's many calls but they're kind of hidden within one agent State we can also see that it finished writing the research report on pasas so if we go back here we can see that it's finished running and if we take a look it now calls the researcher agent under the hood and we can see here if you know this is interesting because this took a pretty long time 45 seconds we can quickly drill in and see that the the longest call was this call to open AI um inside the researcher agent where basically it uh uh has it has all these uh uh information from the internet and it writes this long research report so that's pretty much it for the agent supervisor now I want to talk about the third and final type of agent that we have an example of which is the hierarchical agent team so this is very similar to the previous concept it's just changing what's going on in the uh uh it's just changing what's going on in the sub things so now each agent node is itself a different kind of like supervisor agent setup and so what this means is that it's going to be a lot more kind of like a l graph um and a a a little bit less of L chain we can get started by setting up the environment similar to last time we now create the tools so there's two different sets of tools that we need to create one's for the research team it's the Searcher and web scraper the others for this document authoring team and it's these three tools here so we can do that we can load to villy we can create our scrape web pages tool and then we can start creating these uh other tools as well um and these are for the writer team so we create one that creates an outline read documents write documents edit documents um and then a python reppel tool as well we we're going to create a few helper utilities that make it easier to basically construct this pretty complicated graph so we have one that creates an agent we then have an agent node um and we then have this team supervisor idea and so these are kind of like taking a lot of the concepts that we had in the last notebook and extending them now we can Define these agent teams they're hierarchical teams so we have this research team This research team has a few different attributes on it the main things here are this messages um we create uh we initi in llm we create the search agent and then the search node the research agent and then the research node and now we have the supervisor agent as well we now put this all together um I'm not going to go over all the edges because it's very similar to some of the notebooks that we did before but the end result is that we have this uh uh graph that we create in order for this graph to be used in as a subgraph in another component we're going to add this enter chain function which basically just is just a converter to make it easier to use we can use this directly um so we can try out and again this is just one of the sub teams but we can try asking it a question um and we can see that it does a search it has a search to so it'll get a response from there and it will give us an answer we can see that we get back a response here and if we look at that in lsmith we can see that it's doing a pretty similar thing to before um it's got the supervisor agent it's calling search and then it responds with supervisor we're now going to put together another agent or another graph of Agents so it's this document writing team um so here we are defining a bunch of stuff we're defining the state that we're tracking it's largely these messages um there's then uh some logic that's run before each worker agent begins um so that's more aware of the current state of everything that exists this is basically with populating relevant context we then create a bunch of agents and a bunch of uh uh agent nodes um um and then we create the dock writing supervisor um now that that's done we can create the graph so we're adding a bunch of nodes we're adding a bunch of edges we've got some conditional routing edges um we set the entry point um and now that we have this we can do something simple like write an outline for a poem and write the poem to disk we can see that this is finished and if we look at this in Lang Smith we can see that it also looks fairly similar as to what we were doing before we've got the supervisor we do a bunch of note taking and then the supervisor finishes it doesn't see any of the internal things it just sees the final response and we can now put this together so we now have a layer on top of this we've got this team supervisor um and now we have this top level graph State it's just a list of messages we have some helper functions we start creating the graph we add the research team as its own node it's got a little bit of uh it's got a little bit of wrappers around the previous chain to like get the previous message and then p and then at the begin beginning pass it that into the chain and then call join graph to join it with the rest of the stuff same uh uh with the paper writing team we then add the supervisor we then add some nodes we compile it all right um and so now we can uh do a more complex thing where we call asket to write you know this brief research report on the North American sturgeon include a chart so let's kick this off and I'll come back when this is done all right so it's done took a while if we go to Lang Smith we can see that we have this most recent Trace took 400 seconds if we click inside it we can see that there is a lot going on so a lot of calls um down the hood it looks like there's this big call to the paper writing team at one point that took a while there was also this big call to the research team to start that took a while I can click into any of these I can see like what exactly is going on at any point in time what exactly was happening um so this is pretty helpful for understanding what's going on um yeah wow that's a lot of tokens um anyways yeah that's pretty much it for the multi-agent stuff hope this was helpful I think uh you know the the benefits of thinking about things as a multi-agent way are really that it I think it helps compar compartmentalize what you expect different functions to be happening inside the system so you know each agent basically has its own prompt its own tools even its own llm at times and so you can really focus it on just doing specific things as opposed to having one General autonomous agent with lots of tools and so even if you you know even if you have all those same tools and you just organize it a bit more hierarchically that can often be helpful for for doing kind of like complex tasks as well as just a good mental model to how to think about things so hopefully this has been a good introduction to that these are again probably only a few of the ways that you can create multi-agent workflows with L graph really excited to see what other ones emerge", "metadata": {"source": "hvAPnpSfSGo", "title": "LangGraph: Multi-Agent Workflows", "description": "Unknown", "view_count": 47591, "thumbnail_url": "https://i.ytimg.com/vi/hvAPnpSfSGo/hq720.jpg", "publish_date": "2024-01-23 00:00:00", "length": 1441, "author": "LangChain"}, "type": "Document"}, {"page_content": "hi this is Lance from the Lang chain team and today we're going to be building and deploying a rag app using pine con serval list from scratch so we're going to kind of walk through all the code required to do this and I'll use these slides as kind of a guide to kind of lay the the ground work um so first what is rag so under capoy has this pretty nice visualization that shows LMS as a kernel of a new kind of operating system and of course one of the core components of our operating system is the ability to connect like your CPU or in this case an LM uh to dis or in this case say a vector store some kind of data storage that contains information that you want to pass into the llm and llms have this notion of a context window which typically pass in like say prompts but of course we can also pass in information retrieved from external sources now one of the most popular external sources are vector stores and these have really nice properties like the ability to perform semantic similarity search so you can take a kind of a chunk of text and perform similarity search based upon kind of embedding similarity with index chunks in your vector store so this is like a very popular Paradigm for retrieval with LMS but it comes with some pain points so in particular one is the need to provision your own Vector store um now some providers like pine cone for example do have hosting which has become really popular but even with hosting um in Pine Cone you do have to pay for your index and that is a kind of a monthly fee that is not a usage based fee so as of the recording of this video pine cone has released pound uh this new service called pine cone serverless which addresses these two pain points it offers unlimited scaling with kind of cloud storage GCS RoR um as well as uh uses based pricing so promis to be significantly cheaper so this is something that I'm actually personally pretty excited about um after having used pine cone for quite a few projects so we're going to use pine cone in this demo and connect it to a rag app and we're going to show how to deploy that um and how to work with it now some disclaimers we're going to be using Langs Smith and hosted Langster for this so both are in private beta but you can get access um you can feel free to DM me or other folks in Lang chain team if you want access and also we'll share these slides and I share some some relevant documentation down here now let's kind of go to the coding section and first you can kind of see here we're going to create our environment I'm using anaconda and we're going to install a few things so this is the Lang chain CLI and also Lang serve so I've done that over here I've already kind of done those steps so we you can see our environment is here and we have these packages installed now what you see right here I'm going to run Lang chain app new so this is actually going to create a new and blank Lang serve app and we don't actually need to worry about this quite yet this is going to help later when we think about deployment but for now what we're going to do very simply is just install a few packages that we want to work with um okay so this is the pine cone client that has server list enabled um and you can see there's an issue related to python version so we can actually kind of go here and this empty Lang project is actually managed by poetry and we can actually look here and we can say okay uh I want to change my Pyon dependency to match what Pine client needs no problem so that'll go ahead and install and we can go ahead and add these other packages accordingly good good add Jupiter and just uh in for environment variables um okay so Jupiter is still installing good so now you can see something that's kind of cool when we run these commands poad we can see that our toal file has these new uh packages added to it automatically so our project now has a bunch of useful packages that we want to work with so that's great now typically when I'm building a rag app I start with notebook um so for example you like to use Jupiter notebooks obviously really convenient um and we can see here that let's spin up a new Fresh notebook great we'll select a kernel no is this is nice so we've created a new Lang serve empty project we've added a few packages we want to work with we spun up a Jupiter notebook so next I want to walk you through the process of actually um kind of prototyping some rag code in our notebook now I'm going to start with some code they I've already pre uh you know used previously just to kind of ease things a little bit and save time but we can kind of walk through what's going on so first things first we're going to perform a bunch of imports that's time fine and this is actually an interesting point so the folks from Pine con have actually created a serverless index already and have given me access to it by providing a pine con API key environment and index name so this I already have I've set these keys and I'm just simply defining them here and one other thing I want to highlight is that this index was created from this data set which is a dump of Wikipedia available on hugging face with coher embeddings so I'm going to make kind of make a note of that this is our data set uh right here so we have the serverless index which contains this very large data set of of Wikipedia dump that was provided by pine cone we're going to initialize our pine cone client and right here you can see that we're actually defining our embeddings as well as the vector store um so now we've initialized our serverless index we have access to it and now it's a lang chain retriever which we can use now the second piece here is when rag apps we typically Define a prompt something like this which is basically saying answer the question based upon only the following context if this variable for context and you have this variable for question okay now let's define our model so here's our llm we're going to use open Ai and this is a recent large context llm uh that has 128k context window so that's pretty cool now this is where it's kind of interesting and I want to kind of walk you through this a little bit carefully so Lang chain expression language allows us to compose elements like you can see we have a retriever defined we have a prompt we have a model we can kind of compose these very easily into a chain okay so we're going to call this L cell chain and what's happening here very simply is that we are using our retriever to extract context relative relevant to our question so what's happening behind the scenes is when we run this chain it's going to take the question uh perform similarity search using that question on a retriever and of course we need def find their embeddings of choice so it's going to be using coher embeddings they'll take the question embed it using cohere perform similarity search on or Retriever and then those documents that come back are going to be passed to this context key in our map here in addition a question is going to be passed through from the user to our map and save to the key question then this will be plumbed into our prompt which you indeed see expects question and context we'll then Plumb that to our model and then we'll clean the output so let me Define this and okay let's make sure we have everything defined here good now what's nice is this chain is a lang expression Lango job object which means that it has a a runnable interface with a few invocation methods that are consistent for any line chain expression language object so we can call like stream we can call batch and we can call invoke and we can invoke with a question so again this is Wikipedia contains lots of information let's just say what is film Noir like it's kind of a completely random question right and now this is spinning now what I like to do here is I like to look kind of look behind the scenes and see what's actually happening and so to do this I will pull up lsmith um now I mentioned previously in the slides back here that I have already set my lsmith API key and I've already become a user of Langs Smith so what happens when I go to uh just Langs smith.com right here um I let me just make sure I'm in the right user okay so let's go to my personal user this is what you see when you kind of go to Smith online train.com I can click on this and see all my projects and this rlm is just kind of my default project I can click here and I can see something kind of need see this question right here this is exactly what we asked right here what is fil Noir and the trace associated with that chain is now logged automatically to Lang Smith this is pretty nice so I can then inspect what's going on I can look at this retriever object and I can actually look at the documents that have gotten retrieved from that query and they all are related to this question was from Noir but we can see something kind of interesting the chunk size is variable and I was kind of intrigued by this I wasn't sure what's going on of course this data sets from a third party provider we didn't actually make it ourselves um so there's possible that there's some irregularities in the data itself um but let's go back we ran our chain we can see our answer you know it looks sane we can check our chain here so again here's the retrieve chunks from from our serverless index we can go look here and they're all plumbed into our prompt so that's pretty cool and here's our answer now I noted that the retriev chunks look a little bit weird to me so some of these are kind of small so let's try something that's like kind of a nice trick I'm going to show you a a kind of fun page I like so this is a blog post we wrote kind of a while back talking about Rag and the various approaches to improve rag um and I'll of course share this link but if we look at this diagram here there's a few different places that we can think about kind of improving our rag chain so initially we can think about like if we have a raw user question we can transform it in some way um we can query you can use routing to query different databases we can use Query construction if you want to go from like natur language to SQL or Cipher Cipher for graph databases there's all sorts of tricks you can do when when doing indexing but I want to draw your attention to postprocessing so this is like you've retrieve the chunks and you do something to them prior to passing into the llm so it's postprocessing is like a really nice area to think about because it's given a set of chunks what do you do with them to enrich them and we can actually look here we can see these retrieve chunks and each of them has this URL in the metadata and let's just say we wanted to expand the context so like for every chunk instead of just this little subsection of the document let's go ahead and expand that out what if we could go to that URL grab the whole page and just pass the whole page they this it's kind of a way of like um you can think of this kind of as like a context expansion so from these smaller chunks I want to go ahead and fetch the full page now we can do that and we actually have some code for that already written um so that's you're going to see that right here um so all that's doing so again we talk we saw our chunks that's captur in this context variable and then for each chunk we're going to get the metadata like the URL we'll split it to get get the ID so you can look at these URLs you can see the ID is the thing that's different between each one get the IDS as a list and then call this Wikipedia method for each one and just get the raw page that's all that we're doing now what's kind of nice then is we can see redefine our chain here let me move this around so here's our chain and we can do is we can basically call this function in our chain on the retriev documents using this runnable Lambda so we're piping our documents to this function the function is then processing each one as you can see here and it is extracting the full context uh from the URL and we're going to pipe that into the prompt so let's have a look at what that looks like there we go run invoke and it is running now so we can go back to lsmith and we can have a look so you can see it's running that's great um now we can look at the retrieval okay so it's the same as before that's exactly what you expect now let's see if this is finished okay it looks like it is done or at the very least it looks like the prompt is here and the output is still getting generated so that's fine but let's look at the prompt quickly so here you can see um that these look like full Pages actually um and you can see there's some HTML in there that's kind of fine um okay and it looks like it is completed now so let's actually maybe can refresh this uh okay there we are there's our answer cool so yeah it looks like we successfully able to extract the full pages um and pass all this context in this is kind of a means of like post-processing for context expansion that's what we did and the answer again is kind of sane you can see it's obviously kind of passed a lot of information that LM is very very good at processing and synthesizing as we all know so that's great and that's kind of a nice way we've taken our rag chain and we have um kind of done some minor post processing to expand the context to get better answers now let's say we want to do a little bit more here um let's kind of go back we look at our repo so not much is here we just have a notebook and let's just call this notebook like prototyping Proto right now let's say we want to actually like deploy this as an app okay now and let's say this app is going to look something like this so Lang serve is a really nice way to take any change as we just defined in Lang chain expression language and converted into a web service so each of those invocation methods you remember we use like chain. invoke these are all mapped to http endpoints in a web app and the whole thing is going to look just like this we have our serverless index it connects to our rag chain we connect it open Ai and this will be a little web app that hosts uh basically our chain and can and can uh EXP expose these htpn points that map to our invocation methods and we can access that using a lot of different things then curl requests playground and so forth so let's go ahead and do that and I will take one very quick shortcut just to make this a little bit faster for you guys what I'm going to do is I'm going to create a new file called chain. piy here and I'm just copying this is exactly what we had in our notebook but I just have it kind of nicely formatted all ready to go um and here it is you can see it's the same logic here's our fetch Wikipedia stuff fetch URL here's our chain it's all defined now one interesting thing is it we put this in the app directory just next to server.py we defin this new chain. piy and this has our custom chain logic now here is the Crux of it if we go to ser. and this is a template example that uh I've actually already done but I'm going to I'm using it just to highlight what's going on here what we can do is import from chain the chain and we'll call it whatever we want and now what we're going to do is this add routes thing which I'm going to explain in a minute that basically connects uh our chain to the web app and in particular let's have a look briefly here at what is going on so add rout and server.py creates HTP endpoints that expose the methods of the rag chain on our web server that's really what's going on so that's pretty neat now that's cool that's all we had to do I'm close the notebook down let's just make sure everything's installed install good cool so that's all set so again all we've done is we've taken our logic that we prototyped in the notebook we've added to chain. piy then we have server.py we imported the chain so again go back here chains here we imported that and we did this add routes thing which connects that chain to our web server endpoints um that's great now all we need to run at this point I'll just show you here is line chain. serve and this should spin up a little look at this very nice so it uses fast API and we can see the invocation methods of our chain are now HTTP endpoints in our app so that's pretty cool and what we can do which I really like is we can use a playground to interact with it so here we are now this is just another way of interacting with our chain again remember in our notebook we prototyping in the notebook let's try the same question here what is s newr now this should just work this will interact with our app in the exact same way um so it's running now and we'll we'll see once that starts uh generating text um Okay cool so start streaming we can see it's producing answer probably pretty similar to we saw in our prior testing which is pretty nice and so that's great if we go back to the code again we prototyped this in a notebook we then took the logic we want and create this chain. piy file which basically just encapsulates our our chain we very simply just add it to our app using add routes we import it here add routes and that hooks up the invocation methods of that Lang expression language object to the HTTP endpoints of this Lang serve app and then we can see the endpoints all uh here these are endpoints and then we now have an interactor playground here so that's great so we've G part of the way where we've gone from prototyping to um a web app that's running locally we can even take this one step further now this of course is optional um and you know hosted L server is still in private beta so not everyone will have access to this currently but again DM me I'm happy to kind of get you access if you're interested um I've taken one shortcut where I've actually taken the logic that we see in this repo um and I've pushed it to GitHub so we can see here we are in this GitHub project um we can see it's the same code so nothing different here right now now we can do something that's pretty cool we can go to Langs Smith and you can see this deployments tab now of course this is my personal account I says to sign up here and this is the process I'm happy to kind of give you access to I actually already did this so if I go to my Lang chain org I look at deployments um okay so they're loading um but I can click this new deployment and this I can just very simply ENT enter the URL from um GitHub accordingly name on deployment and I can then run it um add my environment variables submit and then this will be deployed that's really all there's to it so again here I can set set different environment variables I can name my app just add the repo and you're done and you can see right here we have a bunch of existing deployments and I'll show you we actually have this pine cone app from this same this repo we basically just Rec re recapitulated it's already running if I click on this deployment link it actually should take me to um a page that looks very similar we we saw when we running locally it'll be those same um here we go very nice so you can see it's the same invocation methods um but now simply on this hosted app as opposed to running locally and in the same way we can actually run our playground through this hosted app very good nice so this is smoke playground look it looks the same what is film and again this is connecting still with our serverless index but now this is you can see this URL is just hosted on the web this is not running locally on my machine and this can be shared with kind of anyone so it's kind of a true like you know hosted application that just runs out in the web it streams um so that's really kind of the nuts and bolts of this going from all the way from let's kind of go all the way back up the stack what is rag um setting up a project that uses Lang serve now initially we didn't care about Lang surf we just did prototyping in a notebook so that's great that's easy we defined our chain and look in a lot of cases you only ever wanted a prototype and that's completely fine but what's kind of nice is with Lang serve if you defined your chain using L expression language you can just take take this and just run it as a web app really out of the box and we showed that because we can just Define this chain um and this all we have to do Define our chain we import it add routes here and then the invocation methods of the chain are just HTTP end points in our app and we can call it accordingly using our playground or we can then even go further and host it um as a managed service and it runs on the web uh you can see right here this are hosted app so that's about it I just want to kind of give a quick walk through um and maybe give kudos or folks from uh or friends from uh from Pine Cone for you know the release serverless should be really cool um I've definitely had some pain points with you know pine cone and basically paying for indexes I never use and so I'm really excited for this and uh hopefully this shows you how you can like take an index and very easily kind of build a build an app that can run locally or you can host in the web really easily um and um yeah I'm happy to answer any questions we'll make sure this is all public thanks a lot", "metadata": {"source": "EhlPDL4QrWY", "title": "Build and Deploy a RAG app with Pinecone Serverless", "description": "Unknown", "view_count": 8591, "thumbnail_url": "https://i.ytimg.com/vi/EhlPDL4QrWY/hq720.jpg", "publish_date": "2024-01-16 00:00:00", "length": 1444, "author": "LangChain"}, "type": "Document"}, {"page_content": "all right let's get started this is the opening eye prompt engineering page before we can start building with it though we need to set things some things up so I've created a f a fresh uh virtual environment what I'm going to do is I'm going to set up a l serve uh uh template a l or application with this the reason that I'm going to be using lay serve is it'll make it easy to deploy this once I finish creating so the first thing that I'm going to do is bootstrap a link serve project I'm going to do that with the Lang chain CLI I'm installing it here after it finishes installing I'm going to create a new app I will call it open AI prompter I am not going to add a package because I'm going to be creating my own I can then go inside it and if I open it up the main thing that I'm interested in is this Lang serve server right here which wraps around fast API and will make it really easy to deploy my Lang chain chain the last thing I'm going to do for setup is set up laying Smith so laying Smith is in uh private beta if you don't have access to it and you're seeing this and you want to follow along shoot me a DM on LinkedIn or Twitter and I will uh get you access to it um what it will make it really easy to do is debug as we go along and this will involve a lot of prompter engineering and so we'll see how that becomes very very helpful um someone in a previous video commented that I should go over how to set this up more so you can go to your projects let's even create a new project um let's call this uh open AI prompter um that's all we can do let's then go to setup and then we can just export some API Keys here um so you can just copy this you can paste it here you then need to add in your API key you can do that let me let me move my face over here and let's move this over here as well you can do that by going to API keys and all right good it's not showing mine so you can create an API key and then put it there um I've already did this ahead of time um so mine's all set up but if you needed to do that this is how you do that all right so let's go back to this prompt engineering guide the first thing we're going to do we're just going to copy it all because we are going to be using this in our prompt to help us write better prompts let's paste it into a file and then I'm going to I'm going to create my chain in a Jupiter notebook and then I'm going to put it into uh and then I'm going to put it into this app and so the reason that I'm doing this there's uh there's there's one real reason and one fake reason the one real reason is that a lot this will it' be a pretty iterative process I'm going to do some prompt engineering some of that I'll do in Lang Smith but some of that I'll I'll do in uh The Notebook as well and we'll see why um and so having a notebook like environment is really really helpful because it's an iterative environment and I can really easy iterate the other reason I want to do this is I want to show how it's really easy to create a chain and then just export it from a notebook most of the time it's it's pretty simple so let's save this and now I've got this jup notebook let's load um what I just put in there all right so let's do that let's import uh we can print out the head I guess just to see it let's import some stuff from L chain that we're going to want so from L chain core. prompts import template from L core do output parsers string output parser from L chain Community do chat models import chat open AI so I'm importing three important things this is going pretty simple chain that I'm writing we'll see maybe we'll get more complex but it should just be pretty simple because I'm just going to use a model that has a long context window and can work with all these instructions so I'm importing a prompt template this will help me uh structure the inputs to the model I'm importing an output parser that'll basically just convert it from a message format which is how the newer models respond the chat messes respond with uh message but I I really want to set it's string so I'm getting that and then I'm importing chat opening app which is the L chain wrapper around the open AI models which are the ones that I'll be using now is the fun part where I'm going to write my template for creating or my template that's going to help me create a chain that can take in an objective and write a good prompt so let's add this there um and okay so so this is actually an interesting point that I'll get to later but basically what I'm doing right here is I'm going to have these instructions as a variable that I'll pass into the prompt and I'll I'll go into that more later but for now let's just assume that's what we're going to do and then let's add just some delimiters here and then let's say based on the above instructions help me write a good prompt I want a prompt that okay so what I actually want to write is not actually a prompt but I want to write a prompt template because I think most people write prompt templates because they want to write prompt templates that then they can use in their code or at least most linkchain users want to write prompt templates help me write a good prompt template this template should be a python s string it can take in any number of variables depending on my objective return your answer in the following format this is my objective this seems like a decent first pass I don't know we'll see um and then we're going to create the chain okay so as I mentioned we have this thing here um which I'm going to pass in the instructions to let's see what would happen if we didn't do that and if we just did like this which you could also easily do so if I did this I would have prompt equals prompt template from template template chain um equals prompt Shadow AI string up the parser chain. invoke and now I'm going to pass in objective what should my objective be um answer B answer a question based on a based on context provided and only on that context so this is a pretty typical rag prompt or retrieval augmented generation prompt you want the the language model to respond to a user question based on the context provided um so let's do this and I get ke because basically what's happening is in these instructions there's other um there's other curly brackets which aren't variables that I want to format they're just part of the text and so this often happens when you're working with code or something like that and we've got a a lot of questions about how to deal with it so my preferred way to deal with it is to do what I'd done earlier treat the the text that has these unwanted curly brackets as input variables and then okay so so one way to do this is you may be thinking okay so that means that I now have to provide text as text here and this is a little bit annoying because now you have two input variables it's not the end of the world but it's a little bit annoying and if if this chain is part of a larger chain then then it starts to get more and more complicated more and more annoying um and ah I had that should be taex this is erroring because the model's not long enough I could do some fancy rag um it's just like barely over the context window and I'm I'm going to want to use this model anyways because this task is kind of hard so I want a good model to do it um so this will just get me around that and then let me also set temperature equals to zero while I'm at it so I could do it this way I could pass in the text here um and this would work let's see what answer it gives um for the next time I'll use streaming um so we can start seeing what's going on earlier um all right it's writing long thing while while it's working okay so let's um this is a little unreadable um because it's not super readable what I'm going to do is all right so this is Lang Smith this is me debugging I want to see the output um this is a all right so this is better output um I get uh oh interesting okay so all right so it's giving me some instructions on how to use it um I actually don't know if this is correct because of these curly brackets but anyways um what we can do is uh uh sorry back to this what we can do there's a lot I want to show off but first things we can do is partial The Prompt so we can now do this and that means we no longer have to pass this in so we're basically just partialing the prompt we're passing in some input variables ahead of time before we even construct the train and then because we know those are always going to be fixed and they're and they shouldn't be variables so we're going to pass them in and then we can do this I'm going to now change this to streaming for Chunk in or more accurately for token in chain. stream print token um all right I need to add something like this all right so there we go it's not exactly um what I really want I really want like just one I really just want this thing um templ should be a python see if this helps so it's not helping a ton I can go back in here and I can I can start doing some prompt engineering here if I want so one thing that I can do it's not really that helpful here because this is a really simple um chain but if I wanted to I could open up the playground and it's also not helpful here because my prompt is really really long and this isn't the grass oh I can okay that's slightly better so now I have this um and um let's let's just do like bunch of new that thing based on the above instructions help me write a good prompt template this prompt template should be a string that can be formatted as if a python fstring I can take in any number of variables depending on my objective um I can now run I can now run this okay okay that's pretty good all right so that's pretty good I'm I'm I'm relatively happy with this so um let's uh take this from the playground let's bring it back in the notebook 24 being a bit slow okay so it's generally doing pretty good I'm relatively happy with this I might do a little bit more prompt engineering by the time this gets out but I'm relatively happy with this for now so what I'm going to do is I'm going to move it into Lang serve now I'm going to deploy it with Lang serve so I am going to create a new file in here I am going to basically copy this chain over that's done I'm going to go into the server um so I'm going to edit this add the chain you want to add um from app. chain import chain chain path prompter cool um I'm going to go back here and let's try it out and see what happens L chain serve doesn't find the file that is right so L chain notebooks open AI prom what did I name it prompting oh right let specify that let's try serving it again and I'm going to poetry add open AI because I'm going to deploy this project later um and this will add it to the Pi Project file so it's going to save that um let's try it again cool so now if I go to Local Host 8,000 prompter playground I get this let's try it out I take this add it here all right so I get back some streaming um I get back some nice stuff okay so there's definitely some prompt engineering that I want to do um it's doing this um Let me let me pause the video a bit do some prompt engineering and then resume it in a second all right so I'm back with a little bit of a better prompt um and I will uh uh I I'll include links to all the code so I won't read out here basically now if I go here and I type in an objective this gets me um this gets me a pretty solid prompt that I'd use for for retrievable augmented generation so so that's basically how we get to something that's deployed with Ling serf um this is still all local and so for the final part of this I'll walk through how to do this um on laying Smith so that you can share it with other people as you see so I have my my personal tenant my personal tenant doesn't have access to uh laying serve deployments yet on Lang Smith so this is uh this is an alpha feature that we're still testing um it's it's it's only available to a few people um if you are interested in being an alpha tester please let us know um or please let me know and uh we are not letting a ton of people off because we want we do want to make some improvements um but as we let more and more people off um you know we'll we'll will uh let me know and and you can be one of them in order to show you briefly what it looks like I am uh going to switch to the Lang chain account I'm going to go to deployments um and you'll see once it loads that I have a bunch of deployments here these deployments are all connected to GitHub um so one thing that I'm going to do after this is I'm going to pause set up a GitHub um uh repo with this code so that I can easily deploy it um so let me pause and do that and then I'll come back and walk through a new deployment and see what happens all right so we're back so I'm going to create a new deployment I'm going to click here I'm going to choose this repo that I created open AI Auto prompter name it uh open AI prompting helper um I'm going to add some environment variables um namely I'm going to add open AI API key um I'm going to make it a secret G to pause while I put this in all right it's in um I automatically get a tracing project for this so that's a big benefit of deploying on Lang Smith is it automatically connects to everything else in Lang Smith um right now that's tracing we'll make that testing um and and prompts and other things in the short term um I can now submit this and it will spin up a deployment um and so here is this uh uh deployment as you can see it's uh it'll take a little bit so I'm going to pause and come back when it's finished deploying all right we're back our deployment succeeded we can now open the playground for this deployment so we can go here task um what what was the task we had way back in the day so let's just use the same task here we can see it streaming awesome so we get this result um one thing we can do is we can give thumbs up or thumbs down I like this one so I'm going to click thumbs up if I go back here I can see it's only one Trace count but I can see the traces for this so here this is the most recent Trace I can click into it this is a tracing from this is the same tracing that's in uh uh langing Smith as you've been using it so that's one of the big benefits as it comes with all this stuff it comes with feedback automatically hooked up so we have a score here um and yeah that's uh basically it for the video now hopefully this was pretty helpful in terms of getting um a pretty simple chain it's you know it's just a a prompt a model and an Alpa parer up and running using lell uh getting around some uh prompt uh annoyances with all the formatting of the curly brackets shwing how to set up langing Smith showing how to set up a laying serve project and then showing uh the new laying serve deployment feature as well hopefully you guys enjoyed this let me know in the comments have a good one", "metadata": {"source": "mmBo8nlu2j0", "title": "Auto-Prompt Builder (with Hosted LangServe)", "description": "Unknown", "view_count": 11934, "thumbnail_url": "https://i.ytimg.com/vi/mmBo8nlu2j0/hq720.jpg", "publish_date": "2024-01-05 00:00:00", "length": 1386, "author": "LangChain"}, "type": "Document"}, {"page_content": "hey everyone thanks for joining um today I'm going to be doing a long form YouTube video on creating a full stack web app which uses Lang chain um and an xgs front end for indexing performing QA taking notes um and overall just an easier interface for understanding papers from archive uh if you're not already familiar with archive it's a website by Cornell uh essentially it's a place where researchers um Can publish their papers um and if you go to for example this archive URL you can click download PDF and then you can easily get access to their paper in a PDF format um so this app is what at a high level what it's going to do is you're going to have a front end you can submit a paper URL and then the back end is going to parse that paper it's going to split up into chunks it's going to hit GPD 4 um thanks to their new 128k context window we can comp confidently fit the entire paper um no matter what paper it is um into that context window we're going to have gp4 take notes on that paper then we're going to then return those notes along with a set of questions which um is essentially prompting our user to say you know hey you can also ask questions about these different parts of the paper um and then from there the user is going to be able to say you know what is XYZ talking about in this paper um and it'll go back to our server our server will then um uh using are we're going to embed the paper you know split up into chunks um using that it'll be able to look up context um and answer your questions based on the context in that paper so you can you know chat with your paper and understand it a little bit easier than having to go through and read the entire paper you know maybe instead of having to read all 18 Pages like in this paper um you might only end up reading four or five pages total and you get a pretty good understanding the paper so to start we're going to have a um typescript monor repo I've already created the spoiler plate essentially what it has is a a web directory and an API directory um and the web is going to be a nextjs app we will get to that later and the API is just a simple typescript app we're going to use expressjs to make it uh to be able to turn into an API um you can clone this from my GitHub I'll have this this template set up so you can you know clone there and start from where I'm starting at uh yeah so API Source directory we're going to have an index file and to start off we're going to create um uh our main function this function is going to take in three things uh the paper URL the name of the paper um and that's just for like UI so you can on the website you can see what the name of this paper is you're you're doing QA over and then a third optional aru argument pages to delete uh this is important because if we go back to this paper we see it's 18 Pages however as we scroll down on page 10 through 10 through 12 so three pages um there are you know references citations and these are um full of texts we don't really care about and often times they have lots of keywords which can um hinder our our semantic search capabilities when we then go and try and perform Rack in this paper so this pages to delete um optional argument is going to be uh the user can specify which Pages they want to remove and then we won't you know take notes on that paper we won't send his GPT we want embed that or or touch them at all so that um we can have a cleaner um embedding so let's start by making an async name we're going to take in three ARG so paper URL name and pages to delete so this is going to be a paper URL that's a string thank you co-pilot name String page it to delete as number but we want the number array who wants to be optional um and then the first step for this function is actually loading the paper from the URL so we're going to use axios um it's going to be we're going to load it as a buffer and return it um and that way um you know it's an easy way to actually load this PDF and then be able to interact with it so let's go ahead and create helper function we'll call it um function load PDF from URL so URL string and then we actually don't want that co-pilot uh let's type the response promise of buffer nice um and here we're going to have to import axios so import axios from axios and declare our um that is what we want thanks co-pilot uh yes what the this what this is doing is it's making an HTTP request to the URL we specified and it's definining the return type as an array buffer um this is we're going to want to wrap this in a buffer. from though just to make sure we actually do get our buffer return um and then inside of our main function we can do a quick check if paper do url. ends with bangs false um PDF we're going to want to throw because that will not be the valid type so throw a new error not a valid PDF all right and then we can call our function so it's going to be const PDF as buffer equals weight BF URL paper URL so what this going to do is we're going to be able to pass in our paper URL um and it's going to load that as a buffer um and the next step is we're going to want to pass that through to unstructured which is a tool with l um or it's it's a component that Lang chain offers uh which will allow us to break this paper down into smaller chunks more manageable chunks that we can embed um we can then convert that back to a string to send a GPT um and it's overall just a really great way of dealing with unstructured data they support you know every most data types PDF you know HTML Pages they kind of do it all um and they've got some some magic that happens in the background which is able to parse his PDF and break it down into pieces like you know head or a body description or you know an image or a table whatnot um and it makes it really easy to deal with these unstructured data formats uh actually for this I made a mistake since we're uh specifying the return type as a ray buffer we don't need to wrap in a buffer it up from and instead we can just do return response.data as you can see our typescript is not complaining about that um and that's because it's any typed uh but we know know that since we're using this response type it will be the right um response type um so after we've done this we're going to want to go through and delete the pages first if a user has specified pages to delete um and we want to do this before we actually use instructure to parse the PDF because we don't want to be parsing Pages we don't care about uh so let's do a quick check to see if pages to delete is true and pages to delete length is greater than zero um and if this is the case we're going to want to call our pages to delete function so let's go specify that up here step one we're going to be using a library called PDF lib um and their PDF document class to actually load um and handle deleting the pages so let's define a new function async function delete pages and this is going to take in the PDF as a buffer and the page is to delete as an array of numbers um let's use this so what they're doing here they are loading the PDF and awaiting that which is what we want um but this is not going to work what we want to do instead is iterate over each page to delete so we need a counter um for the number of pages to offset by and this is because once we remove a page the total number of pages in that PDF is going to change so we need to add one to that number offset by so let num to offset by and we'll start that as one um and then four cons page number of pages to delete pdf doc. remove page page number minus number offset by and then Plus+ and that looks right and then what we're doing here is we're saving the new PDF and returning it it's going to be our unit 8 array um but we want to return it as a buffer again to to match our previous type so we are going to have to match wrap this in a buffer. from to convert this back to a buffer um so now we can go down here and if a user is specified they want to delete some pages then we can delete them here so um we're going to do wait Pages delete delete pages PDF as a buffer and pages to delete and then we're also going to change this to not be a constant um so we can reassign the PDF value there okay so the next step is actually using unstructured to um chunk and pars our PDF so we're going to define a new function call it async um uh let's see convert PDF to documents um and the documents is referring to Lang chain document type um it's the the type we use for specifying documents that we then embed um and that type is used broadly across the lane chain Library so we want everything to be in that type so we're going to specify you know one argument PDF buffer um but our return type is going to be a little bit different we're going to want to return an array of documents um and this usually does not recognize that documents needs to be a type so let's go ahead and import it from link chain so import document that's not right from Lang chain SL document and then we have the proper document type and our typescript will be happy with this and that is obviously not right so a sync function convert PDF to documents um the first step uh unstructured requires an API key so we'll have a check for that if process. in do un unstructured API key is false not at we want to do is false and we want to throw a new error um no API key set and we'll change that to missing un structured API key uh make it a little bit more specific um and then the one one thing with unstructured um this is also why we had to do it um in the back end and we couldn't or as like a know separate API service running on node and not um not in the edge is that unstructured with L chain requires a a a um file path to read from um and if we're doing that in the edge then we can't actually use you know the node file system to write files so the first step is we're going to get a random name um to make set our file name as and that's to avoid you know file name collisions so const random name equals yep that looks pretty good um and the next step is to actually write a file with this random name and our PDF so let's go ahead and import we're going to have to add two Imports so import write file and then also unlink from FS promises which is going to be able allow us to delete our file when we're finished so unlink from um FS promises and we want to use the um you know the promises library because we're using you know nodes async uh yeah so once we've imported these we can go back down to our function and we can call await write file and then the PDF path which is going to be let's set that as PDFs SL random name. PDF um and then we can actually pass in our PDF and we could measure let's set binary as a type once that's set we can call our unstructured loader so we're going to step one we're going to be importing unstructured import unstructured loader from laying chain document loaders uh so down here we can call we can Define our loader function so const loader equals new unstructured loader and we're going to want to pass in our path to the file which is going to be the same as this so PDF path and then also some options so our API key which is that and then also we want our strategy to be highres strategy is not that want to be highres all right so once we've defined our or we've instantiated our unstructured loader class um we can actually go and load these documents and what that's going to do is it's going to look at our PDF it's going to chunk it um I think I mentioned this earlier unstructured they do some cool things around you know identifying what parts of a PDF is are the uh you know the title or like a b paragraph body um and it and it chunks it into these nice parts and also adds metadata um you know identifying which paragraph relates to which title so we're going to load these documents we'll Define a documents loader and uh thank you co-pilot that's how we do it so we call the load method um and this is going to take a second and it's going to load all of our documents and return it to this um variable and as we can see it's the nice Lang chain um document type and then that is you know what goes inside um and then once we load our documents we can do exactly what copal suggested which is unlink um or delete the file and return documents um so let's go over this one more time what we're doing is we're passing in the PDF as a buffer and we're verifying our unstructured API key is there uh we're creating random name to prevent you know file conflicts if you are you know running this simultan ously you don't want to um just have like a static name because two files could try and write the same place and that would you mess everything up uh then we write the file with our random name as a PDF passing it our PDF buffer and uh defining it as binary type um and then we instance get our new unstructured loaded class give it the name um we want to give it the API key and our strategy and that gets returned to this this uh this loader variable and then we can load our documents delete the file and return our documents so if we go back down to our main function we can do con documents equals weight convert PDF to documents PDF as a buffer all right so once we've got to this point um we've done a few things we've loaded our PDF as a buffer uh We've deleted Pages if we you know specify that and then we've converted our our our PDF to L chain document type um and the next step here we're going to want to actually verify this all works so let's you know console log our documents and then this is probably going to be a pretty large log so at the end we're going to log length um so we can see how many documents were loaded and that'll give us a rough idea as to um you know if our thing worked or not so we'll call this main function um and we'll go back to Chrome and use this PDF that I specified here uh so PDF URL we'll set that to this um or paper URL rather and then we'll make the name we're not doing anything that right now so name can be test and then we'll go to our terminal um if we bring over our terminal uh and then we are in our you know YouTube videos archive paper rag um and we can call in in this this boiler plate um repo that you can clone for my GitHub I find a few scripts in the root um which use turbo repo and allows us to um I'll just show you it allows to start our API start command from Route so start API is use yarn turbo Run start and then filter by our API project so we can just run yarn start whoops yarn start API um and that's actually going to start our API so while this loads okay so we got an error what is this saying no such file or directory open PDF slash theame of our PDF um let's go and let's see if it created our PDF it does not look like it um so what we'll do here instead is we'll go into our probably couldn't there was no directory which makes sense so it could actually write the file so let's navigate API make directory uh what do we name it PDFs PDFs and then I bet but we just run your and start since we're an API if we do this now it will work um an un structure does take a second since it's performing a lot of um you know computations so I'm going to pause this and come back when the start command is finished all right so it's been a little while and as you can see nothing's happened um so I'm going to cancel this and we're going to perform a little bit of debugging um so to start this should work uh let's just add some console logs to see how far we get after you know first if and then we can do after loader and then that shouldn't get called because we're not passing it an array so we'll call after docs um and I'm guessing that the problem is going to be in here so let's add some more functions um we know yeah so we know we were able to get to write file last time because we got the error that the PDF's directory didn't exist um and if we look in here it did write a PDF and it is what we want so we can actually go down and delete these two because we know it made it there um but it's not making it to after documents um so let's make sure we're able to actually instan get our class properly so before load um and the file wasn't deleted so it's probably happening here after load and I bet if we go and we rerun we rerun we're going to see before load and not after load um and if we look quickly as we can see it's created a new PDF um but no logs yet which is interesting I'm going to pause this and if we do get to some logs then I will resume all right quick resume I realized we didn't add a log before here because we're know we're writing but we're not sure if we're able to get best here so if we add that log go back to our terminal we see nothing's logged yet um and we rerun then let's verify yep it's written the third one um but it's here we go after write and before load so after write it's able to instantiate the class and before load which means that this is taking a very long time for some reason um let's see what I would do now is I would go to the unstructured site or I go to the I'll go to the Lang chain um site and look up unstructured and see if they say we're missing anything okay so looks like we need to actually add this Docker container um let's see if that works let's create a new terminal window and just paste in what they gave us uh that makes sense don't have the image locally so it's going to pull it in um and after this is done downloading we can resume and see if that fixed our issue all righty we're back uh so the docker container finished pulling and if we open up orb stack which is my like Docker desktop UI um we can see that unstructured is running so now now if we rerun this um and we hope and pray um okay after right before load this should take a second just because I know that this is an instant um and I will we will come back uh when this completes all right and we're back that did take a while um but we can see that after3 seconds um it generated a bunch of documents um and we got 199 of them which seems about right for this size and um for 18 pages so if we quickly look through here we can see that um for example this one uh it's the page content is this snippet and the categories narrative text this is kind of what we care about um the category parent ID is a title acknowledgements which makes sense um so you can see that unstructured is you know going in looking this PDF and they're not just splitting it by text size or you know by page but they're actually looking at the elements of the PDF and chunking it that way um this will be helpful when we actually embed our documents uh the next step is going to be actually generating the notes based on this text um so let's create a new function we'll call it generate notes async function generate notes and this is going to take in that PDF buffer or sorry it's going to take in our documents documents array of documents and the response is going to be a string um actually no uh we'll probably want to have this be an open ey function call uh which means we're going to have a a structure response and we'll write that type a little later um so the first step these documents we can't actually pass them to open AI so we got to convert them to a string but luckily open AI has or it's not open AI Lang chain has a nice helper function it's called format documents as string um and a essentially that is not the right path uh it's from utils documents util document um and essentially what this does is it allows us to be able to okay so cons documents as string equals this and then we can just pass in our array of documents um and this document type contains this field called page content um if we look at our log we can see that page content contains the content that we want so this format as documents format documents as string function essentially just Maps over documents and it joins them all and um joins them with a new line and returns a string um and the next step so once we do this we're going to have all of our you know full PDF as a text um and then we're going to want to Define our model so cons model equals new chat open AI open AI okay um and let's import that from Lang chain okay and we can't use the default openi model from Lang chain because model name because we want to make sure we're getting that that new 120k context so it's going to be gp4 preview 1106 or gp4 1106 preview gp4 1106 D preview um and this is not going to be the vision model that would uh we need to specify vision but it is going to be gbd4 turbo it's going to have a big context and it's going to be their quickest and smartest model um and then we're going to want the temperature to be 0.0 we don't really want gbt you know being creative we want it to like look at the documents that we gave it and uh take notes on that um and then next we're going to want to Define um we're going to want to attach our function to it so we'll make a new function called const model with tool equals model. bind um and this is a method that uh that our our you know add link chain we have this concept called runnables um and you can bind certain things runnables so here we can bind a list of tools right now we haven't defined our tools we'll give it an empty array um but once we Define our tools we'll pass in our tool there with the opening ey spec um and that uh you know gives our model access to use or to respond in a way which which we can then use that tool um and for this tool it's you know kind of a complex schema so I'm not going to type it all out I have it here I can copy paste uh but we're going to want to put this into an extra and we'll call it a prompts file so we create a new prompts file and then we copy and paste this in and to go over what this is doing let's import this type okay so essentially what this is doing is is defining a new client or chat completion tool as the type it's G to be a function and then the function is going to have a name of format notes small description format the notes response object and the different properties are going to be one array um with the with the key notes and then inside that array we're going to have properties of note which is the actual note that it took and the page number Um this can be the page number that uh the note came from um and this is going to be you know required uh because they're both inside of notes which we defined as being required um so using this we can then import it and pass it through so now we have our model with a tool bound to it and as we can see it's also a runnable now um and because it's a runnable we can define a chain uh using the runnable pipe method so const chain equals um oh first we need to actually Define our prompt um and this is also going to be kind of long so I'm going to copy and paste it but we can go over what it's doing uh so it's our not prompt we're going to wrap it in a chat prompt template which is going to give it that chat prompt template type um which will allow us to pipe it through so our note prompt let's import it import it and then we're going to pipe that to our model um and then after this you will pipe in an output parser but that's not defined yet uh so essentially what this is doing is they taking our prompt um when we call invoke on our chain so const response equals weight chain. invoke um and then we're going to pass in arguments here um and invoke is essentially going to pass these arguments to our prompt in our prompt we've defined one input argument as the paper um and that's going to format this prompt by actually putting the contents of what we pass in through to invoke in here and then it's going to pipe that stringified string formatted prompt to our model model will call we'll have a vote called on as well because model is also runnable um and then the response from our model will eventually get piped to our output parser when we Define that and then after all that it'll get returned to our response variable uh let's quickly go over what our prompt is um and this took me a while to kind of get perfect so there are a few important steps or pieces uh I originally started and I was you know saying what what most people say when they try and summarize something which is you know saying write a summary on of this um but I found that it it didn't give great outputs if you say write a summary because a summary you know like you wouldn't you wouldn't be reading a paper and you want to summarize it you want to get key details from that paper so if you summarize and then it says you know the paper did this but it wouldn't give me details so when I changed it to say take notes then it you know it's replicating what it sees on the internet more which is you know notes on papers or notes on XYZ and and they're they're more detailed and they contain more you know from that uh paper so take notes of fall scientific paper this is a technical paper uh outlin a computer science technique if you want to use this uh chain for something else you know you obviously got to go refactor this prompt and then we say the goal is to be able to create a complete understanding of this paper without reading or after reading all the notes um and then we give it some rules so include specific quotes and details inside your notes respond with as many notes as it might take to cover the entire paper go into as much detail as you can while keeping each note on a very specific part of the paper um these are all important you know we don't want it to take like two notes on you know the first part in the end of the paper we want it to you know take notes on the entire paper and we want it to go into detail um we don't really care if these notes are long because they'll still most of the time unless you give it a really short paper then it might give you more notes than the actual length of the paper but if it's a long paper like the one we're demoing which is 18 Pages we want to go into detail because you know it's might only give us five pages total and that's a lot better than reading 18 pages um include notes about the results of any experiments excuse me that the paper describes now this is important because before I had this I you know I tried getting to take notes and it wouldn't always include what the paper's findings were you know it would say okay this is what they're talking about they're referencing these techniques and that's it and you know if you're reading a paper you want to know what what their outcome was um so this very important uh include notes by any steps to reproduce the results of the experiments um I tested on on one paper it was the violation of expectations um with cognitive meta prompting um and essentially that was a a a prompting technique a series of LM calls that um is a you know you're able to use that and get uh better details and notes about um you know certain conversation or or history um and using this line it would actually give you know gbt's notes it would take would be more details and it would be easier to replicate the paper without just having an idea of what they did you know with this we can actually see how to you know reproduce What the paper outlined um and then do not respond with notes like the author discuss how well XYZ Works uh gbd4 and I'm sure many other llms have a tendency to um not really give you details it's kind of what we touched on up here um but instead say this is what they were talking you not say this is what they were talking about but say they spoke about this so we we don't want it to be General we want it to be more fine grain so in instead explain what x Iz is and how it works um respond with the Json array and then we kind of give some hints about the schema we defined up here so Json array two keys um note page numbers that's our note and page numbers um and then you know give it little better description about what each key should be um and then I had this line at the end you can use different you know ending sentences this is just one I read in the paper a while ago which said you know if you if you speak to GPT like a human you tell it doesn't work step by step take a deep breath really slow down and think about it then it does get better results because you know it's mimicking what it's seen online and there's I'm sure there's lots of content online where it's you know humans talking to other humans and they say slow down you really think about this before you finish it so it tends to get better results if you include stuff like that all right so once we Define our notes we've um piped our notes through to our model the last step for the LM call is to Define an output parser um so since we're using Lang chain it's going to return the output from open AI um in a type called a base message chunk and inside of there there are going to be some arguments and from there we can extract our um function call arguments which is essentially what this is it's reading the paper and it's you know saying here's a function I can call by passing in these notes um but they're going to get returned to Strings we got to do a little bit of parsing so let's say const um it's called output parser equals and then output is base message chunk and then from there we're going to Define our schema after this um and yeah we can go and it so uh first we're going to want to make sure that they're actually the you know the model did call tools so const tool calls equals output dot um additional quars do tool calls um and then if this is UN so this can be undefined so if tool calls is false or tool calls. length equal zero we're going to throw because then the model didn't actually work so throw new error no tool calls um but if it did work we can extract everything so we can do const const notes equals and then we can map over our tool calls so map tool call or always called a call call um and then from here we can say const note equals call. function. arguments um and this these are the arguments that this tool called and these will conform to this spec that we listed here so it's going to have a notes object with a note and a page numbers um but this is type string because it's going to return from open AI so we're going to have to parse it with a Jon json.parse um and then we'll actually be able to do that because it's going to be an object with Note nested so then we can just return note um and let's actually go and Define our type so it's going to be type archive paper note and that's going to be node string P page numbers is an array of numbers this looks right right and it's going to be returning an array um so now that we have this it's going to be type any so we can quickly just Define it there and then typescript will agree because it was typed any before then we can return it and once that's done we can export this to use it inside of our main function um and then we can pipe a response from our mod model through to our output parser um and now you can see that our chain is a runnable which takes in any and the response type is an array of our notes so when we actually call invoke and we pass in the paper as documents as string our response is going to be an array of these notes um and for this we can just return our response uh let's also go and Export our type so we can make sure this never changes so it's going to be a promise of an array come on array of archive notes um and let's import this and finally right beneath here we can remove these say const no or actually we'll use those and we'll just say const notes equals weight generate notes pass in our documents um and then we'll just replace this so we can see the notes it took and how many notes it generated and now we can go back to our terminal and if we rerun it'll take a second you know take 130 seconds to generate that and it's actually going to generate the notes and it should log them and the amount of notes it generated all right so our chain finished running um and as we can see it looks like it did not work like the one undefined uh so now we're going to go into Lang Smith and see what actually went wrong um so I have the run here and we can see it was passed in the whole paper went to our prompt template um and this output looks to be about right passed into open AI tool calls function arguments notes is an array and each element has a note so where did this go wrong this gets pass to our this runnable Lambda is our um output parser so we can see we got tool calls function arguments parse that notes and then each note is an element so if we go back to the code um let's go to our function output parser we have the output um looks like it went right to Tool calls so what we should have done I guess is cons tool calls equals output dot that doesn't exist okay so definitely should be this and we called tool calls um and the input was tool calls so I guess let's just try it again but um console log exactly what we're getting past in so console.log output um and to make this development a little bit easier off camera I'm going to stop reading it and parsing it each time because that takes as we saw before about two minutes I'm just going to have it read from an existing PDF we have so I will resume once I've done that all right so came back uh essentially what I did is you know I took all of this stuff we read the documents I then uh wrote a file here um and then when I need it I would read the file uh and send in generate notes and that way we didn't have to worry about actually you know using unstructured every single time that would take a while um and while I did that I called generate notes I then log the output and log tool calls and if we look at the output we see output has additional quars tool calls with an object so we know that that is right output additional quars tool calls and then here we're calling tool calls. map so we get to this first object and then we parse call. function. arguments so call which is this first object. function. arguments um but we're trying to do object destructuring for a note key um as we can see here it's notes so if we swap this out then it actually does work let's rerun it and see what the output is all right so when we change it to notes we can see that it clearly worked so we got our after notes log and we have our array um and it's a 2d array which means that we forgot to call flat on this but we know that'll fix it so we don't need to run it again um in a length of one but it's because you know the array with all the elements are nested um if we look at this the notes are actually pretty high high quality so introduces paper called gorilla um fine-tuning LL based model designed to surpass gbd4 and writing API calls combine with do retriever it adapts to test time um and yeah so we will actually to read these notes um in more detail when we write the front end but now that we know that this is finished and working uh we can go and you know bring this all back we don't really care about this read file anymore swap that out um we can get rid of this Json parse um and then we're done so we're done with this The Next Step we're going to want to save this all to a database we're going to want to save the generated notes and we're also going to want to save um the documents we we wrote for our you know retrieval at the next step um for that we're going to use superbase um so when we come back uh you can you'll be able to find a uh you know a SQL command to run which will write uh two tables it's going to be one for our documents and then one for our notes um and from there we're going to save those notes and documents send them back to the front end uh after we do the super based database we're going to write a small Express API server um and then that way these notes will persist so if you know you're hosting the site and you refresh the page it won't just be in state you can actually go and refetch those notes um also because these API calls are you know they're not expensive but they're not cheap you don't want to be doing them every time you reload the page so it's going to be nice to persist them um and then you can you know reference your your notes of these papers later all right so to set up our database you're going want to go to superbase you can create a project um it's fairly simple they got great instructions on that um and once you've created your project it'll take a few minutes to initialize and when it's done you you'll see this it's been deployed in your own instance um you're going to want to go to the SQL editor tab from here you can go back to the repos readme inside the readme there is um a set of SQL commands which you can copy and paste in um and what these commands are doing is they're creating three database tables and a function uh to start it from the top we see we're creating an extension Vector this Vector allows us to store embeddings in the vector embedding column type um instead of just putting it in like a 2d number array or something else uh it's using PG Vector which is a postgress vector extension uh and then it creates the first paper or the first table archive papers it's got an ID created at with time stamp um and then a column for paper which is the full stringified PDF uh archive URL which is the URL to the paper notes which are the notes we've generated in our archive notes papers type and the name which is the name of the paper that the User submitted the second table is archive embeddings these are actually going to store our paper chunks our embeddings um and allow us any optional metadata which we will use later um and it'll allow us to query this more efficiently so ID created at content this is very important that you keep these uh column names the same content and embedding because um The Lang chain um C based Spectre store class requires this if we don't want to override it uh so make sure you have content embedding and metadata and the third table is archive question answering these are going to store our questions our answers our context and our follow-up questions for when we eventually do implement the web UI uh and users can ask questions we don't want to just lose those so we'll storm in the database and finally we create this function called match documents this function is what um The Lang chain super base embeddings Vector store class we use to actually perform semantic search on our database it's going to call this match documents function uh and it'll search up um any columns or any rows in our uh archive embeddings table that match um and as you can see we out we are um saving on there here's our embeddings function so if you copy and paste this and you run it you'll see success no rows returned and if you go to our table editor you'll see we have three database tables um you're going to get this warning you're viewing um you're allowing Anonymous access to your table if it's a private app that you're only going to run locally and you're not going to post this anywhere then this is probably fine if you are to go to production or publish your app um to anyone online you're going to want to add rules for this video that's add of the Scopes we're not going to do that we're going to anyone to access um but these definitely are a must if you're going into production once you've generated your super based tables and your project has initialized you're going to want to go into the API package Json and you can see if you're using the template um for my GitHub I have a gen superbase type script and essentially what this is doing is it's creating a file called db. TS inside of a generated directory and then it's running the subbase CLI command subbase gen types typescript schema public pointing it to the file we want to generate in and project ID you're going to replace this with your project ID um this is very simple to access it's in uh the URL of your project so paste that in um and then if we go back to our um terminal we can run yarn build this is going to build the API and the web and in doing so we see that our build Commander API generates our types and this is important because the types it generates um we're going to be able to reference those later when we actually write our database class so we can have typed inputs and outputs um and without this then everything everything would be any typed and we don't really want that so we run yarn build we can see it's building and it built and then if we go and navigate to this file we see it generated all of these nice this nice schema um and later we can go and import database we can pass that in when we're initializing our client and that way superbase has everything nice and typed and we're going to want to export a new class from here essentially what this class is going to handle it's going to handle writing to our database it's going to call handle embedding and writing our embeddings um any you know get requests we want to make um and really everything that involves our database so export class let's call it superbase database um and then we're going to want to have um an initial static method called from we'll call it static from documents so static async from documents um and this will take in an array of documents and it's going to return an instance of our class all right so inside this static um class method we're going to want to perform two checks first um which is getting our environment variables so const uh private key equals process. in do superbase private or you know whatever you defined it as and then we also need the superbase URL or database url const superbase url and I got that right and then we're going to have two checks if they're not there then we're going to want to throw because those are obviously required um in the next step we're going to create a super based client so thank you co-pilot we can import this um and then uh what we did before was when we actually generated this database type it created this called database and we can pass this in as a generic um and that's going to let our client know uh you know what return types uh our you know get requests are going to return um and some other goodies like that and from here we're going to want to in instantiate a vector store and with that we're going to use a l chain so we're do const Vector store equals and then we're going to use this superbase Vector store from Lang chain so await await superbase Vector store um we're going to follow the same you know I guess I copied Lane chains API schema so you know from documents let's make sure this is imported um and this takes in a few arguments so the first step is going to be documents next step is not going to be super base it's going to be our open AI embeddings so new open a I embeddings that's wrong um spelled that wrong open AI embeddings there we go and the final step is going to be our database config so we can pass in our superbase client um it's called client and we'll sign that to superbase and then our table name that we want to actually store the embeddings on which luckily we generated that here so our archive embeddings table and we can import this and then the final step is going to be that function uh we named it match documents and we ran our SQL command and that's how we're actually going to match the documents using this semantic search so query query name match documents uh you could you know Define this as a constant but this is the only place we're going to use it so I don't think it's really necessary um ah this so this is saying um it's not assignable document is not assignable to document and that's because this I guess is an actual you know node type or I guess that's a react type but what we want is the Lang chain type so from Lang chain doent um and then this is no longer going to throw um and in order to this is also wrong because we wanted to do uh we want to be able to have access to our super based client and our Vector store and other methods so we're going to Define our Vector store it's going to be an instance of super based Vector store and then our super based client we will just name it client um it's going to be an instance of or we'll just copy The Source superbase client um which we can import and then we're defining our database type our you know database schema public um I even really care about that so then we need our Constructor to actually assign these class properties so Constructor um uh we need to take in two inputs which is going to be client and that's the same type as that and Vector store um and then there you go thank you copilot it's going to assign our class properties from the inputs and finally we can return a new this um and pass those in and now this is all happy so now when we actually want to instan a new Vector store uh we'll call Bas Vector store. from documents we'll pass in our documents and it'll return a new instance of the class and later on we will write some methods for you know reading um from our table writing to our table adding embeddings retrieving embeddings and so forth all right so the first method we want to add um is for adding our paper to the database so we'll Define a new async method we'll call it add paper um and this is going to take a few arguments it's going to take the paper URL it's going to take the name of the paper it's going to take the full paper text it's going to take our notes and that's it so let's add these types May copile will get it for us let's see what it recommended okay so it's recommended paper URL string name string paper string and notes is string that's almost right we're going to have to replace that with our archive notes uh data type and then it's going to try and call this. client from archive papers guess that correctly but we're going to have to or we will reassign that to um use our constant and then it's going to call insert um we're not going to use this array we're just going to pass we're going to pass in this object uh we don't need to sign an array or an ID um yeah so this is mostly right we got to do a little bit of clean up so this is going to be archive paper notes uh we only need error or I guess we'll get data for now just to make sure our insertion actually worked um let's reassign this to be archive papers table thank you uh we don't need to pass it in as an array uh we don't want to be assigning our ID because that was autogenerated as we saw in the SQL command so paper URL name paper notes um and then we're going to call Select which will actually populate this um this this uh this data object and that way we can you know verify that our data was generated so if we go back to our index file and at the very bottom here we instantiate our database so let's say const database equals weight let's import this and then from documents uh one thing which you'll probably want to do later is add a check which says which which looks up this paper URL in the database and it finds it um you know it's or you're going to we'll add another method that will allow you to um instantiate your database from an existing index you don't have to you know do all this every time um but for this first time it's got to be from documents um and you you know you're probably going to want to add a check saying if this paper exists don't perform all these computations that's going be too expensive but for now it obviously doesn't exist yet nothing's a database um and we can call um await database. addp paper got that right um and then we can pass in our paper URL our name our paper and our documents um let's format this format documents as string just to satisfy these types and then we're done so we should be able to run this um and then let's not return it we don't need to do that we can just console log data and that should allow us to verify that our paper was saved um so we're going to want to get rid of this just for this test case we don't need to convert uh we are going to want to generate notes but for that we will just read this same document file so we're going to say const docs equals weight read file close PDF document. Json let's add that type df8 and then we're going to want to parse these so documents Json parse generate our notes um and then we're going to instantiate our database and add our paper with um all of this so if we go back to our command line let's make sure we're not missing anything first looks good and we run this then by the time this finishes we should have all of our or our first table row inserted um and from there we'll be able to retrieve you return and assist our generated data okay so while that ran I realized I made one mistake um and I tried to assign paper URL when that was not a column and it needs to be archived paper URL so I swapped that um it worked saved and we can see we've saved all these notes notes saved beautifully um and then the next step we're going to actually want to add our embeddings so let's go back here um we can copy this because we're going to want to wrap into promise.all as they don't rely each each other so we'll say await um promise all do an array we'll do this first one and then we can call await database. Vector store. add documents um and this is a method provided by langing Chain where we can pass our documents some options if we want to do that and it will embed using the opening I embeddings we passed in here and then it'll actually insert these table these these um uh these uh the the the embeddings and the the content into our database uh so once we call that we promise. allet we don't need these awaits um and then this method will be finished and we can return the notes that we've generated um and when we set up our Express server we will be able to call all of this it'll you know read our paper paret our notes or our documents uh generate notes save everything to the database and send back our notes that we can then go using the client all right sorry about that my recording was just interrupted so it's a few hours later um but now that we've um implemented our our main function um we can go ahead and write our server. TS file for our Express server so create a new file server. TS and we're going to create a function we'll call it main so function Main takes no arguments um and we're going to also import Express from Express um and do a couple things so first we're going to get our app variable so const app equals Express uh this sounds good um we also want to Define our Port so const Port equals process. port or 3,000 um actually we're going to make this 8,000 because 3,000 will conflict with our our front end um for this index route we're going to make this a health check so add a comment we'll say health check with a space um and this is just going to response. send 200 or response. status 200 send Okay um and then app. listen at our port we should get listing on Port 3000 when we start so let's go to our package Json and we're going to update our start um script to start server. TS now if we run yarn start uh it did not work which is not what we want and that is that makes sense it's because forgot to run our function so if we started now we see listening on Port 3,000 um which also should be port so Port um and let's start change this to template string boom perfect uh now we can define a route uh we're going to make it a post endpoint post and it's going to be we'll just call it take take notes U this is not going to be health check we're going to want to extract some data from the request so const um what do we want we want the paper URL so paper URL name and pages to delete from request body that looks like exactly what we want make sure this is exported um we'll rename this to take notes uh so now we can import our take notes we'll call it we'll say const notes equals a wait take notes um and we can import this we also need to make this async and then once this function runs we'll just say return or response. status 200. send notes and return so that this doesn't do anything after that um and now we can open up insomnia or Postman um and you can actually check and validate that your endpoint works all right so open up insomnia or Postman or whatever you use uh we're going to add Local Host 8,000 and then our URL is take notes um and we need to add a Json body that's going to contain a paper URL and we'll add this later it's going to contain a name add this later and uh we are not going to specify uh pages to delete right now um so let's go back and get our paper URL so archive.org um and we're going to want to put that in there put that in it put the name in run it and make sure it everything works end to end all right so now that we have our paper url url in our uh copy paste we paste it in um I'm just going to go and copy the name of the paper from archive not going to do anything special and we'll send that in there um and when we send this in or when we start our server and then send it in uh it should take us it should air out okay cannot destructure property pay request. body as it is undefined um interesting okay so let's debug this request. body is undefined if we console log request request um we got to restart our server and we send it in then we're going to see did we find archive see if that was found anywhere doesn't look like it nowhere okay so we're not actually passing this in properly which is interesting so all right so we're back um I investigated the error turns out it was um I must have been something with my Postman request because I just created a new request through take notes at it um and it worked did present another error though I had forgotten to pass in the actual PDF URL um so it was trying to save a PDF which was not valid and read it from unstructured um but here you go uh now that we fixed all of that we pass in the proper URL um and make this request we can see we're at PDF um we're going to send it and it's loading and this looks good because it's going to take a while um so yeah I'm going to pause this and when this finishes running we will resume all righty so this took a second but we can see it worked gave us our notes um and I returned them in the format we asked for uh so the next step we're going to want to actually create our question answering um directory and to do that we're going to refact this a little bit so we're going to make a new folder we'll call QA and another folder and we'll call it notes um and then we're going to inside of here make an index.ts and copy over all the contents from here um and then we're going to just drag and drop that in there and this is going to take a second okay um and it should have fixed all of our Imports looks like so uh we can delete all of that too and we got to refactor where that gets that from um and then inside QA we can also create an index.ts file and we're also going to need need a prompt. TS file um so we'll name we we're going to create a function and it's going to have to take in a couple things um the question as a string uh the name of the paper um we're going to use that as metadata when we filter in our database for the embeddings um and yeah I think that should be good for now so async so QA on paper and we're going to take in that question string and name String um and this is going to yeah okay so now we need to make an update to our database right now we only have a from documents method but obviously for um QA we are not going to need to add documents so we're going to want to add a from existing index uh method so from index it's going to take in no arguments perform essentially all the same checks as here but it's going to call the from existing index method here as well and this is only going to take in the embeddings and the DB config so we can swap that out for that and boom now we have our prexisting index now that we have this we can also go and refactor this method um and bump this up here and say from existing index and boom now we aren't embedding our documents and doing the from documents every single time um so we're going to want to copy that over here because we're also going to need this database all right so the first step here obviously we got to import that um and then we're going to want to retrieve our documents so we're going to say const Doc documents equals oh wait database. Vector store. similarity search oops that's not right but similarity search and this is going to take in three arguments so the query K which is the number of documents we want returned and any metadata filtering so obviously question um and then we're going to get five documents uh we'll do more we'll do uh eight documents and this is something where you probably going to want to play around with it um 8 5 to 10 has worked well for me in the past but really really depends on on you know what kind of papers you're you're analyzing and your chunk size and whatnot um and then our metadata is going to be name should just match the name um and this is something we can verify by actually looking at our super base in beddings um and inspecting one of the rows I'm looking at the metadata so if I copy over all of this uh so it actually looks like I made a mistake in the last um the other database step where we're actually inserting these edings we want to add metadata where the name where we're adding the meditated name uh should include the name of the paper that these are submitted um or the paper URL you know it's some sort of identifier so you can reference it later so if we go back here we're want to add documents these already contain our um other metadata so we're going to want to add a loop which is going to say um const new docs equals documents. map doc um and then we're going to just want to return pretty much the entire doc um and then metadata um and we'll use the URL that's probably a little bit more fullprof so doc. metadata um and then URL is going to be paper URL um and this is going to be um let's see uh document but it's an array um and it should be happy with that and then we're going to save these documents um this shouldn't change it's just the page content but this is actually what we care about because now when we go and query it we can use we s that out for the URL so paper URL um and then we can query by that so then this similarity search is only going to perform that search over documents where the metadata has this field and that way we can have tons of different papers indexed um but our similarity search will only be on the relevant documents that we care about all right so the next step we're going to want to actually include all of the notes that GPT took originally in this um API call we're go back to our database and we'll add a new method and we'll call it async get um get paper uh we'll take in the URL that'll be our identifier and it's not going to that we're going to want to return uh database and we can get the type from here so database public tables archive papers row so database was this lower case yes so database public and then tables archive papers finally row and that's going to be our response type so now const data or error equals wait this. client. from archive papers table select equals the archive URL that's exactly what we want we want to check it says if error is true and we'll add a console. error just letting us know this is where it happened so error getting paper from database and then we'll rethrow that error if it's not then we only want one of one of uh the papers in database so if this is true or data is false if it's not then we're going to return data bracket zero um and then we should be able to call this in here and say const notes equals 8 database. getet paper and then paper URL um in these notes as we can see it's at Json schima um but we know that it's going to be our archive notes um and then the next step we're going to want to actually construct our we're going to want to construct our prompt construct our our output parser our tool types and make those requests all right so for our uh tool schema and prompt it's going to be the same deal as before I'm not going want to type this all out we can copy and paste it in but we're going to go over everything uh so we have a chat prompt template like we had before and we have this from opening I to find the type so import that um open a as that from open AI um so what this prompt is doing is we're telling it you know here are the notes you've taken here the relevant documents from marantic search and here's human's question um and we're trying to you know tell this GPT to act like a professor um who knows it stuff and you know can give us better notes because as we know GPD is trained on data it's seen online um and it's likely seen better uh data you know better question answering from these academic settings um and then our tool schema it's going to be an object with the answer and then follow-up questions um and we're also asking gbt to suest followup questions based on the question and answer uh so now if we go back here we're going to want to define a new function async function QA on paper and we'll take the question that's good string uh but we also want the documents array of documents nope we're going to have to add this type from Lang chain as well so import document and that's spelled wrong but we can do this document and then finally we want the notes so it's going to be notes um and we'll type that as the array of archive paper note um and then too Implement for now um and even though this is type Json this should hopefully work and we hopefully don't have to cast it ah we don't okay we know it's archive paper URL so we can just cast it as um unknown as that um and then it's happy with all this so QA and paper uh that's the same name so we will rename this to let's say QA model question answer on model um so now that we have this we need to do a few things we need to convert our documents um into a string we using the same method as we that we used before we need to convert our notes into a string and then pipe through our prompt our chat model and our output parser um which we should do the app parser now um it's pretty much the exact same deal as our um as our parser for taking notes but instead of going in and extracting one more level um for notes we can just return args right away so it's answer followup questions answer followup questions get the tool call error if it's missing map over the tool tool calls parse the arguments because they're string return it flat in case it tried to return multiple arrays um and then return that um and then back here we can say const model equals new chat open AI uh this will be the exact same as before so model name equals GPT for 1106 preview and temperature should be set to zero and now if we what is going on now we want to Define our model with chain so con model with tools model. Bine um this is exactly what we want is that the right name almost the right name so we import this and now we can construct our chain so const chain equals um our prompt prompt import that. pipe model with tools. pipe to our output parser which was named answer output parser um and then we can invoke this oh we got to import that um before we invoke that we have to convert our documents to string so const documents as string equals format documents as string from Lan chain pass our documents um notes we const notes as string and we're just going to map over these notes so notes. map note and then note. note and join them with a new line and now we can say cons response equals chain. invoke here's the paper um which is the documents as string here are the notes and here's our question let's verify that's going to work so we want question we want relevant documents not paper and notes so notes notes relevant documents and question um and then we can see this is the right type and we're going to want to go ahead and return this so return response uh now down here why does it not like that Let's ignore that for oh it's because we we renamed the function so QA model um and now return QA model and yeah this is exactly what we want so we can see this return type is that um and now we we can export this to our server and create a new route for it so import QA and paper from QA index.ts um go back here could actually export this and now this is happy and we can create a new post route we'll just name it QA um and then we're going to use this function but we only need the question and the paper URL so keep question Ur paper URL and then question um and remove the others we'll rename this to QA and return that and now if we go back to our server and we restart it and go to insomnia got our paper URL um oh we actually since we changed how we do our metadata we have to go and and reload this so that the metadata is is is what we want and we can actually search for for the semantic search okay so before we actually go and create our express route I realized we forgot one thing const answer and questions equals waight um and then we want to actually save this to our database so we don't lose them uh if the user reloads the page or you know if you lose that state so go back to database and we'll create a new method called async save QA um this going to take in a few things it's going to take in the original question string it's going to take in the answer string going to take in the context used string and it's going to take in the followup questions followup questions which is an array of strings um and this is going to act very simil similar to our ad paper so we can this. client. from archive QA table um and then it's gonna insert not that so our archive QA table has let's make sure we're adding the right types okay so question question answer um context and follow-up questions beautiful um and if this works then it should save our database and we should be able to retrieve it in the future so we go back here and then we say await um [Music] data database. saave QA um and then since this can technically be an array uh we're allowing GPD to return multiple we have to map over it so um map and we can promise. all this as well QA and then await database. saave QA we don't need that await though we can make it in line just like that um and this takes in question answer context followup so question answer is going to be qa. answer um context is going to be our documents so we will do the same documents format as documents of string for the context because that requires documents to be a string and then the final followup questions are qa. followup questions should be happy with that we got to wrap this in a promise.all a wait promise.all and make this a sync um and then our notes are actually get saved and finally we can return them here um and while you're were gone I also cleared out my database to make sure there were no conflicts so let's restart our server let's go back here we're going to want to take notes on this paper send it off when this is done we can then go back and perform QA and we'll ask a good question and see if it's able to give us a uh an interesting response all right so that took a long while but if we go into our embeddings and we inspect one we can see that the URL now contains our archive URL so we can swap that to be QA keep the paper Euro and swap the question to be um how does how does how does the paper manage the as pretty general question um not really expecting something too technical back from this but it's overall it's related to the paper I it should give us something that's okay so if we send that request this will will probably also take many seconds um when he gets back with the answer and the follow questions we'll resume we can we can read over those all right so that was much quicker than the other ones because it's not actually indexing a full you know 18 page paper um so we see the answer is to paper introduces an abex tree sub tree matching technique to manage functional correctness of generated API calls this method involves parceling the generated code into an A and then finding a subre within the as that matches the API call of Interest by doing this the paper can verify the function um equivalence of the API calls ensuring that the generated code is not only syntact syntactically correct but also functionally appropriate for the task at hand and if you've read this paper you do know that that is overall um you know related to what the paper is talking about they're essentially you know how to use llms um to mock API calls and how do you tell that those those are good uh so the followup questions are can you explain more detail how the as sub tree matching technique Works how does the as sub tree matching contribute to reducing hallucinations in language models and what are the limitations of any of using as sub tree matching uh for the evaluate for for evaluating API call correctness um these are all pretty good followup questions that I would be interested to hear the answer from a few of them but in the interest of time um I think we're going to cut it there all right for the client portion of this we're going to use shadon they provide some nice pre-built components for you know our form our button our inputs the other components we'll need to use um you're going to want to find the nextjs section under installation we're going to set this up you don't need to run this command because the monor repo um structure I've provided already did this for the web directory so we can skip right to step two run the CLI want to make sure you're in the web directory and then you can run this command we're going to say yes to typescript default style default our Global CSS file is located in Source SL Styles plural SL globals plural. CSS I we'll use use CSS variables that's good too that's good that's good and we're not using react server components yes and we let install dependencies once we've done that we can go back we've see you can see it installed a few things um it added this utils file components. Json this when we actually install components they'll go here um or the config and the actual components will live here I'm just start we're going to install the form component so find form scroll down and copy this command for the installation go back to your terminal paste that in let it load and what this is going to do it's going to install some dependencies it's going to actually pull in some code that's that will live in here we can see it's already done that and it pulled in the button and label for us as well which is nice cuz we were going to need those um and next we're going to want the collapse component so we can run this command and while we're at it we also know we're going to need the text area component so we'll copy this as well and we can all we can run it in one line so installing collapsable done and text area is done as well if we go back we can see we've collapsible and text area all right so to start developing our web app we're going to want to run yarn Dev just so we can see our changes um as as we update you can navigate to Local Host 3000 and we can see that our page says not implemented which makes sense that's what we have and we're going to start the overall structure of our front end we're going to have two forms one for submitting new papers to get notes taken um to have all their content embedded and a second for asking asking questions and answers over that paper so we're going to want to have two divs um we're going to have them in a row and we're only going to do just desktop styling right now if you want you can update it for mobile later but for now desktop will work and we're going to use tail one so class name equals Flex Flex row and we'll add a gap of five should be good and then inside of here we're going to create two more divs div and this will be our come on this will be our add paper and this will be our QA on paper inside here we're going to have want to have another Flex box but we're going to put everything in the column because it's going to be a form so class name equals Flex Flex call uh and we'll add another gap of two then we also want to have a border on this so border one pixel and we'll give it a background of let's say gray 400 so BG or border Dash gray 400 and we'll give it a slight rounding we'll do medium um and then for our form it would actually be easier if we just go to Shad CN go to their components find the form Comm form and we can just copy what they have in here for the example so form field the form item um or actually we can go down here for a more complete form so let's copy all of this bring it back and we'll paste that inside here can need to add these Imports so add all missing Imports means we have the button from our components all of our forms from our components input do not want it from Post CSS and form from react hook form don't want that from there either so we can swap that out delete these two and then for our input we where is our input oh we never added input all right so we need to install input uh by finding in J CN going back to our terminal let's open up a new window paste it in done um and now if we go and reload our page get vs code's cache updated and try and input this we'll see we have from components next we got to go back to our form and we have need some extra logic around the form so did they install Zod for us Zod Zod yep they did so we need to create a form scha so install star at Zod from Zod we can do that um that'll work and then we need our form schema we will call this submit paper form schema and this is going to have a paper URL which is a string don't need any of that we need a name which is also going to be a string and what's the last one pages to delete um and is this right an array of numbers that's correct and we also need to make this. optional so once we have our schema we go back what's next um we need to Define our form using the use form Hook from react hook form so we can import that come back Define our form go up and we can also call this submit paper form uh and if we swap out our form schema got to add Zod resolver as well is that the right import yes replace our form schema there as well and we're not going to have any default values all right so now we have our form let's swap that out there swap that out swap that out uh we are going to update this to have different form Fields so we're going to have two main form Fields one for the paper URL and we're going to call this paper Ur URL placeholder we can go and actually use the placeholder from the original paper we Ed so archive uh this is it so we'll get the PDF URL and this can be our placeholder and then form description the URL to the PDF paper you want to submit paper URL placeholder description next we're going to do name so we got to make that name change that to name give it a capital and we can also use this as the default name gorilla and description the name of the paper and now if we go back to our server see onmi is not defined that makes sense we need an onmi Handler so we'll just copy this without any functionality yet go up here paste it in and we're going to actually change the name so as we can flip with our other on on submit to on paper submit add in the right form schema and this is going to infer the right type so on paper submit paste that there we can delete though that for now and we'll prefix this with an underscore so we can delete that and now if we go to our page it should have loaded yes um but we don't want this to be all the way to the left and we don't we want some padding around the edges so we'll go back here um let's indent this a little bit I got to do that beautiful one more and we'll add a padding of let's say two how does that look that's pretty good and then here we should do mx- auto and hopefully this brings it to the middle no it didn't um all right we will touch that later for now we're going to want to get the last form field in and the submit and the last is going to be our pages to delete since this is optional we're going to wrap it in a collapsible so we can find collapsable code it out of their example copy that we'll put it beneath our last form field but above the submit fix that to add in our Imports be beautiful collapsible trigger so now if we look here say can I use this in my project um we also want to have here they had this nice little icon so where is their icon we want Chevron up down let's see did they install this for us if we go to package Json search for that yes they did so we can do chevron Chevrons up down plus where is the plus being used looks like all right we'll ignore that um let's import this icon so if we go back up here let's put it there and we can remove them when we don't need them anymore collapsible div they had it saying start three repositories um and then trigger as child a button with this icon so we can paste this in as the trigger like so fix the Imports um we'll call this to span toggle so if we go back here we see now there's just this we want to add some text as well so we'll go P tag and we'll say delete pages question mark all right this formatting is a little bit off so varant ghost width n we don't need that um let's just delete this styling and there we go delete pages and we want to make one change this font light that's better we'll make it normal yeah before was Bold And now when you click this it opens this drop down and instead of having this content right here we can add another form field um and this time it'll be for the pages to delete so pages to delete and then we'll add 10 comma 11 comma 12 as the placeholder and then the pages to delete from the paper and if we go back see delete pages open it up and here's our input um and then submit we can go back and read this console.log submit paper U we can remove this underscore as well since we're actually going to be using it now and let's test to make sure this actually works um give this a full width and if we say that that and pages to delete we shouldn't need to have like that so let's see test one we don't need that either why did that delete okay so that deleted because we forgot to change the control update this to be pages to delete and now we can add that and doesn't add anything there and if we hit submit we see it expected it an array and we received a string okay so input placeholder ER onchange event blur number undefined is not assignable to string number okay so what we're going to do instead is change this to be string array um and then inside of our onsubmit we can convert it back to being a number so that array that error went away pages to delete submit come on let the same error submit there we go no more error after I Rel load and we can see it logged our values and then if we want to submit some pages as well expected array receive string um definitely don't want to make our users have to punch in an array so we'll just do some parsing after the fact and if we reload here add in our values again and add in our page to delete and submit we can see we got our paper URL page to delete and it's all nice there all righty so next next what we need to do is add some regex after the fact that will actually verify this is a list of numbers that's comma separated um and it'll convert it into an array of numbers okay so for converting our pages to delete to an array of strings we'll create a little util function we'll name it function process pages to delete give me a string and then con num array equals page is to delete. split map parse in Num that looks beautiful um and this should throw if one of these are not a number we're going to want to trim this as well in CL in case the user submits some extra spaces and then we can return this and if we verify the type signature nice and a number array and let's type this just because we don't want anybody changes after the fact in returning the wrong type so now we can get all of our values in the proper formats and we're also going to want to save the paper and the name or I guess just the paper URL in state um and the name just to display to the user and for when they ask questions we'll actually know what paper they're referencing so const let's see paper data or we'll call it submitted paper data submitted paper data and then change that to be set submit submitted paper data equals State um and we'll type out our archive paper notes or well we'll type out this type and we can make it that type or undefined so type submitted oh actually we can just use this well no that won't work because that's the wrong type so yeah we just make a type so so type submitted paper data equals beautiful exactly what we want and then we can type this to be either this or undefined and we got to add this import as well nice so now once we've submitted we can set our data dot dot dot values and pages to delete can use our function nice so Pages delete is true we're going to process and if not undefined and this will work with our type all right Next Step we're going to want to fetch our data from our API so for now we can write out our fetch request um and our processing and then we want to test it we have to actually to start our API so const response equals fetch API papers um and we're going to rename this to take notes which will match our backend and then method post body stringify our values nice and then we can turn this into async um we're going to want to do a do then rest and if rest. okay is true we can return that and else no we don't need an else just return n so this is either going to be the type or any um and then inside of our API directory we create a new file called take notes. TS um and we can just copy what they have there paste it in uh and change this to make a fetch request to our actual backend so const data equals wait fetch um and then for now we can use Local Host but but if you're in production you're going to want to change this to probably an environment variable which can change between you your local your staging or your production URL so for now we just do const API URL equals HTTP con Local Host ad8 um and it's just slake notes and I want to make sure that's actually what we yes take notes so API URL await fetch the URL and then method post header updation Json body we can use the request do body and we don't need to stringify it again because export default async function we don't need to stringify it again because we are already stringifying it here so data and then we'll do the exact same you know if it's okay null if not um return null and then we can say if data is true return response. status 200. Json data and this 200 is what's going to set our okay to be null or sorry our our okay to be truthy or not and then else 400 and we'll just say error for now all right so let's go back to here we can go out we can do yarn start API now our API is started listening on port 8080 and one more thing we can do here we can actually type out the data in our response so we know that this is going to have that response type and we want to just copy and paste that wherever it is delete that if it's not being used here we go archive paper note so we'll just copy this exact same type go here replace that it'll also export from here as well um and now here or we have to count for our error so name and then we'll actually explicitly say it's Error and why don't you like that type error is not assignable to error okay so that's not going to work [Music] um we'll just say or undefined if we get a response status 400 we know that's okay or we know that's an error so now that this is all working like that we can set our response type um to State as well so const we'll call it notes and set notes and then we can use this type as well it's actually not going to be singular instead it's going to be an array as we we can see here so we need to update this to be an array of notes so array archive paper notes or undefined uh go back here we can say an array of archive paper note import it or undefined response and then if response is true con data equals a wait response we don't we need that here so await this response any if response resp is true then set notes response. data and one more time double check beautiful and then else let's just throw so we can see the error throw new error something went wrong taking notes boom uh we should also probably console log this so console.log response.data just cuz we're not actually doing anything with this right now so that's going to make it easier to see if our um API worked so we can go back to here let's copy this paste it in we can copy the name as well paste that in and delete pages we need 10 11 12 is that correct let's see 10 11 12 nice 10 11 12 and now if this all works we know that it should take about two to three minutes and that's accounting for unstructured and the open AI response and then we should get a console log with our notes so submit um and boom we already got an air so that does make sense probably was not awaiting uh because that came back so quickly but just to be sure okay so here's a problem in our API uh throw new error exports back to name index okay where is this coming from valid ah so it's in our PDF document parsing um let's verify that our types are all getting set properly so if we go back here and we console log all of this and we got to restart our API for this as well yeah that gave us a nice error okay let's go back we can just reload copy the name we know we're going to have to do 10 11 12 and we can copy this name all right and if we submit then we go back here and we see paper URL all righty and this was getting passed in as a string so what we're going to want to do is see is our data actually getting formatted properly this is definitely going to be a number array or undefined uh so we should do here instead is I guess UNP parse our notes so convert pages to delete back to array numbers so cons pages to delete array equals pages to delete and then we can just copy over this exact same code um because we can see from the type that it logged it was just a string as comma separated so if we go back back here and we'll just put this right there whoops copy paste so it's going to be a string we want to return an array of numbers and we'll just wrap this uh we need to do the check so dot or we'll just say if it's truthy then process it and else just do an array uh we can actually just do undefined because it is optional undefined put that in put that through and if we copy this one more time console.log just to verify it's all working restart our API go back to our web reload let's copy the name paste it in do 11 or sorry we can just 10 11 12 copy the URL paste submit and we can see that it logged in Array so while the this loads I'm going to pause the video and when it finish loading and hopefully logs our notes I we'll come back and we'll see what it what it logged all right so we're back our response turned undefined but if we go to the network Tab and inspect it then we can see that our payload was right and our preview was this which means we were trying to parse response.data when data did not in fact exist and response would have done the trick um so if we go and we update our index TSX and just do date response we know that'll work and it will actually solve or save our notes all right so before our notes actually return we can go and we can add a section beneath our two forms or where our two forms will be for displaying our notes so we can say if notes. length or if notes is true notes is true and and notes do length is greater than zero we want to log a div class name Flex call Cap 2 border one pixels that looks good uh we don't need a border for this though um there we go and we'll say h you know H2 notes and div Flex call Gap 2 notes. map see note dot note and to prefix we'll makes this a paragraph paragraph tag instead um and we let's see we'll make this class name equals we default text party medium and yeah actually we don't need any styling for this um but what we are going to want is update this Flex call nope want this to be a row with one Gap and we'll keep the padding um and then here we're going to want to have another paragraph tag but class name text- small and text- gray and let's make it 600 to be a little bit in the background and this is going to contain our note. page numbers join comma and we'll wrap this all in an array so people know that there's page numbers um you know what actually I think T was right we do want to have this in a column we'll have this beneath Gap two is probably right as well but what we can do here is actually number our notes so let's do get the index of the array and we'll do index so it'll be you know one it's going to start at zero we got to add one so it'll be you know one then the note two the note and what is it complaining about here we have our opening closing opening closing why aren't you happy div div ah we're missing a closing div okay and that's going to be for this one so div uh and that makes sense we also need a key um and ideally you don't want to be using index as your keys for um arrays or mapping of an arrays and and react you probably want something like an ID so if we what what we should be doing and if you're going to put this in production you should be returning the generated database rows um after you write it so when you when you all right I'll just show you so you take notes you add papers to the database and you should be instead of just returning an error you you should also select them and return the data and return that so then you can return your ID you're created at and the notes and that way you don't have to use an index as a key and you can use the actual ID and if you want to do other things with that data you'll have the ID in the database or you'll have the ID that matches the ID in the database on your client um and it's easier to manipulate and request that data there but for now this will work you know we're only using this locally um and we can save this go back to our front end we there's nothing there cuz we don't have any notes and if we go and do this again and we copy the name done this a million times and 10 11 12 make sure our API is running yes submit and in a couple minutes we'll come back we'll see our data get logged and our notes logged out there all right so we're back we see logged all our notes and our notes got logged here this is not really what we want so we should clean up our styling we can see it's inside this div and we defined it as a row when it should be another div here where we can copy that and then add another closing div there and that will wrap our forms in a row but we want our notes to be in a column and if we go back we'll see there we go I notes in a column and and that's a little make it a little bit nicer we'll set a Max Max width to let's say 600 pixels how does that look much better uh next we need add the second form for question answering um we need to add a check in our back end which will verify that our database does not already include a paper with this URL and if it does we can just return it right away so we don't have to be um embedding and taking notes on multiple papers um and instead of so add that set up a form for our question answering and um add a component for actually logging um our questions and answers and the followup questions and add a way to easily click on the followup question button and have that followup question be sent to our API all right so now that we have our uh paper URL and submitting a paper hooked up we're going to want to set up our second form for actually asking questions and answers so if we go here um we can pretty much just copy this whole div and then remove what we don't want so copy that in um and then we know we're not going to need this collapsible so we can drop this uh we know we are only going to need one form field for the um question the user wants to ask uh so we're going to swap this to be question and the question placeholder or description will be the question to ask about the paper and the placeholder will be why is the sky blue that's good um and then we're also going to need to create a new form schema so we can just copy what we had before and we will rename it to be question form schema and swap out this key to only be question next we can go and put it in here that's not right we need to go and create a new form first actually so copy paste that rename it question [Music] form swap out the schema type copy that and now we can go and swap these out this name will obviously have to be question and we need a new onsubmit so on question submit and then once again swap out the form schema I'm in for now now we will just comment this all out and console log what the values are that get submitted okay uh let's take our form schema pass it in there and if we go back to our UI we should see it looks like that um let's do a little bit of housekeeping to make this prettier um by going to this Di and we'll say margin X Auto and that moves it over and let's move it down from the top a little bit so margin top three there we go that looks better um yeah and now we can test this out hello world submit and we see we get an object with a question of hello world that looks pretty good um and the next what we need to do is connect this to the API so we can copy what we had here or just uncopy uncomment this Swap this to be QA it's going to be a post request our values will not just be this because if we go to our server. TS we see that QA requires the paper URL as well so index. TSX luckily here we're storing um the submitted paper data in this object so this contains the paper URL so what we need to do is create a new values object and we'll name it data so con data equals values and paper URL subit paper to paper URL um but we should also add a check and say if this is false um throw new error no paper submitted so we know that this will be truthy and we can swap out our stringified body to contain this data object um and then once that's done we're going to want to make a new state variable called we'll call it answers and set answers for now we will not type this we'll set our answers just like we set our notes and last step we need to duplicate this take notes inside of our client API I we'll rename it to QA to match our backend and we swap that to be QA post API URL this all looks right except it's not going to be this type as we saw before so if we look at QA QA returns answer and followup questions as an array so we can go and add this type so we'll name it QA response and it will be answer is a string and followup questions as string of arrays as well or an array string that's going to be an array of this or or undefined this all looks good let's go and import this into our TSX file so answer set answers can either be basically the same as this or or not or we can just swap out archive papers for sponse Q&A and import it um and that should all be good so let's quickly add a jsx for our answers just so when this works we'll be able to see them so we'll say answers is true and answers. length is greater than zero um and then we can do copal will probably suggest what we have before so let's see what they suggested so a div that's in a column Max withth of 600 just we wanted answers instead of notes map it over and it'll be answer dot answer um so this is saying the answer and then the content we're not going to want that we're going to want the we're going to want this to be our our instead of our um page numbers we want to be our follow-up questions so we will do answer. followup questions uh. map followup and then we can just copy this and we need to give it a key we'll just give it key equals the index as well so index key equals the index beautiful and then instead of that we need to put the follow-up and let's wrap this all into div which is a columns div last name Flex Flex call Gap two two padding that looks good but we're not going to do follow-up questions there instead we will put that let's format this right above so we will have another just plain P tag followup questions boom and now if we hit this API after we load a paper we ask question then it should all work I'm going to pause this video because this will take a minute and when it comes back we'll hopefully see our notes and our questions and answers all in the same spot one quick thing we can do before we submit that again is go back to our notes section and instead of uh always adding it to the database we can first do a check and we can say or we can add a new um method to our database saying find paper by URL so async or get paper paper by URL and we're just going to take in the paper URL string and we're essentially just going to query the papers table we're going to select one by the URL if it's false we're not going to want to throw error we're just going to want to return no or we will return the actual um data actually we already had a method so that's nice we don't need to redo it we can just do get paper and we're going to swap this to return n instead instead of throwing nice and let's swap out this response type to be or no so now we can call get paper inside of our notes we can say cons existing paper equals a weight database. get paper paper URL and then we can say if this is true return existing paper. notes and we can we'll cast this as array of archive paper notes and then our response type will be the same and for good measure let's type this response type just so we never accidentally return a type we don't want there we go so now if the paper exists we don't have to recompute um our notes and and our PDFs and we can just return it and it can be pretty quick so if we go back here our paper should already be in the database so we can copy this all in uh just for good measure 10 1112 let's go to our terminal which I accidentally just killed the web for hopefully it comes back um we can go up one level and do yarn start API that should start our API beautiful let's bring this away and okay it did reload our page not the end of the world paste that in paste in our name and one two three submit and boom we can see it returned instantly because if P from the database um and now we can ask a question and we will say what are the results or we'll say how does gorilla manage the As and if we submit this it should take a second and when this comes back I will respond with um or I I will resume this video with the different answers I gave us all right it's back took a lot quicker than I thought it was going to be and it says kill manages abstract syntax treat methodology blah and there we go so answer looks good only problem is our follow-up questions let's see what what went wrong there it just logged follow-up and that makes sense because then it wrap this in a bracket so this obviously won't reload um oh it did okay there you go so you see our followup questions can you explain more detail and whatnot um and the last thing we should do is make this a little bit prettier so let's wrap our the this in a div we'll name it Flex call Gap five margin X Auto that's good but let's do this in a row instead just so we have our answers and our notes side by side so then if we copy and paste these inside this div go back and there you go looks a lot better we've got our answers and our notes um let's have it match up with the columns though just one little net boom notes on one side answers on the other side questions and adding a paper up here and it all works pretty well so yeah if you want to finish code for this it'll be in the GitHub linked uh below once again the mon repo template which you can clone to follow along for this tutorial will also be down below um and I appreciate everybody for sticking around this long I hope you learned something um and I hope this site is useful for you um for future papers and other content you want to read from archive", "metadata": {"source": "rQdibOsL1ps", "title": "Build a Full Stack RAG App With TypeScript", "description": "Unknown", "view_count": 16373, "thumbnail_url": "https://i.ytimg.com/vi/rQdibOsL1ps/hq720.jpg", "publish_date": "2024-01-02 00:00:00", "length": 7795, "author": "LangChain"}, "type": "Document"}, {"page_content": "hi this is Lance Martin uh I'm a software engineer at Lang chain and I want to give kind of an overview of how I've been using multimodal LMS uh for a few applications um first maybe an overview of progress in multimodal llms um back in 2012 Andre Kathy put this kind of entertaining blog post out talking about the state of computer vision and the the point was that computer vision models were pretty far away from actually reasoning about an image like this and kind of understanding why it's funny um it was kind of notable that like this was a real challenge for vision models and we were really far away from kind of AI visual recognition that can kind of understand and appreciate humor now today if you take that image and pass it GPD 4 V it indeed is able to understand exactly why you know this is funny and it's actually kind of a fun demonstration to do that just to get a sense for how good these kind of visual recognition capabilities have gotten with recent multimodal LMS you another interesting example of this is Steve wnc had in 2010 the copy test you know what would it take for uh a test for AGI would be that a that a computer could actually make a cup of coffee could move you know enter a room determine all the elements necessary to make a cup of coffee and make a cup of coffee in the gbd4 v paper they actually showed a number of interesting examples visual recognition where you know the M can recognize you know elements in a coffee machine how to operate a coffee machine how to get around a kitchen parts of a kitchen and so you know it's another interesting demonstration that within you know roughly 10 to 15 year time span we've really gone quite far in terms of kind of visual recognition capabilities and now these are really in kind of everyone's hands with GPD 4V and some other models that we'll talk about today um so kind of a quick overview of models a lot of this work of course of course you know predates uh you know the current year of 2023 uh it's probably worth noting clip it's very important work from open AI um that kind of map data from different modalities text and images into a shared embedding space um it's open source and actually clip embeddings are still used uh for visual encoding in models that you'll see today for example lava um and can also be used as kind of multimodel embeddings to take text images and map them to a common embedding space uh so clip is like a very important um kind of work in this in this like Arc of multimodal llms um Lava came out earlier this year and it's really interesting open source model that builds on clip uh that uses um a clip image encoder as noted with 3 336 5336 pixel resolution and an LM vuna which you may have heard of it's a fine-tune variant on llama 2 um variants have been adapted for other LMS like mistol 7B so this is actually really cool we'll show this a little bit later but it is open source you can run your laptop um and it gives you really interesting multimodal capabilities uh fuyu is another one that came out this year uh which actually has an interesting architecture bypasses the image encoder can run it variable resolution that's from Adept um and of course GPD 4B and most recently Gemini from Google have come out these are State of-the-art close Source multimodal LMS very strong performance very very good impressive demos um available through apis so that's kind of the landscape and there's indeed more models is just kind of a short overview with some links as well um and maybe it's worth walking through what's kind of going on so with lava for example um there's a notion of image embedding so we're kind of familiar with text embeddings used very commonly in in a lot of AI work today taking a piece of text and mapping it into an embedding space which is just kind of like a high dimensional Vector representation of a piece of text Chunk of text you can do the same thing for images you can take an image and map it to this kind of this this uh you know higher Dimension uh embedding space and similar images kind of wound wind up in similar space in this in this projection this is kind of like a a 2d representation of um kind of the embedding and you can see that you know similar visually similar objects kind of are mapped to similar regions in this high-dimensional space in this case it's kind of projected on to 2D um so that's kind of step one you can take images you can embed them and then step two is you actually can do this projection step you can take kind of a an image embedding and project it into the same kind of space as text embeddings and so um this is kind of this this very nice kind of visualization showing you have your image encoder like clip you get your your multimodal embedding or your image encoding and then you project that uh into kind of the same the same kind of token space as your language embedding and then those tokens can be incat and pass to llm so you can really think about this as like kind of two steps you're like basically taking visual content embedding it then projecting it into a space that can be concatenated with your text tokens and that's really all you're doing so when you take a multimodal llm you're giving it like a piece of text an image and you're kind of mapping those both to the common token space and it can read that kind of common uh concatenated string of tokens um at once and reason about what's going on so so that's maybe like a simple like mental model how to think about what's happening when you work with multimodal LMS um yeah let's talk a about use cases so Greg Cameron on Twitter kind of had this kind of nice visualization of a bunch of things that have been been shown with GPD 4V um a lot of people seen really cool demos with image captioning um extractions a really good one taking an image extracting elements text elements and so forth um recommendations so there's kind of like a lot of design applications um kind of suggestions about how to improve the visual Aesthetics of a scene of a of a of of like you know um of an object um and of course like interpretation this is like you know common in the rag context for example if you have like a you know collection of say we'll talk a little bit later to it a little bit later about slides um or about diagrams in documents you can of course use a vision model to reason about what's happening there in a question answer context um and this was like an intering demonstration of of extraction uh shown in the in the gbd uh 4V paper here uh actually this is a follow on to the GPD 4V model by Microsoft showing here are some interesting um explorations and they they talked about kind of extraction from complex documents um so let's actually walk through a demo to make this a little bit more concrete and I'll share kind of a bunch of code and and templates that can be easily reused later um so I think you know presentations like slide decks are a really good application for vision models because they're inherently kind of visual they have lots of kind of complex visual elements like like graphs uh tables figures and they're very common you know every nearly every organization uses slides in some capacity and conventional rag approaches that just strip the text out really miss a lot of this so let's try kind of how could we build a rag system over the visual content in in a slide deck um so to start off what I did was I took a slide deck and this is um uh data dog's Q3 earnings report I randomly chose it you know it was just like an interesting demonstration of like kind of complex uh you know financial information and figures and slide deck and I created a set of 10 questions and answer pairs about these slides this is like my evalve set um and this is really easy to do I can just create a CSV that has like my question and my answer in this case like my input output pairs um and it's just a set of questions that I devised myself I looked at the slides I said okay here's some interesting question answer pairs I put them in a CSV and I load these into Langs Smith now Langs Smith is Lang chain platform that supports durability and evaluations um and I create a data set for myself in Lang Smith and there's some links down here that show exactly how to do that but that's my starting point so I say okay here's my evaluation set I have the slide deck I built 10 question answer pairs from the slides now let's compare some approaches there might be two different ways to think about multimodal rag um so one is this notion of multimodal embeddings so we take our slides we extract them as images in every image we use multimodal embeddings to map them into this kind of this embedding space that is common between kind of text and and images um for that I use open clip embeddings um and so I now have an index in this case I use chroma that contains a bunch of images uh that have been embedded using open clip um at retrieval time I ask a question I use I basically take the natural language question embed it indeed with multimodal embeddings same ones similarity search just like normal retrieve images that are similar to my question pass the image to in this case uh my multimodal LM like GPD 4V to answer the question so I'm just doing really image retrieval from natural language using multimodal embeddings that that's process that's kind of step one at the top now option two is a little bit different where I take every image and I caption it so I basically produce a summary of the image and I embed that summary with text embeddings and we'll talk a little bit later why this might have advantages but for now let's just say these are the two approaches so I do this captioning so I take the image convert it to text um embed that text and then I have this linkage between this like this summary or caption and the raw image I search given my question among those summaries so I'm just doing a kind of text embedding lookup and then I fish out whatever image is closest and pass that to the to the multimodal L for synthesis so in short in one case I'm just using multimodal embeddings to do the lookup across my images in the other case I'm using text embeddings on image summaries so that's really kind of the two approaches now first we can do like a sending check we can take our slide deck we can embed it and then we can show does this even work at all so here I ask a question what's the projected Tam over time for observability and I see that this is my retriever I've built and the notebook is all linked here um I can retrieve the slide image that's relevant to that question so that's pretty neat right I can ask a natural English question and get back an image then I can take that image pass it a GPT 4V 4V answers the question very precisely by looking at the details in the image so that's the general flow here um for valuations then all I'm going to do is I take whatever rag chain whether it's my image capturing model or my multiple M embedding model uh and for every for every question I generate the rag answer uh compare that to my ground truth answer I have this greater so Langs Smith will do all this for me it will run a grer that will use for example a specified llm like gp4 to compare those two responses uh to compare like the basically the the rag answer to the ground truth answer and um and then I can yeah I'll show a little bit later kind of do this nice comparison uh between different models for each question and which one's doing better and then root cause so that that's kind of set up here again I I build an evalve set based on the slide deck I have two different rag approaches and I have my evaluation approach right here and we can look at the results so if I use GPD for text only just like a standard PDF loader take the slide deck rip out the text do it it's pretty bad you might I mean of course it's kind of to be expected because a lot of the questions rely on visual content diagrams and so forth which you can't get if you just look at the text now with open clip here's where it's a little bit interesting with multi embeddings using open clip the performance is moderate and you can see I'm using open clip but gbd4 V so really what's What's Happening Here is the retrieval step is a little bit uh I would say of moderate quality and we can talk a about why this is but the intuition is that for slides that are pretty similar multim embeddings may not quite have the the capacity to differentiate them and I made this as a controversial statement it's also Complicated by open clip has many different embedding models available so I chose one that has kind of reasonable memory footprint and uh in like in like solid performance not exceptional performance so again there's a lot you can play with here there's many different multimodal embedding models but I think the thing I'll caution is that retrieval may be challenging using multi embeddings in a setting like slides that are actually pretty like semantically Sim what you really want is subtle differentiation between like the content and the slide like is this table showing a or table showing B but maybe to the multile embedding model they both kind of look like tables there isn't that much differentiation in retrieval so I think that's just a caveat what I see is the multiv vector um which is basically the image captioning approach is really good and you kind of would expect this you if you take GPD VB and ask it to summarize an image uh of like a of a table or graph it does really good like it's you can get really rich summaries and then you're searching in natural language and I'll show some examples later um but you can do really effective retrieval based on an image caption that's gener by a good model GPT 4V um your performance here is very strong um again you know the aor bar show standard error across um it's a 10 EV set in three trials and so there's a little bit of noise I'll talk about later there's also some kind of API flakiness with gbd4 v that that I think will hope hope be resolved soon um but so I just want to the intuition here is that for visual content like slide decks just using text naturally as you expect is is really insufficient for like answering you know interesting questions multile embeddings have a lot of Promise they probably are the most promising in terms of like ultimate ceiling um there's lots of models that are coming out that will come out soon um and you you know it has architectural appeal it's fairly simple it's just like in different embedding model you stick that into a vector store and and it it's pretty seamless the captioning thing actually has really good performance now architecture is a little bit more complicated you have to generate these image captions ahead of time there's some cost concerns there um so I think that's kind of the tradeoffs today the capturing approach can really be effective for things like you know compx content like slides uh but it's a little bit more costly um and this this is kind of showing how you can use Lang Smith like deep dive and compare so this is I can look at all my question answer pairs here and this will show the answer Generations um I can dig into each one um you can see like the the greater scores so zero being incorrect one being correct um and then my experiments across the top so this is just showing kind of an overview of what you get if you run this evaluation with Langs Smith you can look at these these eval you can look at the results kind of in in high grity which is quite convenient um now here here's kind of a fun case study um so the question was Data dog um well the question was how many how many customers data dog have the answer was around 26,800 and that was from one slide you can see here that looks like this and the answer is embedded in this kind of table type thing and the visual model has to find this slide then actually reason about it and get the right answer out so you can see that text only misses this um which is kind of interesting um multimol Bings likely retrieves the incorrect slide um and indeed the multiv vector approach which is our image curing does get this correct and right down here you're seeing this is a an image of the Langs Smith Trace that you get so Langs Smith actually renders the images for you so you actually can look at and I may be able to even open this up now maybe we can we can look at it um yeah actually here it is so this is actually looking at the trace um and what you can see is this is what was actually past the LM so here's the images that got retrieved here's the prompt your an analyst has the answer questions um here is the user private question how many total customers Dat Dog have and then the tables or text and pretty cool it's able to find this in the images and gives you their answer so that's like a nice demonstration of like these are actually really cool and impressive models um and uh just make sure I yeah I'm back on the slides very good um and that that's kind of a nice demonstration of how well the these models can work uh same here just looking at like retrieval from a table again uh actually in this case they all got it right so that that's kind of interesting topk rag was able to car rather get this so in some cases indeed text only is sufficient it can strip this this text out it can reason about this table good enough um so you know I think it's worth noting that depends on the nature of the questions you want to you want to answer to want to ask about your content uh but in many cases um as noted in the eval results imagery is very very important to capture and it could also be the orientation of the text within the image kind of matters fres your reasoning about their content as well which you would lose with you if you just do kind of a a extraction of the text alone but in this case it's a table so you know visual reasoning works really well but hey so do is just stripping the tech stripping the text from the table from the slide as well um so maybe here's like another way to look at all this is we talked a lot about different like rag architectures now we can look across different llms and this is on the same all set and now we're looking at again uh text only so results kind of consistent this is actually a separate set of experiments done by one of my colleagues um similar results though so you can see text only pretty bad around 20% uh some St the standard errors shown uh I guess this is across 10 different trials um now what you see here is we're now comparing Gemini Pro to GPD 4V and what you can see is there's the same textual Trend which is that open clip embeddings at least ones we using fall short of the captioning approach pretty consistently um and indeed the multiv vector approach with image summaries does the best and also uh Gemini which you can see kind of noted here is on power g4v which you know their paper also reports this um you know the precise quantitative differen between the models can be kind of difficult to assess of course a lot was reported the intuition though is like it's a very good model and it it seems to be on par with GPD 4V absolutely worth testing and trying uh so I I you know we've been pretty impressed with it um it's very good to have multiple options for multimodal for multimodal models so I think that that's you know really nice thing to highlight I'll show another interesting thing here I was looking at resolutions so there's this kind of odd thing with multimodel models you have to pass images into them and like well you know what resolution should you provide those images and it it's a little bit kind of vague so I did kind of a titration across different resolutions and this is on that same eval set um here I'm showing the mean fraction correct uh show standard errors I ran three runs on this 10 question eval set um I ran on gbd4 V and Gemini and you can see kind of it's a little bit noisy and I'll show you kind of why in the next slide it's a little bit noisy here the general trend is notable at very low resolutions as 192 by 108 pixels performance is really bad so kind of on the extreme it's really bad and performance is indeed the best kind of in this in this larger resolution range there is kind of noise here in the middle so I still don't have a totally firm view where the precise cuto off is with respect to like uh performance degradation with both models it seems you can kind of a Le all the way down to 40 by by uh 270 performance is still moderate it's kind of consistent in across this mid-range and then you get above 800 into 900 pixels um and the de performance goes up the results are a little bit noisy I'd like to do more studies here at least this is just showing you kind of the intuition is indeed that image resolution does matter intuitively lower resolution is worse which is what we observe um and it seems to be the case that you know larger uh um you really I I guess you could argue this result may be a bit of an outlier I think I'll show you why on the next slide um but indeed higher resolution does better so it's just something to be aware of you should test it for yourself uh but image resolution does have an effect on performance as as you may expect now this is kind of an odd result and actually I hope it's kind of a temporary issue that's resolved and maybe it's data by the time this video comes out I have noticed that gbd4 V does have some kind of odd reliability issues you get 400 errors I link the ticket above I hope it's resolved soon but I saw this quite a bit in my recent on my recent runs uh it did not seem correlated to resolution it seems uh more or less random uh I think it's also skewed the prior results a bit because the scoring um the correct scoring um may have been affected by uh a number of the trials for a basically erroring app out and failing to produce a correct or incorrect score so it injected some noise into the prior result unfortunately um so anyway this just a caveat if you're think about using this in production I I hope it's fixed soon you know I've Reach Out open AI about this and and so you know that's just something just something to flag and maybe again this is a kind of a temporary issue and maybe it's moov by the time this video comes out I'd love for that to be the case I just wanted to flag it um another thing I did uh kind of related task is I took just this one slide and I tried kind of a small extraction challenge I asked eight questions about the slide and I tested a number of different llms um and to see kind of how well I can extract information from this one slide at variable resolutions you can see the results kind of match what we showed before although there's one interesting addition here you'll note first resolution matters so of course low resolution obviously quite bad performance very very good so gets you know all of them correct at 672 and above for both Gemini and GPD 4V again differences between Gemini and gbd4 V you know not really notable um both really good models now I also looked at um Lava 13B because I'm really optimistic about using lava 13B for things like image captioning because it's an expensive task what I did note is that it's not very good at extraction and I went back and listened to some recent videos with the author of The of of the of the paper he mentions that a newer model with better OCR capabilities is actually coming out soon I'm really eager to test that um I just wanted to kind of throw a cave out there that lava from what I can tell is is not very strong um at kind of extraction it kind of makes sense as not before it down samples to uh you know 336 pixels so kind of somewhere in this range so it's downsampling quite a bit you can see even with these really good models if you down sample till 336 you're kind of in this bucket and you you've lost a lot of performance relative to higher resolution so that's issue one and then issue two um I I guess lava just was not really trained for OCR extraction I've seen it being able to exract kind very obvious kind of big text blobs but if you give it a slide like this it's really detailed lots of kind of you know jumbled up content that it's possible that just the down sampling blurs this all together you really can't resolve these like very fine scale details from the image so that's just another caveat for extraction test it doesn't seem like lava's quite there yet but I hope that Chang soon I'm really optimistic I really want to use it for captioning um because that would save a lot of cost and and it's a you know very good repetitive task at open source model would be really helpful for um now maybe just to round this out like how can you get started try these things out for yourself we reled a bunch of different templates um now here's kind of a fun one this is like really simple but um you know most people who have like you know iPhones or Android devices kind of have used the visual search cap ility like you can search through your food pics or your you know family piic or whatever it is ask a question you can find pictures related to that like ice cream and find all my ice cream pictures like it's a nice you know clear functionality a lot of people use and like and what's kind of fun and cool is that with these new multimodel LMS um that can run locally you can like build this for yourself um now I understand this is probably a bit of a demo for now this is not like a production application of course but it's interesting it's a really like in an important like starting point it's cool you can R this locally for yourself and we're still very early in the in the Arc of these models developing and getting better so this template which release basically let you take a set of images and embed them using open clip and you can run this on your laptop I have a I have a Macbook uh Pro uh M2 32 gig Max and um this all works really well like I have bit of a you know more powerful laptop but but you know it works on colleagues who have lower power laptops again using clip to embed these images store them in chroma locally you can ask a question um and it will then pass uh retrieve the image pass it to in this case uh it's actually using baklava which is um Lava plus minrol um which is pretty cool I use olama to basically serve that model and um it can it can do um you know it can serve as a visual assistant it can answer questions about the about the the images and so you know what kind of ice cream I tried oh you know it'll it'll find this ice cream photo and say Sofer that's kind of cool um and it works with both like multi muls and captioning so we talked about those two approaches previously and we have templats for both so you can try both approaches um and again here here's actually like what it actually looks like so when you spin up this templ it's like two or three commands to like create your own index and then one command to like spin it up and it'll basically create it'll it'll basically spin up this little playground for you and you can ask questions you can just say hey like what kind of ice cream do I have it gives you the answer like ITT the image and you know the beaut of these templates is you can play with them yourself modify the promps it just get puts everything together lets you get started easily and this is showing over here the lsmith trace which actually shows the retrieval of this image of of my moner Sofer which I happen to like quite a bit it's very delicious and uh and it shows um you know your your your H assistant give a description of the food pictures give a det time of the image it gives a summary it image is a close of green frothy ice cream um and um yeah it looks like matcha so you know that's pretty cool like it works it runs on your laptop it's it's like you know it's a really fun demonstration you can play with it you can change the prompt you can change the model AMA makes it very easy to test different multi multim LMS and these things are only going to get better so like it's it's a fun thing to to get started with um and this just like a nice way to just have everything together in one piece and to spin it up very quickly um now maybe for applications are a little bit more like you know maybe like commercially relevant or or like a little bit more complicated um we have two different templates that use gbd4 V or Gemini for rag over slide decks and this is like kind of nice to get started so you take a slide deck it produces a bunch of images you embed those with open clip store them in chroma um and retrieve images related to the question and send this to either uh you know either gemini or GP 4V and get answer and this works quite well I mean I have to say I've been really impressed uh this is your multi bets we talked about that before a little bit Limited in retrieval quality the captioning approach though which I'll show here um you know effectively the same except for just this you just this piece uh really does work quite well and so you know this is actually showing an example of you know the answer you get from this question um so it's absolutely worth trying and testing this template lets you get started really easily um and yeah this is like just kind of an example showing you can retrieve the correct image um and reason about it and um you know I I think these templates are a very nice way to just get started quickly um and they absolutely worth trying so that's actually all I had for slides um just maybe hopefully this kind of a useful summary and overview of how to get started with multimodal LMS the different LMS available to you numerous templates that you can use to to test um and um I hope it I hope you find this useful um thank you", "metadata": {"source": "28lC4fqukoc", "title": "Getting Started with Multi-Modal LLMs", "description": "Unknown", "view_count": 4032, "thumbnail_url": "https://i.ytimg.com/vi/28lC4fqukoc/hq720.jpg?sqp=-oaymwEmCIAKENAF8quKqQMa8AEB-AH-CYAC0AWKAgwIABABGCkgWChyMA8=&rs=AOn4CLCPeU4y3IyyG2C3XDHmIYh8efhGbQ", "publish_date": "2023-12-20 00:00:00", "length": 1833, "author": "LangChain"}, "type": "Document"}, {"page_content": "welcome to another YouTube video where we're going to build something really cool this is going to build off of a previous template it's going to build off of a uh it's going to build off the research assistant template but it's going to be a modified version specifically we're going to get it to work over SQL databases so what that means is we're going to create a research assistant that does its research by executing SQL queries and and asking questions of SQL databases and I think this is interesting for two reasons um probably many more but two reasons to highlight uh first a lot of data is in structured format and so being able to not only ask questions of it but also write research reports over it unlocks a massive amount of use cases secondly what we're going to be doing is actually using a subchain a SQL chain as the thing that's doing the research so this is hooking up the research assistant to another chain which is itself you know using an llm to generate SQL queries and then returning that so I think that's really interesting because it shows off this idea of having multiple llms with different responsibilities communicating and and delegating and so we're going to combine the uh the research assistant with a a SQL question answering template one we'll pick one that exists um already and uh yeah I think this will be a pretty cool combin of of two interesting topics so we're going to be working out of the Lang chain templates um repo this is it pulled up here the research assistant template is right here there's a few SQL templates we're going to use SQL AMA I really like AMA it's a great way to run uh llama as well as other models locally um and yeah we're going to dive into it we'll we'll be setting some environment variables but I'm going to do everything from scratch so you guys will be able to see uh exactly what I'm setting and and what I'm having to do I'll probably pause my screen at various points um so I can use environment variables and things like that and not leak those but other than that this will be completely one take from scratch so this is my copy of the linkchain repo um I'm going to initialize my virtual environment um I already have one set up um and then I'm going to go into the templates repo and I'm going to create a new uh a new template so I created a new template called Lang chain or sorry I created a new template using the Lang chain template new and then it's called SQL research assistant um I can now move into it and there we go I've got that set up if we take a look at what is inside it we can see that we have a basic skeleton um and the chain file which is here is uh yeah pretty basic it has this has this like Talk Like a Pirate example so what we want to do is we're going to be working off the research assistant and so let's [Music] grab let's grab this and let's move this in here let's call this SQL research assistant and let's overwrite everything then I need to fix Imports probably so I need to add SQL there SQL there where else is SQL used not there do we use any SQL okay so I've copied over the research assistant I've renamed it to SQL research assistant that seems like a good start if you haven't watched the research assistant YouTube video you definitely should um this will be this will be doable not having watched that but that would provide a lot more context the research assistant basically has two components it has a research um chain and then it has a writer chain the writer chain can be relatively untouched um if we look at that um it is is oh actually this I think no this this is good um it has a bunch of prompts that say to write a research report um it uses open AI we can keep on using open AI um and so the writer chain is good what we want to change is the research chain so previously we looked things up on the Internet we want to change this to now use SQL we can see the chain down here um and so looking at this in more detail the first component generates a bunch of smaller search queries um and then basically we look up we do a search on each of those uh sub questions answer them and join the responses and then and then we pass that as the final research report which then gets Rewritten into an essay so um what we basically want to do we still want to generate sub questions so this is going to be the same we'll maybe change the prompt a little bit but that'll be the same what we want to do is replace this part if we look at where this is defined this uh basic basically gets a bunch of links scrapes them summarizes them that we want to replace this with a SQL chain so we have a bunch of sub questions and we want each sub question to go to a SQL chain and then respond so let's take a look at SQL oama to see what's going on in here so this is the SQL chain um it actually does more than we want so it has a conversational memory we don't need that because remember we just want this one particular chain but let's copy all of this and let's put it in a new file here I guess um let's call this sequel let's put that there we also need this so so this um this example is doing question answering over uh uh sqlite database that has uh information about NBA rosters so we'll end up asking it's a pretty simple database so the questions that we're going to end up asking in this video are probably going to be pretty simple but that's fine the point of this is to show how you can heavily modify the research assistant template to do research over any source so the fact that this particular source is a pretty lame and uninteresting MBA roster database doesn't matter too much so we'll paste that there then um we're going to modify this chain slightly um this is unnecessary um so so this you can see is is doing some saving of memory we don't want to be saving any memory we don't want a concept of memory if we're just doing this we really just need the SQL chain the SQL chain we can also remove this history bit yeah we can remove this history bit and then that'll be good we can use that so we'll repl the SQL response memory with just a SQL chain um cool that seems relatively fine we might have to change some typing around the outputs but let's work with that let's call this uh SQL answer chain and then what we're going to do in here from Lan chain nope from SQL research assistant search SQL import that and then we are going to use that right there so we're taking this chain which comes from the SQL template and is designed to answer a single question and we're basically dropping it in here the map is going to mean that it's going to apply for all the questions that are generated from the sub Generation Um let's add let's add a little test case just to try this out before we spin it up um chain. invoke question um who is typically older point guards or centers um so this might be answerable with a a single SQL query but it's a good uh it's a good example of it's a it's a working functional example um okay so before running we need to make sure that ama is running AMA is what we're using to uh use a local llm and actually let's take a look at which so we're using Zephyr Zephyr is actually a little bit too big for my backbook so I'm switch to llama 2 um if you haven't set up a llama definitely check it out it's super cool repo makes it really easy to interact with language models in order to make sure that I have it running I'm going to do this it's taking a little bit okay hi awesome cool so I can see it running that's fantastic um I'm using open AI at various points throughout here so I'm going to set my open AI API key I'm going to pause the video and do that all right that key is set um and then the next thing I'm going to do is I'm going to set my Lang Smith key so Lang Smith is going to be super handy for debugging everything that goes on because I don't think this is going to work on the first try it never does and so we're going to need to iterate on it a bunch Lang Smith is going to help a bunch with that I can set it up by basically exporting these variables um if you don't have access to link Smith shoot me a DM on Twitter or LinkedIn and we can get you access there um I am going to go back to the terminal and insert those um I'm going to pause the video while I do that to not leak any Keys all right set those variables so we should be good to go um let's try it out actually so so before we do that I do have to install the package that I'm working on so I'm going to pip install d e this is in the SQL research assistant it's going to install um as an editable dependency the files so that way when I modify the files I don't have to reinstall it super handy um I can now run this file where I put this little example um python SQL research assistant chain probably should have added some print statements around it but I think it's fine because this is going to eror anyways okay cool um so I see that I have a key history that's because I've removed the history um from this chain but I haven't removed it from this prompt so let's remove it there and I'm just removing history because the sequel example that I copied was a conversational thing so it had this history key in there that's missing um now because I don't care about that so I deleted that I forgot to delete it from The Prompt now it's good okay so we get some error I was expecting this because I don't think that llama is great at writing squl queries and so I probably need to give it some more explicit instructions so let's take a look at what is going on under the hood there's a lot going on because this is a more complicated thing than usual um this is the first LM call that's being made this is where I'm asking the llm to generate a Persona and so I'm going to be writing a report in the style of a sports analyst agent that seems good so far now here's the next part um where I'm asking uh the the LM to generate some sub questions um it's generating average okay we can maybe return to this and refine this um a little bit later on but I think that works um this is one thing that is generating that kind of looks fine um this is another thing that is generating that also looks fine and we can see here that I think this is where the output error occurs and that is because there is this okay so it's being over lever Bose so it's not I'm telling it to um so this is the prompt that's this is the prompt that's being used to write the SQL quy based on the table schema below write a SQL query that would answer the user's question um no Preamble it's adding things on what I'm going to do is I'm going to add another output parser um that basically um splits on double new lines and just takes the first so there shouldn't really I can't think of a good reason why there would be double new lines in a SQL query so when it seees something like this it's going to split it's going to split here it's going to take the first thing it's going to be that that should probably work um all right let's run it again and see what happens I'm I'm expecting using llama 2 that we're going to have to iterate it on this um quite a bunch which is good because that'll give us a chance to show off lang Smith show off some of how we think about doing prompt engineering bits all of that good fun stuff okay cool so so this actually seems to work relatively well so far um it's actually getting down to line 180 in here where we're trying to join the response of the SQL answer chain together we expect to see um we expect to see strings but instead we're getting an i message M that is because um that's because this we can add a stir output parser that will add in um that'll that'll convert it from an AI message to a string so that's good and then what I also want to do though is I want to change things up a little bit because I don't just want to return the um I don't just want to return the answer so let's go answer there but instead what I want to do is I want to combine it with the original question and have like a nicely formatted kind of like response I can show what that looks like but basically I'm going to do Lambda X then do F string we can do this this uh like that okay right so now this um so this is doing several things um first it's well it's making this return a string and it's making this string be a question and answer the reason that that's useful is that then when that's passed the LM it can can see what the question it asked to get that answer was rather than just seeing all the answers there there'll be a little bit of waiting here because llama's a little bit slow locally we can actually probably see it running if we go here and we can see it in the action and it finishes okay so oh wow so so this is a much longer report than probably necessary for a simple question like this um but if we take a look at it we can see all right so it gives some introduction um okay so the average age of centers is 26.7 years old uh um there's some more it's no data for point guards okay so it's getting some information it's it's like you know I would expect it to have point guard Age center age and then that's kind of it it's not getting there let's let's see what's going on if we look at the final prompt that's inserted um Okay so we see that this is why it can't calculate that it can get that and then it can't get the age distribution of point guards versus centers okay so this is like fine that's like a more complicated SQL query and not a very good question but this is weird why can't it answer this um that's the okay that's let's take a look here um that's Center it has an explanation but that's fine because we slice that out what is here oh that's this is Center that gets the SQL query this is looking up point guard um I am probably [Music] guessing that yeah okay so I'm guessing it's like PG instead of the word point guard well we can what that means this is probably better handled in a SQL um tutorial but what that basically means is we want we need to get more information about the table um so if we look at what is in here this is yeah okay so the SQL database this is a database wrapper that we have um and when we initialize it we're passing in Sample rows and table info which I believe if yeah so so this um so okay so backing up what EX L is going in to the uh llm what's going into the llm look in the playground we can maybe see it is this um and notably the only thing that we're passing in about the SQL table is this information right here which is the schema but we need to know we we want to do analysis on like what is the position column we need to know the values of that so that we can filter it because basically what was happening and what was going wrong is that we were looking for point guard spelled point guard not PG and so that's a mistake that the L made because it doesn't know the values of the column so I'm guessing when we made this template we probably did this because llama gets really confused by by some of the um SQL rows let's put in two and and let's see what happens here um that's kind of the downside of working with the local models they're not necessarily always the best on so here so here you can see oh is that a respon oh no that didn't resp okay so here you can see that we have the two rows from this table that we put in and you can see here that it's a little bit misaligned but position small forward shooting guard so hopefully what should happen is it should let the llm know that the values here are like this style they're not fully written out point guard it should be like PG and so here actually okay perfect so we can see that it recognizes that PG okay so we get a response let's look at this response we have some great background um okay average age of point guards is 27.5 while the average age of centers is slightly lower um H okay so here it's citing some different numbers [Music] um so so it's here it's hallucinating a little bit let's take a look at this prompt and see why it thinks that so it gets this gets this okay so so this so it's getting two different sources of information or two different two different responses one is um this which is saying the average age of point guards is that so what I'm guessing is actually I I let's take a look at some of the other ones [Music] um so this is just hallucinating it looks like like it has something with centers so so okay so so basically this is a hallucinated response um and to to backtrack into how I got that like if we look at the final prompt that's going into the research report we can see that one of the questions and one of the answers that we get is this this is that conflict with this and this I trust these and these much more that they're accurate because these are just simple questions this is like way more complicated this answer so I'm guessing that this is hallucinated if I go back and I find where that ANW responded from I can see the data that it's getting it's hallucinating a lot this is part of the issue with um local models so so what I'm thinking that I'm going to do so so what I want I want to do a little prompt engineering and I probably want to change language models because Ama or just llama in general um you know it's it's not as good as chat GPT it's fine for some of these things and it I think this is this is a separate conversation but it's really important to be able to know when to use different models and when they're good at and when you need to do more complex kind of like um synthesis of information you probably want a larger model at least at the moment if we look here oh I I can open this in a playground okay I wasn't expecting that but that's fine um uh I don't yeah okay so this um so so Lama it can't really be used because we don't support it but what we can do is we can take this we can put it here um and we can use so so now we can try out so this is the prompt template this is the prompt with all the inputs that gets in and we can use we can choose any output that we want let's see what like like what would open AI have said okay so it's listing it out that's much more faithful than olama which just hallucinated things so what I'm going to do is I'm going to change the synthesis part from being done by AMA to being done by chat open AI um cool let's run that and let's see what happens so we can see things in action again let's take a look at maybe what's going on um it's generating that SQL query generating that SQL query that's complicated SQL query but looks like it hasn't errored so far um this is now summarizing some of the outputs here um yeah okay so we're getting the average age of centers we're getting the average age of point guards this last one is very confusing um what is it even doing select age count Star as count of point guards count Star as count of centers from NBA rosters okay so this is like no information or no useful information still running yeah so this this might take a this might take a while again um I will pause the video and come back when it's finished all right so we've got our report um age comparison between point guards and centers um gives some nice Preamble gets average age of point guards is this average age of centers is this and then mentions that 29 centers are in basketball so yeah the so the it it uses the given information um if we look at the prompt and what's passed in we can see that we get the average age of point guards is this the average age of centers is this and then it asks this question about the distribution of point guards versus centers in basketball and it gets back this which is true there's there's 29 centers in basketball in probably probably this database um and yeah so if we look at the response um yeah it's truthful it's rather long for this type of question but again this is just an example question and yeah I think that about covers everything last thing we're going to do is we're going to serve this with Lang serve um so Lang serve is a really easy way to deploy all these all these chains and agents and all of that and so you if you look in the P project. toml you can see that we've got this langing serve tool and it exports this module um so SQL research assistant chain if we go into here perfect good catch so we can see that we can we need to change this to SQL research assistant um so this remember I I copied this code over from regular research assistant which is why it was research assistant but we want to be importing SQL research assistant so we're importing SQL research assistant chain. chain if we go to there this is the chain that we defined now once we have that properly set up what we can do is just L chain serve this will run it up um we can go to Local Host 8,000 go in playground mode we can see now here so this is the this is one of the research variables that we could choose from doesn't really do that much there's the question if we go back here let's copy this question let's put it in there and now we can see the intermediate steps start to creep up this is uh we can see that there's some questions generated there's some intermediate responses um and it keeps on chugging along and then now we can see that the output starts streaming so this is a really nice we you know we build in streaming we build in all of that to L serve so that when you have long reports getting written like this you can still see it as it starts to progress um so this is yeah this is the research report um that about wraps it up uh I can't think of much else to cover cover hopefully this is a good um more advanced uh uh YouTube video on taking the research assistant template and modifying it so that I can do research over any source of knowledge in this case a SQL database and this this this is also really cool because it uses a bunch of different prompts and even different models remember we're using llama to from Lama to generate the SQL um so it uses these prompts and these models in a wide variety of of different ways and it's really this this complex cognitive architecture that we're that we're building up that's about it thank you guys for listening", "metadata": {"source": "es-9MgxB-uc", "title": "SQL Research Assistant", "description": "Unknown", "view_count": 11809, "thumbnail_url": "https://i.ytimg.com/vi/es-9MgxB-uc/hq720.jpg", "publish_date": "2023-12-19 00:00:00", "length": 1840, "author": "LangChain"}, "type": "Document"}, {"page_content": "all right so today we are going to implement a new template um and we're going to implement it for this new paper that just came out um skeleton of thought large language models can do parallel decoding so I saw this on Twitter I forgot where I saw it but I thought it was really cool um and it plays to some strengths of Lang chain and of Lang chain expression language namely it's going to use multiple llm calls it's going to break things down into small chunks and then a bunch of those LM calls are going to be conducted in parallel um and so with Lang chain expression language we can do that fairly easily so we're going to create a template um for those and for Miner what templates are those are are really easy way to get started with any application um and so we have a bunch of predefined templates here um but we're going to create a new one um and so we should have instructions on how to add um yeah all right so let's create a new template I'm if you look here I'm in my workplace Lang chain templates directory um L chain template new um skeleton of thought and something there we go all right uh L all right um so I now have this uh uh example template that was just set up I can go inside it um let's find it where are we skeleton of thought um let's get add everything in here get check out the Harrison skeleton of thought all right just checking out a clean Branch um we've added a bunch of stuff in here we can see that we have a really simple read me um so I need to add a description here I'll do that later environment variables I'll do that later um and then a bunch of predefined uh documentation there got some p project. tunels with some dependencies we'll use open AI for this maybe we'll go into how to do this with a different language model should be pretty easy and then the main part here is just chain we have a really simple dummy chain here but of course what we're going to do is we're going to implement this new paper skeleton of thought so what is this paper I'll post a link to this in the YouTube description as well but basically the idea is it's a really cool idea if we take a look at here if you get a uh question like what are the most effective strategies for conflict resolution in the workplace a normal llm response would just be generating this answer one by one what skeleton of thought does is it generates a skeleton really short bullet points and then it expands each bullet point and the reason reason I like this is it um it speeds things up so it's really really fast which is good um and then also this this kind of speaks a little bit to kind of using the language model to plan and then execute on things so here each execution is just expanding with another language model call but you could easily imagine that each execution could be another you know it could it could it could write a whole paragraph it could write a whole research report for each bullet point um it could actually take actions and do them um importantly like here the the reason that this is helpful and that you can do this as opposed to like a react style agent or something is that all of these are kind of independent so you can do one without doing two and you can do two without doing three or two without doing one um and that makes it really easy to paralyze them which leads to the speed up so if you're doing things where it relies on the results of previous steps that may not be as good of fit for this so let's see see okay so this is prompt one this is prompt two that's great it looks like they're already in Okay Okay so we've got one and this is being used to generate the skeleton so let's copy this um skeleton generator template there's some weird like new line stuff here um I'll just do that now that seems fine and then um skeleton generator prompt and then we're just going to do from template from this template um let's um we're going to import this because now we're going to create our chain our Chain's going to be really simple the first chain so we're going to create our first chain which is generating the skeleton so it's going to be this prompt and we're going to pass this in toi and then we're just going to parse out the string we're probably going to actually need to do other things we're going to need to parse out the bullet points because we're going to want to work with those but for now we can just do this um and then let's test it out um let's see do they have a good example in the all right so this is the example they given the paper we'll use that um some weird stuff happens when copying things over okay let's try that out um let's print out this just to make sure we're also logging things to lsmith um so uh what lsmith is is the debugging logging um uh tool that we've built on the side um and so what I've done before is I've basically copy pasted I'm using my Dev account but um we we you should use the the regular link Smith account it's smithl chain.com um if you you can sign up for Access there it is on a wait list if if you don't have access shoot me a DM on on Twitter or LinkedIn um and we can give you access pretty easily um once you do that I've just exported these three variables you want to pop in your API key there um I've already done that but you you should do that and then what that does is it will Trace everything that we do so here let's do python skeleton of thought chain let's run this and so it should print it out here okay so this uh uh spits out the um skeleton 10 bullet points okay that's a lot we can also see that it should show up um here um so here we have this runable sequence we can see it's really simple it's just a prompt template I'll out preparer what I really like about um Ling Smith is it will let you open things up in a playground so here if we wanted to play around with the prompt for whatever reason um like let's say we wanted to try to change this to just have I don't know two to three points then we do that and we can rerun it and here we can see that just generates two points so this is really useful for debugging things especially when they're in Chains It's also useful for seeing what's going on under the hood which right now doesn't really matter because it is quite simple it's just this but when it comes time to add in the next point the point expanding stage it will come in quite handy so let's take in this point expanding stage and let's do Point expander template um all right we're going to add this in um okay so now this takes in a few different variables Let's uh let's make this point expander prompt chat promp template from template put expander template so this this takes in question skeleton which is what we just generated um Point index and then point skeleton okay so what we're going to want to do is we're going to want to take let's see if there's any good diagrams for here I guess the main diagram wow okay there's a big appendix for this what's in this appendex oh okay so they run it on a a lot of different they have some yeah okay so they have a bunch of different prompts for done different models which is really cool um we'll probably just do a basic one uh one shot prompt um oh that's cool so they do some that's cool so they do they have some like routing between it looks like just a regular response and one that needs a skeleton so they're determining whether this is even needed um anyways long appendex might read that in more detail later um but the main idea is that what we're going to want to do is we're going to want to take each of these bullet points and then expand it so in our prompt template we have question skeleton Point index Point skeleton so it looks like this is the question this is the skeleton the point skeleton would be here Point index would be one point skeleton would be there I think they actually have code for there yeah there they go or there we go they do have some code let's see if we can find where they have prompts in there oh it's probably in here skeleton of thought um what I'm looking for is a parser that'll turn this into a list but we can probably do that okay this isn't really that helpful this isn't really that helpful at all okay so what we're going to want to do is we're going to want to write a some snippet that is going to construct a list of things that we're going to expand so it's going to create a list of things from this output we're ALS we're going to want to have each element in that list should be a dictionary with four things the question the skeleton the skeleton point and then the uh the skeleton index so what that's going to look like is first from Lang chain. schema do runnable import runnable pass through all right so we're going to create let's um let's rename this just skeleton generator chain um okay so we're going to create our final chain or a final chain as run pass through. assign um we already have question that comes in so now what we really want is skeleton and we're going to pass skeleton as just skeleton generator chain so this is going to add a new variable called skeleton and it's going to be the result of calling this great um then what we're going to want to do is we're going to want to turn this into so this will be a single dictionary with question and skeleton we're going to want to turn this into a list of things and then we're going to have our Point Xander actually yeah let's try X expander chain Point expander prompt chat chat opening ey string out parser cool let's try that um Let's ignore this for a moment let's try let's so let's try out this so let's just pretend that would got to the point where we had the question we had this as skeleton um oh that's not that skeleton let's pretend we had this as skeleton um Point index as one point skeleton let's do this cool weird all right so for whatever reason the variables had some spaces in them I don't like that um did it not save um ah there we go okay so it expanded the first one that doesn't actually expand it that much whatever let's um we we fix that with some prompt engineering later on if we really want to so we've got this point expander thing working well now what we really want is we want to write a python function that parses a single number list into a list of dictionaries with each element in the list um add two keys a index key for the index in the number list and a point py for the content let inlo they look something like this and so here we had prompted it with one so I'm going to add that in there and I I'll show how we can do something to just get that so so when it generated it it didn't have the one in front of it but I'm adding it um to start oh interesting it looks like it's using Code interpreter okay cool um we will wait for this to finish in the meantime we can see what was going on with the point expander solution um so this was the point expander um we can see that this is what the um prompt ended up looking like by the time it got into the uh to the uh to the to the llm so this is the fully formatted thing um we're actually missing a one zero one point something there I think so okay cool that that's interesting to know um we'll have to so basically this doesn't generate the one at the start because if you notice we have it here so what I'm actually going to do is I'm going to add something here that's just Lambda X um one so that's just adding it to the response that'll make it just a little bit more convenient to pass around go back to open AI that looks reasonable to me interesting that this chat GPT is getting updated every day it seems all right let's copy this okay so um this par is the list so what I also want to do is um create list elements so so I want something that takes in the output of this which is a dictionary that has question and skeleton and then creates things that can be passed into the point expander which is question skeleton Point index Point skeleton so I'm going to pretend that's input I'm going to um do skeleton equals input skeleton then I'm going to do number list equals par number list skeleton um I'm going to change these to match those things and then for L list L skeleton skeleton why is that getting highlighted I have name r vers skeleton weird um L question question input question return numbered list cool so now I can add this in here and I can do create list elements cool um then I can do ex Point expanded chain should be expander let's fix that um expander expander expander expander thing um math then this is going to get back a list of expanded things and what I want to do is basically uh pass those in uh into or yeah I want I want to combine those into a final answer um so let's just write something like def Final Answer expanded points Final Answer equals that or actually um yeah we don't even need this we can just do Lambda X um join X that should return just a string because it's getting a list of strings back let's try some stuff now delete everything except question um yeah let's run it and see what happens oh okay that was pretty fast so that was faster than I expected so it generates all of this um generates all of this uh by uh well let's see what's going on under the hood this is how it generates all of this and if we expand this okay there we go a lot more LM calls so all these yellow blocks here are LM calls so it actually made a lot of LM calls under the hood it was just done very fast if you look at them in sequence this is the first one and it generates the skeleton and then this is uh mapping over each of them this is expanding the first one um and uh basically so this is expanding the first one of identifying the root cause this is expanding the second one of encouraging open communication blah blah blah we get to The Final Answer um where and then this is the final answer which it takes in all of these and gets back this output if we wanted to change this output in some way um let's uh uh let's maybe let let's let's show how we can do that if we wanted to like format this a little bit nicer right now this is just joining them with that um as you can maybe guess we could just write a simple function like def get Final Answer um which we started to do before but I wanted to make sure it was working turns out it was working it's quite simple actually um final answer string use that um actually let's here's a comprehensive answer um for L and expanded list um and and you know what let's add like that and enumerate expanded list um equals I do L turn Final Answer string drop that in there run it again so now we should get a slightly nicer formatted um final answer so yeah here and it's zero index you know we can we can fix that by going like this but the point is we get a nicely formatted um thing here's a final answer we go back here we can see a new thing popped up input is this question output is this here's a comprehensive answer um okay so it looks like um okay so this is actually helpful so some of them so it looks like it largely okay so this is actually pretty cool so what we can see is going on is the first one it just starts with is and that's because we have this prompt where we ask it to basically continue from the um first one so what we actually want to do is we actually want to add a new element into this thing which is going to be um we can call this expanded answers um and then we can pass it here now this is uh not going to get in the expand so so what we're so basically what's happening is our expanded answers are continuing from here so what we actually want to do is basically take um the uh take this thing and then take this thing and then add in um the add in this part to there so what that's going to look like now is we're going to have something that has question skeleton and then expanded answers um and um or actually what we can do actually I know what we're going to do we're going to reverse this that's fine what we're going to actually do is we're going to change the point expander chain um and we're going to do a runnable runnable pass through. assign here we're going to have continuation there and then we're going to do Lambda x x X um uh it's what are we calling it we're calling it The Point skeleton plus the continuation okay so let's run this see if this works and then I'll explain what it does if it does work okay so now we can see that each bullet point is more of a uh fully featured or of an actual sentence we've still got some weird things going on there likely so let's let we let's debug this a little bit more um so here we have this map now each of these is now basically it's generating this answer um it's generating this answer and then it's appending it to this um and so the weird thing that's happening is that there isn't a space but there should be a space the reason there isn't a space is because why isn't there space well let's see oh I mean there's just there's just no space there um okay so what we can just do is in case there is a space we'll strip then we'll in space um and then there so if we run this now okay that looks good there's proper spacing um they're all uh full sentences and it looks like that should work so um and if we look at what's going on under the hood we have the full thing here um we can expand it out we can see all the calls that are getting called we can jump into any place um debug it hop into a playground try it out with different language models as well one of the cool things that we did um where there we go one of the cool things that we did is we actually worked with fireworks and Google to have some free models here let's see how llama L of 2 13B does um h all right this uh all right so llama llama 13B is not amazing at this where is what about mistr Moll is a pretty good model all right I'm probably not using us the right uh tokens to uh prompt myal correctly so let's stop doing that um but basically yeah this is this we've we've added skeleton of thought as a template to laying chain um oh one thing that we can do now is we can actually see this in action um so if we let's just delete this let's check P project. toml skeleton thought chain yep cool that's all right um what we can do is I think we can just do L chain serve from here could not import model module defined in P project. toml so I think we need do p install D in here to install this module now if we do L chain serve okay so I'm on yeah so I'm on pantic two um we really should make this pantic one by default um but now if I go to this SL playground okay so what I need to do is it can't because I'm using this thing it can't automatically infer what the um inputs are correctly um so I'm going to from base model class input base model with types input type chain input um I think that's do I I don't know if I have to type the output or not um cool okay so we get this um what the most effective strategy contact for what most strategies for conflict resolution in the workpl um yeah so we can see the intermediate steps jump up a lot we get back our answer um this actually also automatically logs things to link Smith so this is uh this is the one that we just ran very recently um but now we have a playground to play around with this as well so if we wanted to share this with uh anyone we can uh spin up a little playground this is just served by fast API and do it this way okay I think that's really all I have now thank you", "metadata": {"source": "wLRHwKuKvOE", "title": "Skeleton-of-Thought: Building a New Template from Scratch", "description": "Unknown", "view_count": 6735, "thumbnail_url": "https://i.ytimg.com/vi/wLRHwKuKvOE/hq720.jpg", "publish_date": "2023-11-27 00:00:00", "length": 1990, "author": "LangChain"}, "type": "Document"}, {"page_content": "hello all right today we're going to go over how to Benchmark some retrievable algorithms using the new uh Lang chain benchmarks package um as well as the public data set that we're releasing um specifically the public data set is for question answering over Lang chain documentation um and so we've provided uh about 86 examples of inputs and expected outputs um and as well as some utilities for doing some basic kind of like in question um and then we've evaluated a bunch of different methods for doing retrieval and then retrieval augmented generation so generating answers to those uh questions we can then kind of like score each retrieval or generation method we'll we'll kind of like change up both pieces we'll score that against the ground truth and see how we do and so we've done that on a bunch of architectures so far um and what we're going to do in this video is we're going to try it out on a new one uh because part of the the the value in open sourcing this is that it can serve as a way for people to Benchmark you know all the different retrieval algorithms all the different models that are coming out and see how it actually does so we're going to be working off of this uh this notebook right here I'm going to I'm going to basically be copying some parts over to my jupyter notebook running it changing some stuff up because uh I want to change stuff up I want to evaluate different things specifically the thing that I want to value8 for this video is uh some sort of like multi-query um query expansion type uh of retriever so in typical rag the question comes in and then you look up five or six documents relevant to that question what I want to do is instead expand a single question into two or three or four different questions look up documents for all of those and then answer based on all of those documents the idea behind this is that when you're looking things up it might be better to break down the the overall user question into distinct sub questions look those things up and then respond with an answer over all of those talks so I'm going to try implementing that and then benchmarking it so I've got this I'm going to copy some of the code over here and install everything that's needed I'm then going to set uh some uh environment Vari Ables um so let's copy this um I need my Lang chain API key so I'm going to go here grab my API key from here U if you don't have access to Lang Smith uh shoot me a DM on Twitter that's probably the best way and and I'll get you an access code um I'm going to do this in a little bit but I'm going to do it along with the other uh environment variables I'm I'm going to do that I'm going to pause when I do that so that you don't see uh my screen and my secrets there the other environment variables I'm going to set at the same time are these I'm not actually going to use hugging face I'm just going to use open AI for everything so I'm going to set my Lang chain API key my anthropic API key and then my open AI API key let me do that and then I'll come back okay we're back I set my API Keys now I'm going to follow this code so I'm going to use uh the Lang chain benchmarks code to check out some we have we have a variety of tasks in here the one that I'll be doing is this Lang chain doc Q&A task we'll we'll we'll go into more details on some of the other ones um but this is the this is the main ready one as of today um so if we take a look at it I can see that there's a data set ID I can click on this this is a public data set that we've published under the link chain org um so you can see that there's 86 different examples um and then there's also a bench of test runs that that we ran um along with some scores for how well those did so once I've done that um or I haven't done anything yet I'm just kind of like looked at what everything's is now I'm going to clone this public data set this is going to move it in to my link Smith account so now if I click on it this is in my personal link Smith account um and I have no tests on it because I just cloned it in here but these uh inputs and outputs are all the same from uh the data set that we collected so if we take a look at some of the questions you know we can take a look we see question will this work from Lang chain. chat models import chat open AI llm equals chat openi model equals CLA 2 and then the answer is here no Cloud 2 is a closed Source model developed by anthropic the chat open AI class interfaces with open ai's chat model so these are um so there's 86 examples some of these have you know simple answers like yes no other of these have longer uh natural language output so once we do that um we're now going to set up the retriever I'm not going to change too much about the retriever um in in this video I'm going to use the basic retriever the only slight difference that I'll do is I'm going to change this from hugging face embeddings to open Ai embeddings and then this stuff uh so so this is loading some of the docks that we're going to use and then this is creating the retriever um this abstracts away a little bit of what's actually going into creating the retriever it's a it's a kind of like handy method for uh just creating a simple retriever if you want to change up some of the logic of this um the Logics in the the uh Lang chain benchmarks code we'll probably try to make this a little bit more apparent in the future what exactly is going on but for all intents and purposes this is creating a really simple Vector store retriever so let's run this and it'll do some work while it's running we can take a look at the next cell so here's where we actually set up the chain that we use um and so let's take a look at what's going into this chain we've got this format format docs function this takes in a list of documents and returns a string so it's basically formatting the documents into some context that we can pass into the language model this could be one thing that we play around with we could play around with kind of like the the tokens used to denote um or pass in various metadata or Source or content um this is definitely something we can play around with this is the prompt um again this is something we can modify right now it's pretty simple you are an AI assistant answering questions about link chain respond Solly based on document content and then the human question we're going to be using uh the anthropic uh new model that was just released um and then uh the this is the so this is generating the final response it takes it's the prompt llm and then it just turns it into a string um it turns it into a string because the chat model returns a chat message by default and then if we look at the the the chain um we can see that we are adding in a new variable um this variable is called context we're getting context by taking the question passing it to the retriever passing the response to format docs this is getting a string so this is assigning a new variable called context with type string and then it's passing it along to the response generator the response generator takes question and context so context is coming from the retrieval questions getting passed along so runnable assign just adds in a new variable so it passes along everything that's in it up to that point we can see that this looks like it is done so let's um so let's copy this code but we want to do some changes specifically what we want to do all of this looks fine um all of this looks pretty fine what we want to do is we want to change this from taking the question passing it to the Retriever and then and then just formatting the documents we want to take this question generate a bunch of sub questions from there we want to uh uh look up all those questions with the retriever combine all the documents put them into a single string and then that's what we're going to use as context so um what we're going to do to generate those sub questions is we're going to go here and this is we we call this there's there's two different ways that we kind of refer to this we call this multiquery or query expansion so I'm just looking in the prompt tub to see if there's any prompts that I can use to kind of like help with this query expansion doesn't return anything if we do multi-query we can see that there are a few different prompts um including one that that I uh that I uh created when I was uh checking this out preparing for this um let me um let me try to delete this um or I'll just work with it basically if you want to work with your so you can see that this is forked directly from Jacob multiquery retriever what I what I don't like about this prompt is that it's asking it to generate different versions of a given user question and I really want to generate more like sub questions so if I try this out in a playground if I pass in I also don't like the fact that I have to specify query count here I want to like dynamically choose how many it should generate but let's just say there's three and then let's say say the question is like how do I use open AI with chroma DB um let's pick a model um let's pick yeah 3.5 turbo seems fine um so if I look at all the questions that are generated they're um they're not they're all basically like variants of the same question which are exactly what the template asks for what the prompt asks for but I really want more specific things like I want something like how do I use open how do I use chroma DB it can look up things for both of those and then it can kind of maybe figure out how to use them together along with the original question or or some rephrase of how to use them together in case there is a document just around that so basically I want to modify this so what I've done um actually yeah what I did was I clicked the fork button and then saved it as my own um so now what I'm going to do is I want to modify this a little bit um so also I think there's some weird entation in this prompt so I'm going to say like I want to generate one to five different sub questions or alternate versions of a given user question to retrieve relevant document from the vector database by generating multiple versions of the same us question by generating sub questions you can break down questions or concepts that you can break down questions that refer to multiple Concepts into distinct questions this will help you get documents for constructing Final Answer um yeah I'm fine with XML tags so now let's try this out how do I useb together um let's use chat openi for this let's start seeing so it's still kind of focused on how to use them together um if uh if multiple concepts are present in the question you should break into some question one question for each concept um let's try this out that's amazing okay so that's what I want I might have like overfit slightly on this example that's fine there's a lot of prompt engineering that goes into things like this we can evaluate this and see how it does that's the whole point of this video um so let's commit this um to there um if we go back here cool it is updated um let's copy this because we're going to use this in our code code um and then from model chat that's fine oh yeah don't want to do that quite yet chain let's call this Q gen equals um question Cool okay so it's generating each of those um so so I also want to probably keep around the original question as well so I basically want to take the original question question generate some documents generate some code questions get some documents for those look them all up comine all the results fine um so now I need to add in something that basically splits this apart um oh I also want to add in a stop sequence um because I want it to stop when it uh this is just going to force it to stop when it generates that to now let's see this um okay cool um I don't like how it's doing one two three four um so let's go back here let's open this in the playground um let's uh let's wrap these in quotes and see if that fixes some of it to switch this how I okay so that seems good so so wrapping in quotes um i' still uh quotes are going to be like a little bit annoying to parse out so let's just do bullets and okay cool um oh yeah you didn't see there okay so that's so I don't have the stop token here so it kind of like runs on and creates two blocks which my stop token it'll save that so let's commit this now if I pull down this new thing cool I get back some bullets um so what I'm going to do now is I want to basically write some output parser or some parsing logic that's going to split this into a list of questions um so let's do string um I can first I want to uh let's get rid of this so let's do text text strip um questions um let's do questions equals uh text. split new line Dash space return question let's add this up here let's now type this in to split let's run this okay so it's getting one so we can just do um it's getting one like weird thing there so we can just do return Q for Q questions if Q I think this actually works cool so now I've got a list of questions um let me just strip that andri that I will get rid of the the new line that I saw there okay so this get a list of questions now what I want to do is I basically want to uh uh create this okay so I have this Q gen this returns a list of questions one thing that I could do is basically just Q gen um Q gen retriever map so what's going on here and and actually let me let me combine this into retriever chain equals Q gen this is going to take in uh uh question return a list I'm then going to do retriever do map this is going to take each element of the list pass it to the retriever so it's going to take a question pass it toet take second question pass it to the retriever then I need something to flatten this list of documents or format docs so I have this here um what I'm going to do is I'm now going to create version that is format list Docs um this is going to be a sequence of sequence is because it's getting here it's each element that retriever returns is a list so it's going to get back a list of lists um and then uh I'll do doc lists for docs in Doc lists that that um so there's going to be some weird things because of the index I um so what I'm just going to do is I equals [Music] zero for docs for Doc and docs I plus equals one cool um and then let's do format plus Docs this is a retriever chain let's try this let's see what happens so this should get back a really long string because it's going to have all the docs concatenated so yeah we get back this really long string cool we're using CLA 2 because it has a really long context window so this is still a really long all right so we're getting a lot of context back maybe this is good maybe this is bad this is why we have evaluations um so let now we can do retriever chain um and I don't actually so so this plucks out the question but I don't even need to pluck out the check question I can just do like this um and let's wrap this and so this is just some nice config to make it show up nicely in in traces um but I can just do this this is going to take everything pass it to retriever chain retriever chain expects uh a dictionary with a key question so that's perfect um cool okay so this is my new modified chain let's try it out on this question if we're curious about what's going on in the hood we should be able to see things in our uh we probably haven't set tracing equals to True okay so let's actually do that before we go any further because that'll be very helpful Imports um if I want to have things logged here I can go to setup I need to set this equals to true I wonder if this will work if I have to restart the notebook if I have to restart the notebook then I will not do that um and I think I may actually have to um yeah okay so it's not working the the the tracing isn't working because I haven't uh set the environment variable before that's fine um would have been good to do so I could see what's going on but it looks like the response that it's giving is is it's reasonable um actually I think it would be really cool to see what's going on so let me pause the notebook or let me pause the video restart the notebook with that environment variable set and then we can see what's going on under the hood because I think it's going to be really cool okay so it was actually uh running before it just wasn't logging to the project I expected it to be logging to because I had some other environment variables set from before so I changed it up I'm now logging to uh dedicated project for this multiquery eiles if we click into what was just logged from this run um we can see a few things that are going on um so we can see that there's this step which is basically going to be the retrieval step um that has this format docs thing and then there's this chain that has generate response and this is where it's generating the response so this is the retrieval this is the response the retrieval is using this multiquery thing so if we look in here this is the prompt that we had um it's taking in the question and generating uh uh two sub questions to ask we're then parsing that and then there's two calls to a retriever here so for in each of these we can see basically the documents that are returned I'm actually curious whether these are the same or not so how-to cookbook linkchain expression language introduction how tobook linkchain exression language introduction so these are actually returning the same things um one thing we could and should do is D duplicate these in format list of docs um I'm debating whether it's worth doing that live or not uh probably will'll leave that as an exercise left to the reader um but then that's all pred into this anthropic model um and so you can see the very long system prompt here because it has lots and lots of context um and then it returns uh this actually I I probably will um spend some time D duplicating it so we can see here um that uh okay so I think what I'm going to do is I'm going to add this um for do hashes um I'm not could be a set to be faster but not too worried um I'm just going to Hash the content for now I could hash everything actually I'm I have the doc string actually wait yeah this is going to be uh way easier than I thought um if doc string not in formatted docs then I'm going to pend increase ey okay so now if I rerun this hopefully I shouldn't change the answer too much it actually should just be completely D duplicating um things and I can peek out what's going on under the hood this again runs in real time so I can kind of see what's going on um still pending a bunch all right there we go I was just debating whether to pause the video or not so good thing that it went through um so now if we look at what's going on we can see that the retriever oh okay so it's changing it up it's actually generating uh three sub questions um so it's generating uh yeah three sub questions now I don't have temperature set to zero so we are seeing some Randomness this is a cookbook and then in cookbook introduction how two L expression language cookbook introduction how two L expression language okay so it's getting the same docs if we now look at format list of docs um this is should be shorter than uh let's make this wider what we can actually do to see if it's shorter or not is we can look at some stats so in this final thing um if we click into here um we can look at the uh input um and we can see this input here we can look at some of the things on the side so this is right about 10,000 total tokens if we go back to the previous one and we look at everything this is at about 6,000 total tokens y this should be duplicating why is it not duplicating um it is not duplicating because we have the i in here okay so what I'm going to do real fast is just look at okay so I do need this then I guess because I have the ey in there actually um yeah let's change this up let's change this up a little bit let's remove this for now and this for now and then going to do something like this thing for I and enumerate form a docs remove this remove this um so now I don't even need ey so basically the I was causing things to be different that was the only difference so now I can just format them you duplicate them along the way and then add in the I later on so if we do this we do this we do this we do this now this should result in fewer tokens so let's take a look it's still pending it'll take a little bit awkward pause in the video there we go and if we take a look at this trade we can see here it only uses 3,500 tokens yay we've successfully duplicated uh the documents so that's going to save us some good tokens um as we as we start using this all right now we're going to run it over the whole data set and get some evaluations so let's copy this code we'll put this here let's copy this code we'll put this here and this is running on the data set and we're getting some progress of what's going on we can take a look at it in Lang Smith and we can see that we're running things this might actually cause some errors because I have some rate limits on anthropic that are not super high regardless this is going to take a while to run so I'm going to pause the video If I make any changes I will restart the video and say what I did but if not I'll restart it when this is finished running because this is going to take a little bit and I don't want to sit here for a while all right so we're back if we look at our notebook and finished running we get some nice um metrics from our uh uh evaluators that we're using so we calculate the embedding cosine distance the the accuracy score that's uses in llm um and then a faithfulness score as well um and so if we go into here now we can see that we have one test run we can click into it and we have a uh we have a bunch of data points we can look at it what class type is returned by initialized agent the right answer is the agent executor class this gives a more lengthy explanation but it still says the agent executor class we can see that nice it gives a string score of one so this is using the llm and it's recognizing that this is the same as this technically you know it's not exactly the same that's why we have to use an LM the eming coine distance is um I think that's seems to be yeah okay so that's higher than usual so that's not great so so the main metric that we look at here is this score string accuracy one um we can do some filters so we can look for so let's add uh less than5 so these are ones that are actually let's even make this worse less than3 let's try to find ones that are really bad what does on tool start mean so this is a method that is defined on classes um uh that works with callbacks um nothing in the docs that that we could find if we want to figure out like so this gets a bad answer because it doesn't give the correct answer um at least it's not making up an answer um so we should uh uh you know we we can add other metrics to calculate whether it makes up something or whether it just I think that actually might be what faithfulness is um one thing we can also do is if we click on this run um we can open the run and we can see what's going on so we can see everything that's going under here we can see that we have this multiquery thing that generates like what is the meaning of all to start can you explain the definition of all tool how would you define the term on tool start we can then see the retrievers that are used um these are returning slightly different things Okay cool so we're getting um some differences we can then see the model that's being used um and uh the output there if we go back overall we can see some overall metrics so we've got a faithfulness of77 and an accuracy of 63 if we compare that to the original public version of the test set we can see that accuracy um uh I don't think we have we don't we didn't run faithful enough on this one so if we look across accuracy this is actually crushing all other accuracy metrics um point so 63 that's higher than the best one which was the 61 so 63 is you know and and I I barely did anything here I think this actually speaks a lot to the power of the multiquery or this query expansion thing it's really really powerful um there's a let me try to find uh open AI retrieval Dev day so when they talk I'm looking for one image in particular this one yeah okay so if you look on this um if you look at this image query expansion they have this jumping up the Gap let me close my messages there um if you look at this it jumps up it it's it's one of the things that worked and so I think query expansion is really really powerful um so yeah hopefully that uh that helps um show how you can evaluate new retrieval methods um um we are going to add more data sets to evaluate on and we're also going to make the public benchmarks actually show more comprehensive leaderboard that you can easily explore because right now it's a little tough to know what's going on if you look at this we have a corresponding blog post I'll link to that in the YouTube um description I'll also link to uh I'll link to that I'll link to the The Notebook here so that you can run it yourself I'll link to this public data set um yeah that's that's pretty much it um really looking forward to continuing to Benchmark things as well as uh to to see what you guys come up with um you know there's lots of cool ideas out there ideas are only as cool as you can Benchmark them so let's do that", "metadata": {"source": "ObIltMaRJvY", "title": "Benchmarking RAG over LangChain Docs", "description": "Unknown", "view_count": 3344, "thumbnail_url": "https://i.ytimg.com/vi/ObIltMaRJvY/hq720.jpg", "publish_date": "2023-11-22 00:00:00", "length": 2083, "author": "LangChain"}, "type": "Document"}, {"page_content": "today we're going to build a research assistant from scratch This research assistant from tavil AI is one of my favorite llm applications it's complex it's more than just chat it's actually useful and it performs well um and and so we're going to build this from scratch using Ling chain and uh take a look at what goes into all the decision points along the ways so if you're not familiar with tavil AI research assistant it is a web hosted platform although there's also an open source version and we'll talk about that but it's a web hosted platform where you can give it a task such as what is the difference between Lang train and Lang Smith that will do a bunch of research from the web and then write this really long report so this is much longer than uh uh you know you might get back from chat GPT or or one of the other search engines it takes a lot longer as well as we'll see there's a lot going on behind the scenes so you're not going to get back this instantaneous response so there's there's pros and cons but I really like this because you are able to do a lot of research behind the scenes you are able to put more work in and that generally allows better and more interesting responses so tavil AI is based off of this open source uh repo called GPT researcher um we've worked very closely with them at linkchain um and and to understand what's going on they have this great diagram and so what's going on in the hood is it takes in a given task and then for that task it generates a bunch of of research questions so it will generate a bunch of research questions and for each one it will then look that up online it will get a bunch of web pages so this is a web research agent we'll talk about how to make it more generic than that but this is a web research agent so we'll get those web pages it will then summarize those web pages and then using all the summaries it will generate a final report so that's going on what one the hood a lot of moving pieces a lot of it can be parallelized as well so I'll talk about how to do that so it gets a little bit faster and uh uh yeah let's uh let's jump into it and build it from scratch we are going to be using open AI so if you don't have an open API key you can go ahead and get one um from open AI uh themselves um and then we're also going to be using lsmith a bunch to kind of like show what's going on under the hood so this is a logging debugging tool that we built out Ling chain if you don't have um access to this yet feel free to shoot me a message and we can get you off the wait list so the first thing I'm going to do is I've got my environment I've got my code editor set up what I'm going to do is set a few environment variables I'm going to set the open AI API key and I'm going to set my lsmith keys I want to pause the video and do that now so you don't see my secrets all right done so now uh we're going to install a few packages um we're going to install L chain open Ai and then Duck Duck Go as well so we're going to use them as our search engine got to look up what their python package name is here it is duck duck go- search so we're going to install that um we made some more libraries but we'll install that um to get started while that's going let's think about what the first thing that we want to implement is so let's start from one of the smaller kind of like parts which is basically the summarization of a web page so at the core what it's doing in the middle is it's taking a question it's taking a web page and it's summarizing the response and the reason that we want to do that is a few fold one there's going to be a lot of different web pages and a lot of different information here so maybe we could pass this all to the final report agent maybe the 128k context Windows enough but even if it is it's generally more performant a split up into these smaller tasks um and ask ask the language model to kind of perform those it's more focused for one um you can paralize it better for two um and then three you know you can use other models besides the really long context window ones so let's focus on this first task where we're going to take in a web page take in a question and summarize the response so let's write this so and then once we get this working we can work worry about kind of like scaling that up so just as an example let's go to the Lan chain log um let's pretend we're asking about the difference between L chain and Lang Smith Let's Pretend one of the sub questions is like what is link Smith so linkchain blog link Smith let's find one of the blogs all right so let's take this URL let's just set this as the URL um what are we going to import from Lang ch ch. chat models import chat open AI from Lang chain do prompts import chat prompt template um well let's uh make some prompt template um hold I'm going to pause it and fix my python interpreter issues to avoid all those R Squig cool all right we're back so let's make our template let's say our template is going to look something like summarize the following question based on the context we're going to update this this just a placeholder we're just going to get started let's put question question context context prompt we're going to set chat prompt template from template template um okay so that's that now we need to figure out how to get the content from this URL in order to do that we're going to need actually two more modules we're going to pip install requests and then we also going to want let's look up beautiful soup I may have spelled that wrong um yes so this is the one we want um and let's add that these are going to help us scrape some uh web pages all right so we've added those we're going to go back we're now now going to create a function to get the content of this web page so I'm going to put this here import requests um from PS4 import beautiful soup this is just a nice little helper function that's going to scrape text from a web page it takes in a URL then hits that with their requests um then scrapes it with beautiful soup if it was able to otherwise it just uh prints that and prints that prints the error and says that it's failed to retrieve the web page cool so we've got that we can now do something like um page content equals scrape text URL um this could be really really long so what I'm going to do is I'm just going to take the first like 10,000 characters that should probably work um I am am then going to create a little chain to get this working so let's do prompt chat open AI um model let's use let's use a longer context one so open AI models um let's see what we can use Let's uh use let's use this one you know cheaper longer content window that sounds fantastic so let's use that um we then so this is going to return a chat message we just really want the string from here so from Lang chain. schema. help parer import stir out wither this is really simple just parses the message into a string um let's call this our chain and then let's do chain dot in v um uh question what is BL Smith content um page content so again we're using this pretty basic prompt we'll update it it'll get better let's just make sure this works right now um so let's do Pyon main.py we run it okay we get some error context cool update that run it okay I forgot to print out what it was luckily we can go to Lang Smith we can see here we've got this um and we can see the chain really simple one only three steps prompt template L um output parser input is this uh input is basically this dictionary of context and question and then the output um and then uh we've got the answer here the question is asking from Lang Smith which is described as a uni platform for debugging testing evaluating monitoring all applications cool it's working um let's update the prompt that we're using a little bit awesome so I've updated the prompt now this is actually taken from uh gbtc researcher repo um so we're going to yeah we're going to use that heavily and and and um take a lot of inspiration from that so we've got this summary template it's you put the text up here you ask it to answer the question um updated down here um let's uh run this again make sure it's working notice that I changed the context key to just text to update the prompt template that they were using if we go back here we check yeah so we can see that we now have a different response um and looks a little bit better than before the previous one uh said that it was doing a summary this just responds that's better cool um let's make this um let's make this chain a little bit more involved so what we're going to do now is let's actually um add in a step that is going to look up the and get the content by itself so we're going to import from linkchain um schema runable will import runable pass through and so what runnable pass through is going to do is going to take the current input and add into it so the current input will just be a question and then what we're going to do is we're going to set text equals to Lambda um and actually let's make this not only a question but also a URL URL so we'll pass in url here we'll go over how we're going to get those URLs in a little bit um so let's set text equals to Lambda X and then basically this bad boy right there so now we're actually doing so we're moving more and more of it into the chain and this is useful because we're going to package this up into one big application later on so let's run this now we get a here text um why is that happening that is happening because okay so what was happening was basically I'd forgotten to put this assign thing here um assign is the thing that actually uh does the assignment of text to a text variable before that it wasn't doing anything I also updated this to be uh uh to pluck the URL key from the input X and pass it to scrap text before I had just hardcoded this here but of course that's wrong we want it to be as part of the input we can run it and we can see that we get something like this where we pass in the URL on the question and then it basically uh runs uh behind the scene gets this output for text um passes it into the prompt template passes it into the passes into the output parer all right so we've done one part which is we have uh we've gotten to a place where we can take a URL and a question and and we can uh get content for it and and then summarize it so let's now go uh one step further and let's do this for this question we're first going to look up stuff using duck Dogo and then we're going to get the results from duck. go take like the three URLs and then we're going to apply this to each one so let's see what that looks like all right so I've added in a few things I've imported duck ducko search API rapper from Lang chain this makes it super easy to work with um and defined um uh this wrapper and then also Define this web search function which is going to take in um the query the number of results that we want to get and let's just set this to three um as a default to make it easy and then uh it's going to return a list of links so what we want to do is for a given query um we want to look up the list of URLs and then we basically want to apply this function to that okay so what that's going to look like is let's do web search um and we're going to wrap this in a runnable Lambda and basically what the runnable Lambda is it's going to let us Define a function and then from there um but it's defining this function in this uh and using Lang chain expression language so that we can easily compose it with other things um so let's do X web search x. question um cool this is going to return remember a list of URLs from there what we want to do is we want to turn that list of URLs um into something that we can pass in in to uh uh this chain so the chain takes in dictionaries with questions and URLs so what we're going to now do is actually I think yeah so what we're going to do is actually let's use runable pass through again because we just want to assign um let's assign URLs equals to that um then so this is going to add a new key called URLs then what we want to do is is we we're going to have this be a dictionary it's a question and then a list of URLs we want to turn that into a list of question and URLs so let's we can write some function to do that Lambda X um we can do uh question X question um URL U for you in X URL so We're looping over the URLs that are passed in and then for each one we're constructing a little dictionary where there's just that URL and then that question so this is going to give us the input and it's a list and so then what we can do is we can call let's name this something like um and summarize chain what we can do is we can do this and we can do this map thing this map is going to apply this chain to every element in the list so let's now call this chain that um let's do oop that's the wrong thing let's you chain. invoke we can now remove this URL because we're going to do the researching of the URLs under the hood let's run this see what happens okay so it took a little bit let's check out Lang Smith and see what actually happened so we had this input and we get back in output which is a list of things um so it's a list of things and so what's going on under the hood is first we have this thing which is getting a list of uh blogs to look at this is then getting converted into a list of dictionaries with each element in the dictionary having a URL and then a question then for each of these elements we're applying this chain and this is the exact same chain that we were working with before so if we look into it it this is the one where we're asking um things to we're asking to summarize the above text in short with the following question perfect this is working great so we've gotten to a point now where we have something and we can generate some answers still returning a list um that's probably fine what we want to do now is this is still just one question what we want to do is generate a bunch of sub questions and then from there we want to apply the same chain to all of them that's going to get back a list of lists of things then we can then combine all of those things into one big thing and then pass that to the final response okay so there a lot to digest but to start we're going to work on adding a uh llm call that'll generate a list of sub questions to go research all right I now added this new search prompt which and again this is taken from GPD researcher so it's going to look up uh basically um uh let's remove this part so they have an agent prompt we're going to skip that I'll I'll link in this uh I'll link in the description to their repo and then also our Lang chain template which uses this it's more advanced basically we're going to write um we're going to use this one which just writes uh three Google search queries to search online that form an objective opinion from the following question so we've got that that um we can now construct a search uh search question Chain by doing search prompt um we can then pass this into chat open AI then from here what we want to do is we want to split this out into so this is returning like a list and then each one is a uh it's returning a list of quoted search questions or search queries so we want to now parse that bit out that bit is actually just Json so we are just going to import Json where then going to use the string out parser to just get the string and then we can do json. loads I believe this should work and this will now return a list so let's double check that that is right and let's just run it on this and let's start now changing this up what is the difference between L Smith and Lane chain let's run this let's take a look at what's happening this is now again just a simple list and we get back out as our final output we get uh some questions Lang Smith versus Lang chain similarities and differences pros and cons of using Lang Smith user reviews and experiences with L chain so these are all sub questions that we now research to answer this overall question and so in order to do that what we're going to do is we want to take the output of this and we want to pass it to this chain up here that we created let's call this like web search chain and we want to pass each element into this um again we need to do the thing where this is returning a list of questions each one takes in each of these takes in a single question so what we want to do is let's do search chain and we'll type that into web search chain. map again let's call this let's see what happens cool okay so we got uh some error with the input to runable pass through. assign must be a dect let's look up to where we're using runable pass through. assign which is right here and so what's going on is this returns a list this actually expects the input to be a list of dictionaries but this is returning a list of strings so we can just map that pretty quickly um with a nice little function Lambda X um and then we can do question Q for Q in X um try that we to see what's going on we can actually see this while it's running um so we can see it start to populate still pending looks like it's done yep all right success we can see now that uh we have this question it uses this it gets back a list of um questions I'm going to set temperature equals to one just to be easier to uh debug so I set that to that um and we can see that the final answer is this list of lists things um cool okay so we get this list of lists um let's um now pass this in to a final prompt and ask it to basically generate a final report so what we're going to do is we are going to need something to flatten that list of lists into a nicely formatted uh string that we can pass in and then we're GNA need a prompt that we can pass into that string so let's first get that prompt cool all right so we've taken some prompts again from GPT researcher we've got the system prompt and then we've got this uh more in-depth prompt that writes a certain style of research report you can easily swap these out but we'll construct this with the writer system prompt and the research report template this takes in two variables research summary and question so question we already know what it is um and so now we need to add research report summary this is going to be coming from this chain let's name this full research chain but we want it to be a string right now we have a list of lists so what we're going to do is let's use runable pass through. assign again let's call This research summary because that matches the variable here let's do uh full research chain and then let's do Lambda X um we want to be joining things together um so let's this is just going to be some simple string formatting we can easily change that if we want um so let's join together um various things the things that we're going to want to join together are actually let's just write a little hper function for this to make this easier um def collapse list of lists list of lists content equals that for l in list of lists content. append add two new lines now we're going to join this list then we're going to joining all the content together cool let's add this function in right there okay so now we've added in the research summary we can now pass this to prompt we can now pass this to chat open AI although we're going to want um we're going to want both this we're going to want the long context window if this isn't long enough you know we can use anthropic or something like that that um and so let's get chain equals that and then let's see what happens so this should generate all the questions get all the research by scraping and then summarizing and it's going to pass it to this final prompt at the end let's kick it off let's see what happens so we've now got this this is starting to uh This is Gonna generate all the things that we want so for each one it's looking up each of these questions it's doing more summarization for each of these blog posts um and we can get back uh uh some responses looks like it's finished Okay cool so if we collapse this there's a lot going on here so if we look here we can see that this has all of the work that we were doing before and it's got this list of lists which is now returns this single string um which has all the stuff that we researched online we can now look um at the uh uh chat model that we're calling where we pass in this system prompt and then this prompt with all this information and then we get back this resour report report difference between Lang Smith and Lang chain introduction Lang chain and Lang Smith are two products developed by the same care it goes into some of the focuses it adds features target audience competitive landscape potential impact and then conclusions doesn't do a great job of signing it sources so one thing that we haven't done is we haven't passed through the URLs okay so what we're going to now want to do is let's actually update our scraping so that we do that so back in the code we can see that in our scrape and summarized chain we're just passing through the summary we're not actually doing anything with the urls so let's change that so what we're going to do is we're now going to have this be summary um and I actually think what we're going to want to do is we're going to do we're going to do a run runnable pass through summary equals this um so now we're getting this back so now this is going to have all the previous inputs which are question and URL and then also summary and then what we're going to do to get the final thing is we can do Lambda we can just simply do some nice string formatting and we can do URL X URL um new line new line and then we can do summary that summary so then this should now we're not just using summaries we're also including the URLs let's run this and see what happens okay so we get back an answer it's missing Key summary um let's see did I misspell something um I forgot to do the assign again nice thing again for long running things we can check what's going on here let's dive into it a little bit and so we can see here actually let's look at the collapse list of lists so here we can now see that there is in this list of lists that gets collaps into a single string we can see that there's this URL summary URL summary I could maybe do a little bit better job of formatting it so that they're all together um and yeah that that's one Improvement that I can maybe make let's check out the final answer of this now um so if we look at this Final Answer um it cites things right so it's got these uh URLs and it's citing it accordingly um awesome so that so we passed in more information and now started to site things um let's do one final thing which is just expose a nice web UI for this so we're going to do that with Ling serve which is something that we added from Ling chain to make it easy to deploy link chain apps it's going to give us a nice uh little playground as well so if we go to L serve we can see this beautiful little example app right here I'm going to copy this I'm going to now I'm just going to add this in pretty halfhazard L at the bottom um I need to pip install Ling serve it's going install a bunch of packages um takes care of that looks like I need to install fast API as well easy um we can do this um what I want is I want to take this chain this is the main thing I want to serve the thing that I copy has a bunch of extraneous things we can easily remove that um we can now do add routes app chain this is all useless stuff from before let's call This research assistant um and then we can do if we do this install SEC Starlet that's an easy fix no need to run that twice query run main install unicorn burun may now we can go here and we can add in research assistant playground and so now we get this thing what is the difference between L chain let's change it up what's the difference between L chain and open AI so this is a nice little this is all autogenerated we know that the input's question because we we know the internals of the chain that we wrote you can see the intermediate steps streams things automatically um Lang chain and open a are two prominent entities in the sphere each offering unique Frameworks and models so not exactly right we don't offer any models okay here we go open the eye provider of Link language models um Lang chain is a framework for language model applications cool so it gets those right general purpose versus chat focused um okay so it talks about the two different classes in Lang chain talks about our Integrations um developer platform um and conclusion and so we get a bunch of sources as well so that's pretty much it for this video um I'll post the code for this um in a in a simple gist or something um I'll also post uh the code for a more complex uh research assistant um oh let's maybe do one last thing let's maybe change this so instead of scraping the web it's using a different retriever of our choice and this is really interesting because uh you can now change it to do to do research over any corporate of data that you want so we'll change it we'll do some research over uh let's do some research over um over archive data all right so I've done some basic setup I've imported the archive retriever from L chain and I've got uh I've created the retriever class here what this retriever class does is it takes in a string and it outputs a list of documents and so what we're going to do is basically this thing takes in a string and then adds in the summary and the summary what this do is summary doing is it's scraping text and so basically we want to replace um the text that's scraping from the website with a document and this is actually getting called we can see here that it's getting called over this uh uh uh thing where we list where we list out a list of um pages to look at so we kind of want to rewrite this web search chain so let's uh let's actually comment out of this a bunch of this stuff we can revisit that later web search chain um so let's uh do this instead of URLs though we're going to do documents um and we're going to do retriever do get get summary as documents um so this is just going to use the summaries of the archive papers you know we could we could do full archive papers if we want we could do other things but this is going to get um the summaries and each of them are going to be their own documents um we can then do this to turn this into a list of questions and documents and then the scrap and summary chain needs to change a little bit um because basically let's copy here we know longer need to look things up okay that's one thing um so but we what we do want to do is we want to okay so the summary now the summary prompt um summary prompt let's look up what that is the summary prompt takes in this text in this question so let's copy this down here we're going to modify this slightly because now we no longer have a variable called text um we have a variable called doc so let's modify this let Tak in Doc um and that looks like it should do the trick so we are now pass calculating the summary by passing in summary prompt we've got this getting back here we no longer have URL okay um so I believe let's actually see what this gets us so if we do retriever dot get summar as doc what papers did know sayaz here right um let's do this um run this a cool install archive let's run this let's go to Lang Smith see what's going on and it is not logging things because we're not passing in the call backs um that's fine we can do uh uh oh it's not logging things because we want to do uh get relevant documents there you go this will now log things to L Smith cool we get this get back a list of documents what papers didn't know Z right um these are pretty bad oh there's one okay talking Head Detention um this doesn't look like it has anything um gated lineary units also good Okay cool so we get this um let's check out the paper we can see that uh there's the um there's information on publish authors titles um let's change this to be title um so we're going to update here title title I believe yeah that's good and then this actually is part of the document so doc um doc meta data um that looks good okay so let's do web search chain do invoke any question do this cool we have this what papers did know zero right um we get back a list of it's got a bunch of stuff in it because we're using the runable pass through um and we can see here that we get um we have this a list of documents um okay cool and that's because web search is it's not doing the scrape and summarizing yet cool we can add that in by um yeah we need to add that in up here so let's add in this um and then that and then if we run this you can see that we now get back this list of documents um and then this is the uh this is the this is the final output that we get for each so uh title and summary um and we get that um okay cool so we now have a new version of this web search chain let's now uncomment all of this let's now get this up and running let's go back to our playground what should we ask it um what's a good research question to ask it um uh let us ask get what are some advancements that have happened in LM prompting strategies and so now it's going to be using archive as the background for a lot of this in addition to seeing the intermediate steps here what we can also do is we can follow along in the UI here so this this is this playground's very cleanly hooked up to L Smith and we can see um okay so we're getting some uh streaming error in the output um and basically what's going on is there's some serialization that's not implemented this is happening for uh okay so there's some object it looks like on the retriever that's not getting serialized looks like that's largely fine here and so we can see that we get back this big this awesome research report chain pole Chinese prompt attack data set Dynamic strategy chain structured Chain of Thought causality Centric approach prompt middleware metacognitive prompting insights and strengths exploring prompting strategies so there so it writes the report this is all based on archive papers we don't have the URLs but we have uh the titles um if we did some more prompt engineering we could probably get the urls actually I think we need to update the archive um the archive retriever to return those but this is a basic idea you can um create a research report by generating a bunch of sub questions answering those sub questions those are tied to a particular retriever this is where you can really customize it and make it your own plug it into your retriever of your choice is it archive is it PubMed is it toil search um so for the web uh is it a vector database where you've stored a lot of your information you can then take all those documents summarize them with respect to the question generate a final answer and uh there you go that's pretty much it hope you enjoyed", "metadata": {"source": "DjuXACWYkkU", "title": "Building a Research Assistant from Scratch", "description": "Unknown", "view_count": 22397, "thumbnail_url": "https://i.ytimg.com/vi/DjuXACWYkkU/hq720.jpg", "publish_date": "2023-11-16 00:00:00", "length": 2620, "author": "LangChain"}, "type": "Document"}, {"page_content": "a little bit about Lang serve um Eric will talk about uh Lang chain templates for the majority and then we want to take a lot of user questions so if there's questions uh that you have drop them um in ideally in the Q&A box so if you look on the right there's a little chat box um that you guys are all in by default and then below that there's a Q&A box um drop them in there you can you can upvote um other questions as well so that way the most popular questions will kind of uh rise to the top uh um and yeah we want to leave a lot of time for questions so we'll leave probably like 20 or 30 minutes for that so so feel free to start dropping them in there right away um and the biggest question we usually get is whether this is being recorded and it is being recorded um so it will be available at this link after the fact and then we'll upload it to YouTube as well um yeah so I think that's the main Logistics um so with that out of the way um yeah my name's Harrison um uh uh CEO at linkchain um and I've done a bunch of these so hopefully you guys recognize me and then Eric do you want to introduce yourself yeah and I'm Eric I'm an engineer here at L chain this is my first webinar so nice to meet youall and I'm doing a lot of work on templates in the CL awesome so so I'm going to talk a little bit about uh Ling serve this start um and so um the main thing that I'm going to walk through um is just the read me on the Lang serve page and and walk through it and explain it so I put the link for that in the chat as well if you guys want to follow along there um and so um what uh uh what Ling serve is is basically the easiest and and best way um to deploy kind of like Ling chain applications um and we didn't build this for a while because I think like uh we didn't know how much value we could add on top of existing kind of like serving um uh uh methods that that were out there um but I think with some uh changes and advancements in the L chain core Library as well as just ideas that we had about how to what like a good Lang chain deployment would actually look like um where we're excited enough about this to have have put some work into it um and so yeah as a high level overview this helps people deploy um L chain runnables and chains as as a rest API so chains um are are kind of like the traditional chain class and then runnables if you guys have noticed we've been doing a lot of um like the the pipe syntax with the Lang chain expression language over the past few months and we just call those rles and the I think the reason that this is so powerful as well is um it's really easy to create custom versions of those and one things we've noticed is people don't just want to use like the conversational retrial chain out of the box they maybe want to like modify certain parts of it um whether it be the prompts or the logic under underneath it and so having this runnables is a really easy way to dynamically create things um and edit things um is really powerful we use fast API and pantic um kind of like under the hood so pretty uh traditional um pretty traditional uh uh Technologies there um and and then some some other there's there's a bunch of features um and so some of these are uh all of the Lang chain objects that you write even the custom ones that are in this Lan chain expression language we uh uh try to infer as best as possible the inputs and outputs and so this lets us automatically generate kind of like API schemas um that that you can call um we also yeah so we autogenerate these API docs um we expose a bunch of different endpoints so invoke batch stream and then stream log so invoke is just like one thing in one thing out batch is multiple things in multiple things out streaming you or stream you get back a stream of of of things which is important if you're streaming llm calls and stream you only get the streaming bit of like the final response stream log actually streams every intermediate thing um so this is useful if you want to stream the intermediate results of a chain or the intermediate steps or something like that and we can do these all um or we can expose all of these end points because of of some stuff we've done under the hood in link chain to make it easy to to do this and then because we know that these methods exist in in the main Lang chain class we can expose them as end points we've added like a playground page um where you can easily interact with your chain um so super simple UI you know it's not going to be the prettiest thing but it will kind of like get the job done um everything kind of like integrates with Lang Smith and we're working on adding more Integrations here um it's built on top of yeah battle tested libraries uh We've added a runnable so we've added like a simple SDK basically um for calling the API um and so this makes it easy to basically wrap it and and and and then call it you can also call it directly as a as a rest API um and then we've also added a bunch of templates um and so Eric will walk through some of that as well um going through some or highlighting an example of what this actually looks like and Eric will walk through this in more detail with the actual real example but this is kind of like a toy example so what you can do is you can um you you can basically uh import fast API so it's built on top of fast API and then you can import Lang serve ad routes um you initialize your your app um and then you basically use add routes and pass in one of these runnable things and so a runnable could just be a language model um or it could be a chain and then you can specify the path as well and so this this allows you to add multiple in the same server um and that's it really um there there's a few more kind of like Advanced options but for the most part this is pretty much it you specify the runnable which again is like a chain you specify the path and you use add routes and you get everything mentioned above um so you'll get the docs um at local host or wherever you deploy it to on docs um you can have this remot runnable which is basically an SDK um and then you can use this um in just like you would uh a normal chain so you can call invoke you can call a invoke you can string it together um and create chains um you can use this as well in in JavaScript so um we're you know we're thinking about and working on a JavaScript version of all this but in the meantime if you deploy something in Python you can easily call it from JavaScript um and and will'll likely work on adding sdks for other languages as well um and then this is what the playground looks like and Eric will uh I think show a live example of this but you'll notice that the playground has a few things so first it has some configuration options so one thing that we've also done is make it possible to specify different configuration options so like what is the temperature of the llm that you want to use which llm do you want to use open AI or anthropic um what prompt do you want to use what retriever do you want to use um and so if you specify those as configurations you can then change them in the playground and so this makes it easy to play around with things um and test things out that's the first bit the second bit is this try it part where basically you have the inputs so here there's only one input um so that's why only one input box shows up but if there was multiple then multiple things would show up um we've done a little bit of work as well to make things like chat history render nicely and files as well so we actually have a few examples of file uploads so you can easily and and so you know I think part you know we're not intending to build uh anything like streamlit or anything like that where you can dynamic Al create uis and we'll always integrate super nicely with streamlet and all that we do think there's three or four basic uis that cover maybe like 80% of the use cases out there or more and so we're going to build you know support for those so that you just get it free out of the box and then if you want to use it with a different UI again every the everything's exposed as apis so very possible to do so the output um shows the final output but then also the intermediate steps so you can see all the intermediate steps of the chain under the hood um and then sharing actually shares the configuration of this so if you have changed like the prompt and LM and you want to share this with a co-worker so they could play around with it um you can get a nice URL for that configuration um Eric's going to cover how to install and use um and then yeah the the last thing I highlight is this deployment section which is a little bit sparse um right now we only have instructions for deployment for gcp we'd love to get more instructions for deploying to other clouds and other services so um if that's something that uh you know people want to try out and and provide feedback on and add instructions we're incredibly open to contributions there um and then we're also working on a hosted version of our own which has like a one-click deploy and I'll add a link um if you're interested in in in beta testing that um the advanced stuff um yeah we talked a little bit about files um custom inputs and output types we we try to automatically infer all the inputs and output types for chains but sometimes we can't always do that and so if you can't always do that we've added like this with types thing um and so you'll notice that a lot of the chains um in the templates that Eric will talk about have this with types so you can always manually specify the typing if we're inferring it incorrectly um and then you can also uh do uh basically custom user types um and this is uh uh kind of how we do some of the um file stuff as well um it basically specifies how to decode things um and then playground widgets um as well and so This Is How We Do file widgets as well so if you look at file widgets um this is this is a yeah this is this is the last thing a bit more of advanced use case but you import this custom user type you specify your input um and then you add this widget type thing um that specifies it again we're not intending for this to be of like a a streamlet cover we're not going to have we're not going to have hundreds of widgets we're going to have like one or two that we think are like pretty common for a lot of the uh uh llm use cases that's pretty much it as an overview of Ling serve um answering um a few of the questions that I see in the chat before handing it off to Eric um how can we use Ling Ser with services like Azure app Services I'm not entirely sure what azer app services are I'm assuming it's a way to deploy things and so we should add some instructions on how to deploy within that um and uh uh yeah the we we if people are interested in adding um bindings for the or sdks for these things I see a plus one for goang yeah we'd love to please uh please reach out um I'll I'll drop kind of like an email where people can reach out if they want to contribute that at the end of the day it should be pretty simple this is a you know this is a um this is a rest API and so I think we just need like all the remote runable does is it exposes methods to call the invoke batch um and uh uh streaming things and streaming logs might get a little bit complicated because some of the objects we pass are like Lang chain objects and so if there's not already Lang chain schemas then I gu starts to get a little bit gnarly um but at the very least we can add kind of like simple sdks for when the inputs and outputs are strings and things like that with that I'm going to hand it off to Eric who will I think go talk about templates and also walk through um an actual example or two instead of just showing documentation like I did yeah um let's let's play a little bit um so just I'm just going to share my screen and um and we'll talk a little bit about how how you can get started with this pretty quickly um so yeah as Harrison mentioned uh Lang service is a great way of kind of Hosting uh chains and uh kind of these llm backends that you might be deploying um and we wanted to make it as easy as possible for you to kind of get started with that and so that's kind of where templates come in where we have at the moment there's I think 46 of them um different kind of starting points um depending on what kind of use case you want to work on um and we'll go through those a little bit and so all of those actually just live in the um python Lang chain GitHub repo under this templates folder um and you'll see that we have we have kind of a list of all of them um just in the file system so each template just occupies its own folder up there um and then we have a read me kind of walking through how to get started so we're just going to go through um getting a new Lang serve app started and installing kind of a a rag conversation chatbot um into that and kind of modifying that to a a kind of silly use case that we can use today um but it's supposed to mimic how you can do that as well um so everything is coordinated through a CLI that we've launched um and you can just install that here uh and kind of with that um you the only other thing that you'll want from this page is kind of figuring out which template you want to get started with um so for uh helping with that we have this nice little index page at the bottom of this um which goes through a few different use cases that we can go through um so as mentioned we're going to go through the retrieval augmented generation chatbot um which works with open Ai and pine cone but there's lots of different uh templates in different kinds of categories that you can get started with and the cool thing about Lang serve and templates is that you can really um go as deep as you want into the construction of them um and modify anything you want um so that let's get started with the the rag chat bot um each of the templates actually has instructions on spinning up that specific example um so in this case um I already set my API keys in my environment um and we can kind of just get started with installing the CLI um so kind of in my ndvs code um the first thing we'll want to do is that um just install command for the CLI um I already have it installed but we'll uh pip install it anyways here um and that should work and then right off the bat um we can just create a new app um so we'll do linkchain app new uh my app and then we can actually just Define a package right away um to install as you'll notice in the readme there's actually ways of like adding packages to your app after the fact as well um so that'll just download the template into um the template along with a lang serve instance into this my app folder um and so the Lang serve instance is or the Lang serve app is in app um so we'll see that a server that looks uh roughly like what Harrison was showing off before uh exists in there uh and then it'll download this rag conversation package um into packages um so we can uh install that as an editable dependency um um just so our app can access that um and then it'll even generate us some ad route code um which doesn't automatically insert into your file yet um but potentially coming soon um so all we're going to do is we're going to add this rag conversation chain into our server um so we'll paste that in there um in a perfect world I would reorder the Imports and things like that to get rid of that error um but from here we can really just run our app so we'll just run Lang chain serve and this will start up um a little web server uh which has both the fast API docs already generated um as well as that playground that um Harrison mentioned before so let's actually try playing with that playground um this particular app is indexing um this uh blog post um about agents um it's I don't know one of Lance's favorites so it's a common one in in a lot of the examples um and we can just ask some questions about it so we can ask something like uh I don't know what is or what does uh coot stand for or some other questions about um about this blog post and it'll do semantic search and actually generate a response for us and then as Harrison mentioned we can actually see all the intermediate step that are running as well um in terms of like which sections of the document actually got passed in um and uh kind of what prompt it's using in order to generate or stream that response back to us um but where it gets really fun is we can actually play with this uh and edit it to our use case so if we just go into this rag conversation chain um this isn't installed as like a non-editable python dependency um it's really meant to be edited to your use case um and there's a lot of runable code in here but we can just modify it to kind of suit what we want and so for example um here the answer synthesis prompt is like asking us to answer a question based on the following qu following context um maybe we just want to say uh like respond as if you were a pirate um or have it respond in some other style and so just by saving that we can now ask our same uh question again um and we'll get back kind of a a more pirate themed Chain of Thought response um that's the playground uh one other thing that comes with fast API this isn't something we added but you can actually see all the API docs for your Lang serve instance um so behind the scenes the playground is actually uh calling stream log um because it's streaming back all of those intermediate steps as well um but you can of course use any of these uh from other languages uh kind of sdks TBD um but uh yeah it's just a nice little rest API that you can use anything I missed right off of that Harrison I think that's a pretty good overview cool um yeah I I'm going to go back to the chat and see if there is any uh oh was I sharing my code properly hopefully or was it all web browser um I think a lot of it was web browser um oh awesome okay thanks for off um just wanted to make sure I was showing off the terminal as well but yeah that's kind of how you get started um with L serve templates um yeah whatever's most useful we can go through some other templates if people are interested there's templates in there ranging uh as you'll see from the list there's actually a ton of retrieval augmented generation ones just because that's a use case we keep seeing um there's um some uh there's some fun ones doing rag on top of like graph databases like neo4j um there's a really cool one using time series data uh from time scale um but it's really cool that we have a bunch of Partners who are kind of helping get people started uh with these kinds of Advanced retrieval techniques and you don't really have to write all that much code to start yeah and and not just like Advanced retrieval techniques but there's one that does like extraction of like plate data for like biotech data right and so like that's something that would never go in Lang chain because it's just like hypers specific um but now we can have like an end to-end example or template and it's easy to pull down easy to modify easy to see what's going on um and and so one thing that I'm really excited about is basically like we want we want to add a lot more templates for a lot of these like use cases as well um and so if people have ideas um like one um actually so one uh we'll we'll share a link um to a a GitHub project where we have a lot of where we're starting to keep track of all these ideas for templates and encourage you to both add ideas there but then also see if there are ideas there that you want to add um and Implement yourself um and uh yeah and and then the other thing um that I think is uh there there are a lot of cool templates there that do things that we hadn't had in linkchain before um so uh one of the and and basically what we'll be doing over the next um uh few days is is highlighting those on on Twitter so like yesterday you know we we have like a parent document retriever that uses like neo4j right and that's something that just like wasn't in Lang chain before but now it's in a template and I think when we announced it we you know announced it big with all like 50 templates or however many but there's a lot of really cool ones that to deserve their own shine um and so we'll be we'll be giving a lot more attention to those over the next um few weeks um uh I think we'll probably do you have anything else that that you want to share um Eric I see you can do like an extraction one if that's fun yeah yeah if you do you may me want to get that queued up and I can answer some questions and then we can hand it back to you for one more thing and then answer more questions after that sounds like a plan cool all right so let's go uh to some of the questions and I'm looking at the questions in the QA box on the right so please drop them there and upload ones um that you want to see answered um so okay so I see a few um or I see two that are kind of like related to the UI Parts um so one which is when could we expect to have out of the box Integrations for standard uis like streamlet verell um and then the other one that I see that's similar is um is that front and editable and if it was built in streamlet uh doesn't mean that I can change it basically um and so okay so so right now the current front end is not really editable it's inferred it's uh kind of like uh uh uh just intended to it's out of the box for free you get that um we're thinking about how to do editable front ends um and so uh yeah I I think our current um so again like all of the um the the front end is just backed by this like uh you know rest API and so it can hook into any front end and so one of the things that we're thinking about a little bit is having like just like you have templates of um uh uh templates of the back end of the chains that you pull down and run we're thinking of having templates of front end things that you pull into your application and then you use those instead of the playground um if that's something that people are interested in working on um again um I'm gonna I'm gonna put um yeah the the I'm gonna put an email in the chat um which is probably like the best place to reach out if you have kind of specific questions um or or or you want to work like front end templates are a good reason to reach out because those don't exist yet so there's not like a standard way to add them um and we obviously have some thoughts on how they should be add but we haven't really we're still figuring it out so if that's something that's interesting to you want to like jam on or add some please reach out to the email I just put in the chat like would love to would love to hear and and and work with folks on that um yeah so the front end's currently not um not configurable um we want to have a similar way of like pulling in or we we think that's interesting to explore um I'm not going to yeah there's there's obviously a lot to do and a lot to prioritize um uh uh one the next question I see pantic in addition to pantic in addition to just with types um so I think this is around how to type the chains um correctly um so yeah basically uh yes like the way that with types work is you pass it a pantic model um and that and that base model kind of like exposes the types um so it's it's conjunction with with both it doesn't have to be a penic model actually so you can pass it like a string the issue with passing it a string is it doesn't um give it a good name um so I think all the examples we have use pantic um and then pass pantic models to with types um how can we deploy with langing serve to production um so we have one example in the Ling serve um GitHub um which I linked at the beginning of the chat of using gcp to deploy um this is another area where if if you want to work on templates for deploying to Azure to AWS um it would be uh yeah it would be good to reach out or or you know yeah I mean we don't have a template to deploy to gcp we just have instructions to deploy to gcp um so I think like adding instructions to deploy to AWS to Azure to to whatever kind of like uh to versel whatever um is interesting we we'd love to have those and would love contributions there um do you have a do you have a example lined up Eric that we could back to yeah uh we can just do an open AI functions one let's do that so back to screen sharing and I'll hide the uh the little extra thing this time um so yeah this for this one uh we can go down to the extraction section and um use an open AI functions one um and so all the all the readme look very similar but uh they have different uh packages that they actually install um but in this case we actually have an existing project um so we can run this Lang chain app ad instead of just creating a new one with a package um so if we go back to um our app um so as as we saw we have like our rag conversation one installed from before um and so we can just add um extraction open AI functions um so this will download and install um that uh new template and so this one will be more of an extraction use case rather than rag um so we'll actually provide uh context for it um instead of uh instead of asking a question so here we can just add our second route so now we're going to have um two different sets of endpoints um and I can actually just show that off in the uh fast API doc so if I refresh this we'll see now that we have all the invoke batch stream and stream log uh calls for rag conversation which was the first example that we went over or first template that we went over um and we also now have these extraction open AI functions ones um which just taken a single string instead of both like a chat history and a question and so if we go to our playground um so this is going to be our rag conversation playground um but we can go to extraction openingi functions um and we're adding a little index so it's easier to to get to these links um so in this case if we look at the readme for extraction open AI functions um what this is going to do is it's going to um extract the title and author of papers um which uh we'll look at in a sec and we'll we'll try and customize it to extract something else um but we can actually just use the same article over here um just because it also has paper ERS and authors um so if we just paste in some section of this um we can see that it's able not reminders um we can see that it's able to extract out um those authors and papers that are kind of covered in this Tas do composition section um and let's actually go into that template um to see why it's it's doing just papers um in instead um so here we can see that we have just a prompt going into a model uh which has kind of some open AI functions um set on it and then we'll talk a little bit how about how we can design those ourselves um and then in the end it's just going to Output that papers key um which is just going to be a list of papers according to our kind of pedantic model here um and then we can see that it's extracting title an author because we um Define those as as the fields to extract so let's say um I don't know what's something we might want to extract from Harrison let's say we want to extract environment variables from our uh our nice template readms um so in that case we can kind of switch out our template uh our prompt for something where uh you will get a um like a read me file about um a project about a developer project um please extract the environment variables necessary to get started with it um so let's use that one instead of this um and then instead of extracting titles and authors um we'll just have kind of uh uh environment variable as a string um and then instead of calling that paper let's call that uh environment variables so with that we should be able to and then we probably want to return that instead of um instead of returning the papers so this is a live demo so hopefully this works uh but uh we can go back to our playground and then instead of pasting in um this section of paper um we can just paste in our extraction open AI functions um one as our input and um and one thing say that again one one thing that we can add um more templates for is basically like post putting in a URL right so so like the exraction like copy pasting that Eric does like you can put anything behind these functions it doesn't have to like take in text so it can take in a URL it can then like load that URL pass that um to uh uh the chain Downstream so I think like the yeah like the I think or you can upload files like with the file widget so I think for like a lot of the extraction use cases we probably should add more examples of doing that um of like you know actually loading the document because one of the nice things about Lang chain is we have a bunch of document loaders as well so we can take advantage of those yeah and right now a lot of those document loaders have to be used um kind of on the user side and so we have a few examples of using those from uh like jupyter notebooks that are included uh with the templates um but yeah long-term the vision is to to include those uh document loaders not only with uh the templates or in notebooks with the templates but as um like kind of first party citizens such that you can upload a file and actually run like a PD F document loader on it or that kind of thing um and before you may have caught that I had a little bug that uh was causing me to still return paper information it's because I was editing the wrong chain. piy file because I've created too many of these templates in the last week um but yeah as we can see now we have our uh kind of input set here and it's able to extract out an open AI API key is the only one um if we go back to our uh rag one which had a few more environment variables that were necessary um we can copy this whole thing um and see if it's able to extract all four of those as well cool oh and it's extracting some extras it's actually telling us that uh in order to set up lsmith there's actually three extras that we can install from down here in our in our uh optional lsmith configuration section so that's Nifty awesome um so I think probably good to wrap up and answer some of the final questions that are in the chat I see a few around um uh memory and and chat history memory which is a very good question um right now yes the Lang Ser servers are meant to be stateless um so I think uh we ex so right now the main way that we recommend people do chat history is handle it on the front end or handle it outside the API um where are looking into improving that experience so that people can take advantage of a lot of the CH uh external database Integrations that we've added for chat history um and so that's one of the big things that we're pushing on and we should have some updates there because we know that so yeah we're working on making kind of like the playground support chat natively we're working on better documentation for how to uh manage chat in the front end um and then we're also working on on supporting using the external databases in the back end so pushing on a few different um few a few different uh uh paths there um and and I I think this is also related is there a predefined way to restart Lang serve chat history um right now nothing stored there um we'll we'll add more documentation and basically beef up that support over time um could you outline the development road map for Ling particularly with its role in facilitating scalable and complex AR architectural architecture designs additionally will leg serve provide specialized endpoints for different components agents chains Etc yeah that's a good question so um uh I'll maybe answer the last one first um so will we provide specialized endpoints for different components right now all components expose kind of like the same interface even in linkchain um so so they do so if you think about retrievers like they do have like a a a get um they have like a get relevant documents method um which is obviously different than like a chat model um method but they also expose the same interface in the sense that they all expose the invoke they allo expose batch um they all expose stream things like that um and so what we're doing and that's due to some changes that we made under the hood to have kind of like a common interface for everything and so uh what that means is that we probably won't have specialized endpoints at the moment for different things um because they'll all we'll handle them in like a generic way but I do think there's a good chance that we start having like a collection of templates that are themselves retrievers or something like that um so I think like we have some Advanced rag methods in there that I think actually some of them probably are just like retrievers and they return documents and so we'll start kind of like um maybe having those be a more formalized thing um and as a note on that the whole of Lang serve it just hosts like a fast API instance um so it's really easy to just add more like app. routes in the same way that you would to any fast API so if you have some custom use case that you don't want to call invoke batch or stream um you can easily add that as well and I think and I think that Point's also relevant for the next part which is like facilitating scalable and complex architecture designs part of this absolutely is to like I think like you know I think we've taken kind of like a pragmatic view in building I think we've taken a pragmatic view in building L chain and building agents and building complex things where I think like you know first um I think first we tried to think about L chain as you know what do you need in order to build these things you need language models you need uh you need prompts you need Vector stores you need tools you need ways to connect them all that's kind of like laying train the open source um then we saw that like you know the next blocker was debugging these things so that's kind of like laying Smith the platform for like debugging um then it's time to deploy like kind of like a simple application and so that's kind of like laying sech the next thing if is kind of like a way to orchestrate many simple things together in like one larger thing and so yeah we're working on some things there um we think langing serve is a great way to deploy like these individual components and then a lot of it a lot of it will kind of like come down to uh uh creating an application that calls these individual components in multiple ways um and so I think like um one of the things that um I want to actually add um over the weekend maybe is uh GPT researcher I don't know if people have played around with it but it's a great open source project and it has a few different I wouldn't I wouldn't necessarily say it's like a multi-agent architecture because agents an overloaded word but it it has like a few different roles where there's um uh something that does research there's something that writ summaries and then there's something that combines everything into one document and I think so those could all be their own individual endpoints and then they're kind of like orchestrated together um outside and so I think yeah as as we add more um definitely expect to see more in this vein um I see one question asking about kind of like an architectural decision for a use case probably going to skip that just to focus on Lang serve um and keep it kind of like narrowly focused on that um I see a question about passing is it possible to pass mvar as. n you yeah you can use a n file and just load it um I think uh I think that should work equally well um when you deployed gcp are when you deployed gcp are the endpoints public or somehow protected so that only you can your clients can talk to the server I imagine that's probably how you define the I imagine that's probably up to you when you define the command that deploys it onto gcp um yeah fast API security extensions if you want to add some sort of like user authentication or just like basic username password you can just add those in um and then I see a comment called laying syn yeah you know part of the fun is that we get to come up with different names with different emojis so um cool um yeah I think that's pretty much all I had anything that you want to add Eric no thanks all for coming in all right yeah thank you guys for tuning in definitely check it out let us know any feedback and then shoot us some emails at that uh email if you're interested in any of the more extended uh uh opportunities and if you have any templates requests there's also a forum in Discord that you can uh request specific ones all right byebye", "metadata": {"source": "o7C9ld6Ln-M", "title": "LangServe and LangChain Templates Webinar", "description": "Unknown", "view_count": 5436, "thumbnail_url": "https://i.ytimg.com/vi/o7C9ld6Ln-M/hqdefault.jpg?sqp=-oaymwEXCJADEOABSFryq4qpAwkIARUAAIhCGAE=&rs=AOn4CLDf7gvV8D3I2UFy0UsA2Wh0qUhA-A", "publish_date": "2023-11-02 00:00:00", "length": 2441, "author": "LangChain"}, "type": "Document"}]